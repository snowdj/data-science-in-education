[
["index.html", "Data Science in Education Using R 1 Introduction: Data Science in Education–You’re Invited! 1.1 Learning Data Science in Education 1.2 Making the Path a Little Clearer", " Data Science in Education Using R Emily A. Bovee, Ryan A. Estrellado, Jesse Mostipak, Joshua M. Rosenberg, and Isabella C. Velásquez 1 Introduction: Data Science in Education–You’re Invited! Dear Data Scientists, Educators, and Data Scientists who are Educators: This book is a warm welcome and an invitation. If you’re a data scientist in education or an educator in data science, your role isn’t exactly straightforward. This book is our contribution to a growing movement to merge the paths of data analysis and education. We wrote this book to make your first step on that path a little clearer and a little less scarier. Whether you’re a data scientist using your skills in an education job or an educator who wants to learn data science skills, we invite you to read this book and put these techniques to work in the real world. We think that your work in the eduation community will help decide how education and data science come together going forward. 1.1 Learning Data Science in Education Over the coming chapters we’ll be learning together about what data science in education can look like. But to understand why we were compelled to write about the topic, we need to talk about why data science in education is not such a straightforward thing. Learning data science in education is challenging because there isn’t a universal vision for that role yet. Data science in education isn’t straightforward because the role itself is not straightforward. If education were a building, it would be multi-storied with many rooms. There are privately and publicly funded schools. There are more than eighteen possible grade levels. Students can learn alone or with others in a classroom. This imaginary building we call education also has rooms most residents never see: Business and finance staff plan the most efficient use of limited funds. The transportation department plans bus routes across vast spaces. University administrators search for the best way to measure career readiness. Education consultants study how students perform on course work and even how they feel about class materials. There are a lot of ways one could do data science in education, but building consensus on ways one should do data science in education is just getting started. The data science in education community is still working out how it all fits together. And for someone just getting started, it can all seem very overwhelming. Even if we did have perfectly clarity on the topic, there’s still the issue of helping education systems learn to leverage these new analytic tools. In many education settings, school administrators and their staff have never had someone around who understands education, knows how to code, and uses statistical techniques all at once. 1.2 Making the Path a Little Clearer As data science in education grows, the way we talk about it also needs to grow. We begin this book by offering a primer for data science in education, including a discussion of unique challenges and foundational skills in the programming language R. Next, you’ll take what you’ve learned and apply it in our data analysis in education walkthroughs. The walkthroughs in this book are our contribution towards a more example-driven approach to learning. They’re meant to make the ambiguous path of learning data science in education a little clearer by way of recognizable and actionable demonstrations. These examples fall into three different education data themes, with walkthroughs for each theme: Student perception of learning - Walkthrough 1: multi-level models of survey data - Walkthrough 7: text analysis of tweets Analyze student performance data - Walkthrough 2: analyzing gradebooks - Walkthrough 3: using machine learning to predict final course grades Get value from publicly available data - Walkthrough 5: analyzing aggregate data - Walkthrough 6: doing longitudinal analysis on special education enrollment data We’ll end the book by discussing how to bring data science skills into your education job. We hope after reading this book you’ll feel like you’re not alone in learning to do data science in education. We hope your experience with this book is challenging, but fun. And finally, we hope you’ll take what you learned and share it with others who are looking to start this journey. "],
["how-to-use-this-book.html", "2 How to Use this Book 2.1 Different Strokes for Different Data Scientists in Education 2.2 A Note on Statistics 2.3 What This Book Is Not About 2.4 Contributing to the Book", " 2 How to Use this Book We’ve heard it from fellow data scientists and experienced it many times - learning a programming language is hard. Learning a programming language, like learning a foreign language, is not just about mastering vocabulary. It’s also about learning norms, the language’s underlying structure, and the metaphors that hold the whole thing together. The beginning of the learning journey is particularly challenging because it feels slow. If you have experience as an educator or consultant, you already have efficient solutions you use in your day-to-day work. Introducing code to your workflow slows you down at first because you won’t be as fast as you are with your favorite spreadsheet software. But learning how to analyze data using R is like investing in your own personal infrastructure–it takes time while you’re building the initial skills, but the investment pays off when you start solving complex problems faster and at scale. One person we spoke with shared this story about their learning journey: “The first six months were hard. I knew how quickly I could do a pivot table in Excel. It took longer in R because I had to go through the syntax and take the book out. I forced myself to do it though. In the long-term, I’d be a better data scientist. I’m so glad I thought that way, but it was hard the first few months.” Our message is this: learning R for your education job is doable, challenging, and rewarding all at once. We wrote this book for you because we do this work every day. We’re not writing as education data science masters. We’re writing as people who learned R and data science after we chose education. And like you, learning to use R and data science to improve the lives of students is our daily practice. Join us in enjoying all that comes with that–both the challenge of learning and the joy of solving problems in creative ways. 2.1 Different Strokes for Different Data Scientists in Education It’s tough to define data science in education because people are educated in all kinds of settings and in all kinds of age groups. Education organizations require different roles to make it work, which creates different kinds of data science uses. A teacher’s approach to data analysis is different from an administrator’s or an operations manager. We also know that learning data science and R is not in the typical job description. Most readers of this book are educators working with data and looking to expand their tools. You might even be an educator who doesn’t work with data, but who’s discovered a love for learning about the lives of students through data. Either way, learning data science and R is probalby not in your job description. Does this describe your situation? You’ve got a full work schedule and challenging demands in the name of improving the student experience. Your busy workday doesn’t include regular professional development time or self-driven learning. You also have a life outside of work, including family, hobbies, and relaxation. We struggle with this ourselves, so we’ve designed this book to be used in lots of different ways. The important part in learning this materials is to establish a routine that allows you to engage and practice the content every day, even if for a few minutes at a time. That will make the content ever present in your mind and will help you shift your mindset so you start seeing even more opportunities for practice. We want all readers to have a rewarding experience and so we believe there should be different ways to use this book. Here are some of those ways: 2.1.1 Read the Book Cover to Cover (and How to Keep Going) We wrote this book assuming you’re at the start of your journey learning R and using data science in your education job. The book takes you from installing R to practicing more advanced data science skills like text analysis. If you’ve never written a line of R code, we welcome you to the community! We wrote this book for you. Consider reading the book cover to cover and doing all the analysis walkthroughs. Remember that you’ll get more from a few minutes of practice every day than you will from long hours of practice every once in awhile. Typing code everyday, even if it doesn’t always run, is a daily practice that invites learning and a-ha moments. We know how easy it is to avoid coding when it doesn’t feel successful, so we’ve designed the book to deliver frequent small wins to keep the momentum going. But even then, we all eventually hit a wall in our learning. When that happens, take a break and then come back and keep coding. When daily coding becomes a habit, so does the learning. If you get stuck in an advanced chapter and you need a break, try reviewing an earlier chapter. You’ll be surprised at how much you learn from reviewing old material with the benefit of new experience. That kind of back-to-basics attitude is sometimes what we need to get a fresh perspective on new challenges. 2.1.2 Pick a Chapter of Interest and Start There We interviewed R users in education as research for this book. We chose people with different levels of experience in R, the education field, and statistics. We asked each interviewee to rate their level of experience on a scale from 1 to 5, with 1 being no experience and 5 being very experienced. You can try this now–take a moment to rate your level of experience in: Using R Education as a field Statistics If you rated yourself as a 1 in Using R, we recommend reading the book from beginning to end as part of a daily practice. If you rated yourself higher than a 1, consider reviewing the table of contents and skimming across all the chapters first. If a particular chapters call to you, feel free to start your daily practice there. Eventually, we do hope you choose to experience the whole book even if you start somewhere in the middle. For example, you might be working through a specific use case in your education job–analyzing student quiz scores, evaluating a school program, introducing a data science technique to your teammates, or designing data dashboards for example. If this describes your situation, feel free to find a section in the book that inspires you or shows you techniques that apply to your project. # LEFT OFF December 2, 2019 2.1.3 Read Through the Walkthroughs and Run the Code If you’re experienced in data analysis using R, you may be interested in starting with the walkthroughs. Each walkthrough is designed to demonstrate basic analytic routines using datasets that look familiar to educators. In this approach, we suggest readers be intentional about what they want to learn from the walkthroughs. For example, readers may seek out examples of aggregated datasets, exploratory data analysis, the {ggplot2} package, or the gather() function. Read the walkthrough and run the code in your R console as you go. After you successfully run the code, experiment with the functions and techniques you learned by changing the code and seeing new results (or new error messages!). After running the code in the walkthroughs, reflect on how what you learned can be applied to the datasets, problems, and analytic routines in your education work. One last note on this approach to the book: we believe that doing data science in R is, at its heart, an endeavor aimed at improving the student experience. This endeavor involves complex problems and collaboration. Be sure to read other areas of the book that give context to why and how we do this work. Chapter Twelve in particular explores ways to introduce these skills to your education job and invite others into analytic activities. The skills taught in the walkthroughs are only one part of doing data science in education using R. 2.2 A Note on Statistics It’s been said that data science is the intersection between content expertise, programming, and statistics. You’ll want to grow all three of these as you learn more about using data science in your education job. Your education knowledge will lead you to the right problems, your statistics skills will bring rigor to your analysis, and your programming skills will scale your analysis to reach more people. What happens when we remove one of these pieces? Consider a data scientist working in education who is an expert programmer and statistician but has not learned about the real life conditions that generate education data. She might make analysis decisions that ignore the nuances in the data. Or consider a data scientist who is an expert statistician and an education veteran, but has not learned to code. She will find it difficult to scale her analysis and have the largest possible influence on improving the student experience. And finally, consider a data scientist who is an expert programmer and an education veteran. She can only scale surface level analysis and might miss chances to draw causal relationships or predict student outcomes. In this book we will spend a lot of time learning R by way of recognizable education data examples. But doing a deep dive into statistics and how to use them responsibly is better covered by books dedicated solely to the topic. It’s hard to understate how important this part of the learning is on the lives of students and educators. One education data scientist we spoke to said this about the difference between building a model for an online retailer and building a model in education: “It’s ok if I get shown 1000 brooms but if I got my model wrong and we close a school that will change someone else’s world.” We want this book to be your go-to R reference as you start integrating data science tools into your education job. Our aim is to help you learn R by teaching it in two contexts: data science techniques and workaday education datasets. We’ll demonstrate statistics techniques like hypothesis testing and model building and how to run these operations in R. But the explanations stop short of a complete discussion about the statistics themselves. We wrote within these boundaries because we believe that the technical and ethical use of statistics techniques deserve their own space. We hope that you’ll take a satisfying leap forward in your learning by successfully using R to run the models and experiencing the model interpretations in our walkthroughs. We encourage you to explore other excellent books like Navarro’s Learning Statistics With R as you learn the required nuances of applying statistical techniques to scenarios outside our walkthroughs. 2.3 What This Book Is Not About While we wrote Data Science in Education Using R to be a wide-ranging introduction to the topic, there is a great deal that this book is not about. Some of these topics are those that we would like to have been able to include but did not because they did not fit our intention (providing a solid foundation in doing data science in education). We chose to not include others because, frankly, excellent resources already exist. We detail some of what we had to not include in the book here. git/GitHub: Using git and GitHub are parts of the educational data scientists workflow; however, these can be a challenge to us (and are not needed to get started). Moreover, an outstanding introduction to their use exists in Bryan’s freely-available Happy git with R. Building R packages: If you are carrying out the same analyses many times, it may be helpful to create your own package, such as the roomba (for tidying complex, nested lists) and tidyLPA (for carrying out Latent Profile Analysis) packages that authors of this book created. However, building an R package is not the focus of this book, and Wickham has made the helpful R packages book freely-available Advanced statistical methodologies: As noted above, there are other excellent books for learning statistics; while we do discuss statistical methods (including some that can be considered to be advanced), this is not, primarily, a statistics text, and we especially do not consider this to be an advanced statistical methods book; one that we think is excellent as an advanced text from a machine learning perspective is Hastie et al’s Introduction to Statistical Learning. Creating a website (or book): While this book is created using the bookdown package (for creating a book through R), and many of us have created our personal websites using the blogdown R package (for cerating a website through R), this book does not describe how to do these; there are excellent, freely available books (see books for blogdown and bookdown books). 2.4 Contributing to the Book We designed this book to be useful and practical for our readers in education and by doing so almost certainly left out much. We did this to create a reference that is not intimidating to new users and indeed creates frequent small wins while learning to use R. But how do we expand the work as data science in education itself expands? We wrote this book in the open on GitHub so that community members can help us evolve the work. We hope that as the book evolves it grows to reflect the changing needs of data scientists in education. We want this to be the book new data scientists in education have with them as they grow their craft. To do that, it’s important to us that the stories and examples in the book are based on your stories and examples. And so we’ve built ways for you to share with us. Here’s how you can can contribute: Submit a pull request to our GitHub site that describes a data science problem that is unique to the education setting Submit a pull request to share a solution for the problems discussed in the book to the education setting Share an anonymized dataset "],
["what-is-a-data-scientist-in-education.html", "3 What is a Data Scientist in Education?", " 3 What is a Data Scientist in Education? One way to define data science is to think of it as combining three skills to do data analysis: programming, statistics, and content knowledge. Though if you Google “what is a data scientist,” you’ll won’t find a simple answer. But for this book’s exploration, thinking of data science as a combination of these three skills is useful because we can try substituting the field of education in for “content knowledge.” Even then, we still face a broad field of possibilities when imagining what a data scientist in education actually does on a day-to-day basis. While having no established data science identity makes it hard for educators to explain their data work to the layperson, it does allow them to take on a variety of data-related activities and, ultimately, build the definition of the role. So rather than grapple with defining this role, let’s share some examples of what data scientists do in the field of education. Leading Office Culture Toward a Data-Driven Approach Jesse, a director at an education non-profit in Texas, is setting up a database to house student achievement data. This project requires a number of data science skills we’ll discuss in chapter five, including cleaning data into a consistent format. Once the data is prepared, Jesse builds dashboards to help her teammates explore the data. But not all of Jesse’s work can be found in a how-to manual for data scientists. She manages a team and serves as the de facto project manager for IT initiatives. And given her expertise and experience in data science, she’s leading the charge towards a more data-driven approach within the organization. Helping School Districts Plan to Meet Their Goals Ryan, a special education administrator in California, uses data science to reproduce the state department of education’s special education compliance metrics, then uses the results to build an early warning system for compliance based on local datasets. In this case, Ryan uses foundational data science skills like data cleaning, visualization, and modeling to help school districts monitor and meet their compliance requirements. Doing and Empowering Research On Data Scientists in Education Joshua, Assistant Professor of STEM Education at University of Tennessee in Knoxville, researches how students do data science and helps teachers teach the next generation of data-informed citizens. He makes this work possible by building R packages - self-contained groups of data tools - that he and other researchers use to analyze datasets efficiently. The data scientists in these examples apply statistics and programming to create new knowledge in the education field. But that’s as far as we can go when looking for commonalities in their day-to-day work. Maybe the education community will develop common norms and expectations for how it all works together as the relationship between data science and education grows. But because this relationship is still young, it is important that the people growing data science within education understand the culture and unique challenges in their education job. After all, the defining feature that will differentiate data science in education from data science in general will be doing data science that meets the unique needs of students, staff, and administration in education. "],
["unique-challenges.html", "4 Unique Challenges 4.1 Challenges Common to Doing Data Science in any Domain 4.2 Challenges Common to Doing Data Science in Education 4.3 Analytic Challenges 4.4 Not Just Challenges", " 4 Unique Challenges Because data science in educational settings is a relatively new phenomenon, it’s understandable that school staff may be wary of how data is collected and analyzed. It’s common for school staff to question how data is used, particularly if the data is used to describe staff and student performance. One of the biggest challenges that can arise is if individuals begin to feel that they are being evaluated by metrics that they feel are unclear or even unfair. Usually, “data-driven” efforts mean different things to administrators as compared to educators. To an administrator, a data-driven effort might be an endeavor to better understand the strengths and weaknesses of pre-existing systems, with an eye to eventually proposing new systems that are more efficient. To an educator, a data-driven effort might feel like an approach that masks the individuality of students by reducing them to numbers. Perhaps neither perspective is exactly correct. Whereas maximizing efficiency and preserving students’ individual needs should certainly be goals of educators and educational administrators, data science is a versatile tool that can be leveraged to help answer a variety of meaningful questions. This chapter will present some challenges to the adoption of data science cultures in educational contexts, and will provide some strategies to address these challenges. Data science in education is a new domain. It presents opportunities, like those discussed in the previous chapter, but also some challenges. These challenges vary a lot: we consider doing data science in education to include not only access, processing, and modeling data, but also social and cultural factors, like the training and support that educational data scientists have available to them. These challenge range from the very general (and common to all domains in which data science is carried out) to very particular to the field of education. These are discussed in the remainder of this chapter. 4.1 Challenges Common to Doing Data Science in any Domain 4.1.1 A New Field One challenge for educational data scientists is common to data scientists in other domains: combining content knowledge, programming, and statistics to solve problems is a fairly new idea. In particular, the amount of data now available means that programming is often not only helpful, but necessary, for stakeholders to effectively use data in education. Programming is powerful but challenging, and many of us in education do not have prior experience with it. Despite this challenge and the difficulty of writing the first few lines of code, there is good evidence, and many examples, that those of us without prior programming experience can learn. It is not necessary to have a computer science background or even any formal or informal training related to coding. The great thing about entering a field as flexible as data science is that you are joining a vast crowd of individuals who are self-taught. You will find that there is a very supportive online community to help as you learn through this book. 4.1.2 Addressing Ambiguity: A Reproducible Approach One way to address concerns of educators feeling wary of ambiguous data processes is to build analytic processes that are open about what data is collected, how it is collected, how it is analyzed, and how it is considered alongside other data when used in decision-making conversations. This can be achieved through a number of activities, including regular conversations about analytic methods, written reports describing data collection, and receiving input about analytic goals from staff members. One such process for achieving openness in data collection and analysis is called reproducible research. The concept of reproducible work is the idea that a completed analysis should come with all the necessary materials, including a description of methodology and programming code, needed for someone else to run the analysis and achieve the same results. If school staff are apprehensive about how school data is collected and used, it follows that a more transparent method for using data could go some way towards putting school staff - the consumers of school data - at ease. An additional benefit of a reproducible approach is that it can ease in transition periods. If a data-science-in-school advocate leaves their original position, ideally, they would leave behind not just descriptions of the analyses that they did, but also the specific files needed to run the analyses again. The new individual who takes their place will be able to seamlessly transition into the new role. If asked to run “the same report I always got from your predecessor,” the new person will understand immediately what files were needed to create that original report and will be able to request all necessary data to generate a new version of the report. To implement a reproducible approach in your organization, you can start by keeping all files related to each project you do in their own folders. As you create reports from the data, keeping notes in the files will help you to easily generate similar reports in the future. Many educators find that even though changing administration might mean changing requests, having careful documentation of past processes allows for more efficiency in the way they use data to answer those requests. 4.2 Challenges Common to Doing Data Science in Education 4.2.1 Addressing organizational resistance: A self-driven analytic approach One challenge of adopting data science strategies in educational contexts is that in some environments, there is a lack of precedent - it is not common, for example, for a teacher to be conducting analyses on data. However, it’s not necessary to wait for a district-wide or state-wide initiative to begin to implement the techniques you will learn in this book. An organization should encourage their staff to do their own data analyses primarily for the purpose of testing their own hypotheses. In a school, for example, a teacher might wonder about student learning in their classroom and might want to utilize data to directly guide decisions about how they deliver instruction. There are at least two benefits to this approach. First, staff begin to realize the value of doing data analysis as an ongoing inquiry into their outcomes, instead of a special event once a year ahead of school board presentations. Second - and more important for the idea of reducing apprehension around data analysis in schools - school staff begin to demystify data analysis as a process. When school staff collect and analyze their own data, they know exactly how it is collected and exactly how it is analyzed. The long-term effect of this self-driven analytic approach might be more openness to analysis, whether it is self-driven or conducted by the school district. Building and establishing a data governance system that advocates for an open and transparent analytic process is difficult and long-term work, but the likely result will be less apprehension about how data is used and more channels for school staff to participate in the analysis. Here are more practical steps a school district can take towards building a more open approach to analysis: Make technical write-ups of data analyses available so interested parties can learn more about how data was collected and analyzed Make datasets available to staff within the organization, to the extent that privacy laws and policies allow Establish an expectation that analysts present their work in a way that is accessible to many levels of data experience Hold regular forums to discuss how the organization collects and uses data By adopting a self-driven analytic approach, individuals can help their educational organization to embrace the potential of utilizing data to anticipate and possibly forestall problems in the future. 4.2.2 Lack of Processes and Guidelines One challenge is the ambiguity around the process and practice of doing data science in the educational context specifically. While there is a body of past research on students’ work with data (see Lee &amp; Wilkerson, 2018, for a review), there is limited information from case- or design-based research on how others in education –teachers, administrators, and data scientists– use data in their work. In other words, we do not have a good idea of what best practices in our field are. This challenge is reflected in part in the variability in the roles of those who work with data. Many districts employ data analysts and research associates; some are now advertising and hiring for data scientist positions. Education is a field that is rich with data: survey, assessment, written, and policy and evaluation data, just for a few examples. Nevertheless, sometimes, there is a lack of processes and procedures in place for school districts and those working in them to share data with each other in order to build knowledge and context. In academic and research settings, there are not often structures in place to facilitate the analysis of data and sharing of results. This means that there can be silos of information - one group could do a survey, and another group could not hear about the results or even hear about the fact that the survey happened. The good news about this is that many educational organizations are curious and passionate about supporting student success, so it is likely that even if many separate data collection efforts are being implemented (rather than one unified strategy), you will not be dealing with the problem of “I don’t have enough data to work with.” As a pioneer for data science in your organization, you can help to clarify these redundant processes and can offer your skills to help make sense of the wealth of information already being gathered. 4.2.3 Limited Training and Educational Opportunities Educational data science is new. At the present time, there are limited opportunities for those working in education to build their capabilities in educational data science (though this is changing to an extent; see Anderson and colleagues’ work to create an educational data science certificate program at the University of Oregon and Bakers’ educational data mining Massive Open Online Course offered through Coursera). Many educational data scientists have been trained in fields other than statistics, business analytics, or research. Moreover, the training in terms of particular tools and approaches that educational data scientists utilize are highly varied. However, we believe this diversity of training creates unique opportunities for the development of data science in education as a field. Indeed, educators’ diverse training and backgrounds positions them to tackle educational challenges creatively. 4.2.4 The Complex and Messy Nature of Education Data Another challenge concerns the complex nature of education data. Education data are often hierarchical, in that data at multiple “levels” is collected. These levels include classrooms, schools, districts, states, and countries - quite the hierarchy! In addition to the hierarchical nature of education data, by their nature, these data often require linking with other data, such as data that provides context at each of the aforementioned levels. For example, when data is collected on students at the school level, it is often important to know about the training of the teachers in the school; data at the district level needs to be interpreted in the context of the funding provided by the community in terms of per-pupil spending. A final aspect concerns the type of data collected. Often, education data is numeric, but just as often, it is not. Education data involves characteristics of students, teachers, and other individuals that are categorical (a descriptive type of variable with multiple levels for which a the levels do not signify quantity but instead signify groups, such as sex or grade level), open-ended responses that are strings (a type of variable used to store text), or even recordings that consist of audio and video data. All of these present challenges to the educational data scientist. As with the diversity of training for educational data scientists, though, the complexity of educational data also presents opportunities for educators to creatively approach their tasks. If you are faced with a large and complicated dataset, you might begin by asking yourself what you are curious about and carving out just a couple variables that you can use to answer your question. Your colleague might be interested in an entirely different question, and might consider different variables from the same dataset in their analysis. The complexity of education data need not discourage educators from pursuing their interests. 4.2.5 Ethical and Legal Concerns Related to the complex and messy nature of education data is its confidential nature. At the K-12 level, most data requires safeguards because youth are a protected population. A closely related issue concerns the aims of education within predetermined constraints. Those working in education often seek to improve it and often work to do so with a scarcity of school and community resources. These ethical, legal, and even values-related concerns may become amplified as the role of data in education increases. They should be carefully considered and emphasized from the outset by those involved in educational data science. If you feel resistance in your organization as you begin to adopt the principles you learn in this book, you might begin by offering to analyze “de-identified” or “anonymous” data. In this way, you could show your administration what is possible and foster additional buy-in further down the road. 4.3 Analytic Challenges Due to the challenging nature of education data, analyzing education data is hard, too. The data is often not ready to be used: It may be in a format that is difficult to open without specialized software or it may need to be “cleaned” before it is usable. In data science, “cleaning” data refers to reorganizing or restructuring the dataset to make it easier to analyze. This process would be analogous to if you received an Excel spreadsheet but the columns were in an order that didn’t make sense to you, or one column was duplicated in the spreadsheet. The process you’d go through to reorganize the data to make it logical is data cleaning. Closely related to the ethical and legal challenges discussed above, educational data scientists should be conscious of potential racial and gender biases in school models, and challenge not reinforce them. Because of the different types of data, the educational data scientist must often use a variety of analytic approaches, such as multi-level models, models for longitudinal data, or even models and analytic approaches for text data. In later chapters of this book, you will learn more specifics about building models. 4.4 Not Just Challenges While there are many challenges to working with education data, there are many opportunities as well. Once they unlock the power of data science to reveal insights about their organizational context (their students, their teaching, etc.), many educators will become more interested in gathering more data and continuing on this path. Data science becomes a useful tool to help connect with the purpose of your job. Once you begin to rely on data science, it can be hard to stop! As an educational professional, remember that you are more closely acquainted with your context than any outside analyst could ever be. This affords you the unique opportunity to become the data and analysis guru in your area. In summary, educators that want to evolve their data analysis processes into something practical and meaningful to student progress will need to address some unique challenges in order to help all stakeholders to understand the benefits of the questions being answered with data. That hard work will pay off. "],
["foundational-skills.html", "5 Foundational Skills 5.1 Track One: Getting Started 5.2 Downloading R and R Studio 5.3 Check that it worked 5.4 Help, I’m completely new to using R / R Studio! 5.5 Creating Projects 5.6 Packages 5.7 Loading Data from Various Sources 5.8 Processing Data 5.9 Communicating / Sharing Results 5.10 Other Foundational Notes", " 5 Foundational Skills This chapter is organized into two tracks (though, of course, you are welcome to read both). If you have experience using R - or have used it a few times, attended a workshop, or been involved with a collaborator who used it - consider starting with Track Two, focused on ‘data loading and manipulation using the tidyverse’, which covers reading/saving files, pipes, selecting, filtering, etc. chapters. Otherwise, start at Track One, which covers installation, projects, and packages–and then proceed to the second track. 5.1 Track One: Getting Started First, you will need to download the latest versions of R and R Studio. R is a free environment for statistical computing and graphics using the programming language R. R Studio is a set of integrated tools that allows for a more user-friendly experience for using R. Although you will likely use R Studio as your main console and editor, you must first install R as R Studio uses R behind-the-scenes. Both are freely-available, cross-platform, and open-source. 5.2 Downloading R and R Studio 5.2.1 To download R: Visit this page to download R: https://cran.r-project.org/ Find your operating system (Mac, Windows, or Linux) Download the ‘latest release’ on the page for your operating system and download and install the application Don’t worry; you will not mess anything up if you download (or even install!) the wrong file. Once you’ve installed both, you can get started. 5.2.2 To download R Studio: Visit this page to download R studio: https://www.rstudio.com/products/rstudio/download/ Under the column called “R Studio Desktop FREE”, click Download Find your operating system (Mac, Windows, or Linux) Download the ‘latest release’ on the page for your operating system and download and install the application If you do have issues, consider this page, and then reach out for help. One good place to start is the R Studio Community. 5.3 Check that it worked Open R Studio. Find the console window and type in 2 + 2. If what you can guess is returned (hint: it’s what you expect!), then R Studio and R both work. 5.4 Help, I’m completely new to using R / R Studio! If you’re completely new, Swirl is a great place to start, as it helps you to learn R from within R Studio. Visit this page to see some directions: http://swirlstats.com. And if you’re ready to go, please proceed to the next sections on processing and preparing, plotting, loading, and modeling data and sharing results. 5.5 Creating Projects Before proceeding, we’re going to take a few steps to set ourselves to make the analysis easier; namely, through the use of Projects, an R Studio-specific organizational tool. To create a project, in R Studio, navigate to “File” and then “New Directory”. Then, click “New Project”. Choose a directory name for the project that helps you to remember that this is a project that involves data science in education; it can be convenient if the name is typed in lower-case-letters-separated-by-dashes, like that. You can also choose the sub-directory. If you are just using this to learn and to test out creating a project, you may consider placing it in your downloads or another temporary directory so that you remember to remove it later. Even if you do not create a Project, you can always check where your working directory (i.e., where your R is pointing) is by running getwd(). To change it manually, run setwd(desired/file/path/here). 5.6 Packages “Packages” are shareable collections of R code that provide functions (i.e., a command to perform a specific task), data and documentation,. Packages increase the functionality of R by improving and expanding on base R (basic R functions). 5.6.1 Installing and Loading Packages To download a package, you must call install.packages(): install.packages(&quot;dplyr&quot;, repos = &quot;http://cran.us.r-project.org&quot;) You can also navigate to the Packages pane, and then click “Install”, which will work the same as the line of code above. This is a way to install a package using code or part of the R Studio interface. Usually, writing code is a bit quicker, but using the interface can be very useful and complimentary to use of code. After the package is installed, it must be loaded into your R Studio session using library(): library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union We only have to install a package once, but to use it, we have to load it each time we start a new R session. A package is a like a book, a library is like a library; you use library() to check a package out of the library. - Hadley Wickham, Chief Scientist, R Studio 5.6.2 Running Functions from Packages Once you have loaded the package in your session, you can run the functions that are contained within that package. To find a list of all those functions, you can run this in the R Studio console: help(package = dplyr) The documentation should tell you what the function does, what arguments (i.e., details) needed for it to successfully run, examples, and what the output should look like. We’ll be using filter a lot in this book, so let’s take that as an example. filter is a function in the dplyr package that removes rows from datasets based on logical criteria, like whether a column takes a specific value. To learn more about using filter, run this in the R Studio console: ??dplyr::filter Once you know what you want to do with the function, you can run it in your code: dat &lt;- # example data frame data.frame(stringsAsFactors=FALSE, letter = c(&quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;), number = c(1L, 2L, 3L, 4L, 5L)) dat ## letter number ## 1 A 1 ## 2 A 2 ## 3 A 3 ## 4 B 4 ## 5 B 5 filter(dat, letter == &quot;A&quot;) # using dplyr::filter ## letter number ## 1 A 1 ## 2 A 2 ## 3 A 3 5.6.3 Track Two: Welcome to the Tidyverse The Tidyverse is a set of packages for data manipulation, exploration, and visualization using the design philosophy of ‘tidy’ data. Tidy data has a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. The packages contained in the Tidyverse provide useful functions that augment base R functionality. You can installing and load the complete Tidyverse with: install.packages(&quot;tidyverse&quot;) library(tidyverse) For more information on tidy data, check out Hadley Wickhams’s Tidy Data paper. 5.7 Loading Data from Various Sources In this section, we’ll load data. You might be thinking that an Excel file is the first that we would load, but there happens to be a format which you can open and edit in Excel that is even easier to use between Excel and R as well as SPSS and other statistical software, like MPlus, and even other programming languages, like Python. That format is CSV, or a comma-separated-values file. The CSV file is useful because you can open it with Excel and save Excel files as CSV files. Additionally, and as its name indicates, a CSV file is rows of a spreadsheet with the columns separated by commas, so you can view it in a text editor, like TextEdit for Macintosh, as well. Not surprisingly, Google Sheets easily converts CSV files into a Sheet, and also easily saves Sheets as CSV files. For these reasons, we start with - and emphasize - reading CSV files. 5.7.1 Saving a File from the Web You’ll need to copy this URL: https://goo.gl/bUeMhV Here’s what it resolves to (it’s a CSV file): https://raw.githubusercontent.com/data-edu/data-science-in-education/master/data/pisaUSA15/stu-quest.csv This next chunk of code downloads the file to your working directory. Run this to download it so in the next step you can read it into R. As a note: There are ways to read the file directory (from the web) into R. Also, of course, you could do what the next (two) lines of code do manually: Feel free to open the file in your browser and to save it to your computer (you should be able to ‘right’ or ‘control’ click the page to save it as a text file with a CSV extension). student_responses_url &lt;- &quot;https://goo.gl/bUeMhV&quot; student_responses_file_name &lt;- paste0(getwd(), &quot;/data/student-responses-data.csv&quot;) download.file( url = student_responses_url, destfile = student_responses_file_name) It may take a few seconds to download as it’s around 20 MB. The process above involves many core data science ideas and ideas from programming/coding. We will walk through them step-by-step. The character string \"https://goo.gl/wPmujv\" is being saved to an object called student_responses_url. student_responses_url &lt;- &quot;https://goo.gl/bUeMhV&quot; We concatenate your working directory file path to the desired file name for the CSV using a function called paste0. This is stored in another object called student_reponses_file_name. This creates a file name with a file path in your working directory and it saves the file in the folder that you are working in. student_responses_file_name &lt;- paste0(getwd(), &quot;/data/student-responses-data.csv&quot;) The student_responses_url object is passed to the url argument of the function called download.file() along with student_responses_file_name, which is passed to the destfile argument. In short, the download.file() function needs to know - where the file is coming from (which you tell it through the url) argument and - where the file will be saved (which you tell it through the destfile argument). download.file( url = student_responses_url, destfile = student_responses_file_name) Understanding how R is working in these terms can be helpful for troubleshooting and reaching out for help. It also helps you to use functions that you have never used before because you are familiar with how some functions work. Now, in R Studio, you should see the downloaded file in the Files tab. This should be the case if you created a project with R Studio; if not, it should be whatever your working directory is set to. If the file is there, great. If things are not working, consider downloading the file in the manual way and then move it into the directory that the R Project you created it. 5.7.2 Loading a CSV File Okay, we’re ready to go. The easiest way to read a CSV file is with the function read_csv() from the package readr, which is contained within the Tidyverse. Let’s load the tidyverse library: library(tidyverse) # so tidyverse packages can be used for analysis You may have noticed the hash symbol after the code that says library(tidyverse). It reads# so tidyverse packages can be used for analysis`. That is a comment and the code after it (but not before it) is not run (the code before it runs just like normal). Comments are useful for showing why a line of code does what it does. After loading the tidyverse packages, we can now load a file. We are going to call the data student_responses: # readr::write_csv(pisaUSA15::stu_quest, here::here(&quot;data&quot;, &quot;pisaUSA15&quot;, &quot;stu_quest.csv&quot;)) student_responses &lt;- read_csv(&quot;./data/student-responses-data.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_double(), ## CNT = col_character(), ## CYC = col_character(), ## NatCen = col_character(), ## STRATUM = col_character(), ## Option_Read = col_character(), ## Option_Math = col_character(), ## ST011D17TA = col_character(), ## ST011D18TA = col_character(), ## ST011D19TA = col_character(), ## ST124Q01TA = col_logical(), ## IC001Q01TA = col_logical(), ## IC001Q02TA = col_logical(), ## IC001Q03TA = col_logical(), ## IC001Q04TA = col_logical(), ## IC001Q05TA = col_logical(), ## IC001Q06TA = col_logical(), ## IC001Q07TA = col_logical(), ## IC001Q08TA = col_logical(), ## IC001Q09TA = col_logical(), ## IC001Q10TA = col_logical() ## # ... with 420 more columns ## ) ## See spec(...) for full column specifications. Since we loaded the data, we now want to look at it. We can type its name in the function glimpse() to print some information on the dataset (this code is not run here). glimpse(student_responses) Woah, that’s a big data frame (with a lot of variables with confusing names, to boot)! Great job loading a file and printing it! We are now well on our way to carrying out analysis of our data. 5.7.3 Loading Excel Files We will now do the same with an Excel file. You might be thinking that you can open the file in Excel and then save it as a CSV. This is generally a good idea. At the same time, sometimes you may need to directly read a file from Excel. Note that, when possible, we recommend the use of CSV files. They work well across platforms and software (i.e., even if you need to load the file with some other software, such as Python). The package for loading Excel files, readxl, is not a part of the tidyverse, so we will have to install it first (remember, we only need to do this once), and then load it using library(readxl). Note that the command to install readxl is grayed-out below: The # symbol before install.packages(\"readxl\") indicates that this line should be treated as a comment and not actually run, like the lines of code that are not grayed-out. It is here just as a reminder that the package needs to be installed if it is not already. Once we have installed readxl, we have to load it (just like tidyverse): install.packages(&quot;readxl&quot;) library(readxl) We can then use the function read_excel() in the same way as read_csv(), where “path/to/file.xlsx” is where an Excel file you want to load is located (note that this code is not run here): my_data &lt;- read_excel(&quot;path/to/file.xlsx&quot;) Of course, were this run, you can replace my_data with a name you like. Generally, it’s best to use short and easy-to-type names for data as you will be typing and using it a lot. Note that one easy way to find the path to a file is to use the “Import Dataset” menu. It is in the Environment window of R Studio. Click on that menu bar option, select the option corresponding to the type of file you are trying to load (e.g., “From Excel”), and then click The “Browse” button beside the File/URL field. Once you click on the, R Studio will automatically generate the file path - and the code to read the file, too - for you. You can copy this code or click Import to load the data. 5.7.4 Loading SAV Files The same factors that apply to reading Excel files apply to reading SAV files (from SPSS). NOte that you can also read CSV file directly into SPSS and so because of this and the benefits of using CSVs (they are simple files that work across platforms and software), we recommend using CSVs when possible. First, install the package haven, load it, and the use the function read_sav(): install.packages(&quot;haven&quot;) library(haven) my_data &lt;- read_sav(&quot;path/to/file.xlsx&quot;) 5.7.5 Google Sheets Finally, it can sometimes be useful to load a file directly from Google Sheets, and this can be done using the Google Sheets package. install.packages(&quot;googlesheets&quot;) library(googlesheets) When you run the command below, a link to authenticate with your Google account will open in your browser. my_sheets &lt;- gs_ls() You can then simply use the gs_title() function in conjunction with the gs_read() function: df &lt;- gs_title(&#39;title&#39;) df &lt;- gs_read(df) 5.7.6 Saving Files Using our data frame student_responses, we can save it as a CSV (for example) with the following function. The first argument, student_reponses, is the name of the object that you want to save. The second argument, student-responses.csv, what you want to call the saved dataset. write_csv(student_responses, &quot;student-responses.csv&quot;) That will save a CSV file entitled student-responses.csv in the working directory. If you want to save it to another directory, simply add the file path to the file, i.e. path/to/student-responses.csv. To save a file for SPSS, load the haven package and use write_sav(). There is not a function to save an Excel file, but you can save as a CSV and directly load it in Excel. 5.7.7 Conclusion We will detail the functions used to read every file in a folder (or, to write files to a folder). 5.8 Processing Data Now that we have loaded student_responses into an object, we can process it. This section highlights some common data processing functions. We’re also going to introduce a powerful, unusual operator in R, the pipe. The pipe is this symbol: %&gt;%. It lets you compose functions. It does this by passing the output of one function to the next. A handy shortcut for writing out %&gt;% is Command + Shift + M. Here’s an example. Let’s say that we want to select a few variables from the student_responses dataset and save those variables into a new object, student_mot_vars. Here’s how we would do that using dplyr::select(). student_mot_vars &lt;- # save object student_mot_vars by... student_responses %&gt;% # using dataframe student_responses select(SCIEEFF, JOYSCIE, INTBRSCI, EPIST, INSTSCIE) # and selecting only these five variables Note that we saved the output from the select() function to student_mot_vars but we could also save it back to student_responses, which would simply overwrite the original data frame (the following code is not run here): student_responses &lt;- # save object student_responses by... student_responses %&gt;% # using dataframe student_responses select(student_responses, SCIEEFF, JOYSCIE, INTBRSCI, EPIST, INSTSCIE) # and selecting only these five variables We can also rename the variables at the same time we select them. I put these on separate lines so I could add the comment, but you could do this all in the same line, too. It does not make a difference in terms of how select() will work. student_mot_vars &lt;- # save object student_mot_vars by... student_responses %&gt;% # using dataframe student_responses select(student_efficacy = SCIEEFF, # selecting variable SCIEEFF and renaming to student_efficiency student_joy = JOYSCIE, # selecting variable JOYSCIE and renaming to student_joy student_broad_interest = INTBRSCI, # selecting variable INTBRSCI and renaming to student_broad_interest student_epistemic_beliefs = EPIST, # selecting variable EPIST and renaming to student_epistemic_beliefs student_instrumental_motivation = INSTSCIE # selecting variable INSTSCIE and renaming to student_instrumental_motivation ) [will add more on creating new variables, filtering grouping and summarizing, and joining data sets] 5.9 Communicating / Sharing Results R Markdown is a highly convenient way to communicate and share results. Navigate to “New File” and then “R Markdown”. [add] Then, click “Knit to PDF”, “Knit to HTML”, or “Knit to Word”. 5.10 Other Foundational Notes 5.10.1 Configuring R Studio There are a number of changes you can (but do not need to) make to configure R Studio. If you navigate to the Preferences menu in R Studio, you’ll see a number of options you can change, from the appearance of the application to which windows appear where. One important consideration is whether to save your workspace when you close R Studio. By default, R Studio saves all of the objects in your environment. This means that any data that you have loaded–or new data or objects that you have created, such as by merging two data sets together or creating a plot–will, by default, still exist when you open R Studio next. In general, this is not ideal, because it means that you may have taken steps interactively that are not documented your code. This means that when you share your code, or re-run it from the start, it may not work. An easy way to change this is to tell R Studio to start from scratch (in terms of your workspace) each time you open it. You can do that by changing the dropdown menu pointed out in the image below to “Never”. optional caption text While this may seem like a dramatic step - never saving your workspace - it is the foundation for doing reproducible work and research using R Studio (and R). It also represents one of the biggest shifts from using software like Excel or SPSS, where most steps are not documented in code. This involves a shift from thinking that your most permanent and important part of an analysis is your data to thinking of the most important part as being the code: with the code, you can keep your data in its original form, process it, and then save a processed file, through running code. This also means that when you have to make a change to this code, you can re-run the entire analysis easily. 5.10.2 Getting Data In and Out clipr is a package to easily copy data into and out of R using the clipboard. [add more] datapasta is another option. [add more] "],
["education-dataset-analysis-pipeline-walkthrough-1.html", "6 Education Dataset Analysis Pipeline: Walkthrough #1 6.1 Background and Purpose 6.2 Processing the data 6.3 Viewing the data 6.4 Processing the pre-survey data 6.5 Processing the course data 6.6 Joining the data 6.7 Finding distinct cases at the student-level 6.8 Visualizations and Models 6.9 Linear model (regression) 6.10 What is next?", " 6 Education Dataset Analysis Pipeline: Walkthrough #1 6.1 Background and Purpose In the 2015-2016 and 2016-2017 school years, researchers carried out a study on students’ motivation to learn in online science classes. The online science classes were part of a statewide online course provider designed to supplement(and not replace) students’ enrollment in their local school. For example, students may choose to enroll in an online physics class because one was not offered at their school (or they were not able to take it given their schedule). The study involved a number of different data sources which were explored to understand students’ motivation: A self-report survey for three distinct but related aspects of students’ motivation Log-trace data, such as data output from the learning management system Discussion board data (not used in this walkthrough) Achievement-related (i.e., final grade) data Our purpose for this walkthrough is to begin to understand what explains students’ performance in these online courses. To do so, we will focus on a variable that was available through the learning management system used for the courses, on he amount of time sudents’ spent on the course. We will also explore how different (science) subjects as well as being in a particular class may help to explain student performance. First, these different data sources will be described in terms of how they were provided by the school. 6.1.1 Data Source #1: Self-report survey This was data collected before the start of the course via self-report survey. The survey included 10 items, each corresponding to one of three measures, namely, for interest, utility value, and perceived competence: I think this course is an interesting subject. (Interest) What I am learning in this class is relevant to my life. (Utility value) I consider this topic to be one of my best subjects. (Perceived competence) I am not interested in this course. (Interest - reverse coded) I think I will like learning about this topic. (Interest) I think what we are studying in this course is useful for me to know. (Utility value) I don’t feel comfortable when it comes to answering questions in this area. (Perceived competence) I think this subject is interesting. (Interest) I find the content of this course to be personally meaningful. (Utility value) I’ve always wanted to learn more about this subject. (Interest) 6.1.2 Data source #2: Log-trace data Log-trace data is data generated from our interactions with digital technologies, such as archived data from social media postings (see Chapter XXX and XXX). In education, an increasingly common source of log-trace data is that generated from interactions with learning management systems and other digital tools (Lazer et al., 2009; Salganik, 2018; Welser, Smith, Fisher, &amp; Gleave, 2008). The data for this walk-through is a summary of log-trace data, namely, the number of minutes students spent on the course. Thus, while this data is rich, you can imagine even more complex sources of log-trace data (i.e. timestamps associated with when students started and stopped accessing the course!). 6.1.3 Data source #3: Achievement-related and gradebook data This is a common source of data, namely, one associated with graded assignments students completed. In this walkthrough, we just examine students’ final grade. 6.1.4 Data source #4: Discussion board data Discussion board data is both rich and unstructured, in that it is primarily in the form of written text. We collected discussion board data, too, and highlight this as a potentially very rich data source. 6.2 Processing the data This analysis uses R packages, which are collections of R code that help users code more efficiently, as you wil recall from Chapter INTRODUCTORY. We load these packages with the function library. In particular, the packages we’ll use will help us load Excel files, organize the structure of the data, work with dates in the data, and navigate file directories. library(readxl) library(tidyverse) library(lubridate) library(here) library(dataedu) library(apaTables) library(sjPlot) This code chunk loads the log trace data using the read_csv function. Note that we call read_csv three times, once for each of the three logtrace datasets. We assign each of the datasets a name using &lt;-. # Gradebook and log-trace data for F15 and S16 semesters s12_course_data &lt;- read_csv(here( &quot;data&quot;, &quot;online-science-motivation&quot;, &quot;raw&quot;, &quot;s12-course-data.csv&quot; )) # Pre-survey for the F15 and S16 semesters s12_pre_survey &lt;- read_csv(here( &quot;data&quot;, &quot;online-science-motivation&quot;, &quot;raw&quot;, &quot;s12-pre-survey.csv&quot; )) # Log-trace data for F15 and S16 semesters - this is for time spent s12_time_spent &lt;- read_csv(here( &quot;data&quot;, &quot;online-science-motivation&quot;, &quot;raw&quot;, &quot;s12-course-minutes.csv&quot; )) 6.3 Viewing the data Now that we’ve successfully loaded all three logtrace datasets, we can visually inspect the data by typing the names that we assigned to each dataset. s12_pre_survey ## # A tibble: 1,102 x 16 ## RespondentId StartDate CompletedDate LanguageCode opdata_CourseID ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 426746 2015.08.… &lt;NA&gt; en FrScA-S116-01 ## 2 426775 2015.08.… 2015.08.24 1… en BioA-S116-01 ## 3 427483 2015.08.… &lt;NA&gt; en OcnA-S116-03 ## 4 429883 2015.09.… 2015.09.02 1… en AnPhA-S116-01 ## 5 430158 2015.09.… 2015.09.03 9… en AnPhA-S116-01 ## 6 430161 2015.09.… 2015.09.03 9… en AnPhA-S116-02 ## 7 430162 2015.09.… 2015.09.03 9… en AnPhA-T116-01 ## 8 430167 2015.09.… 2015.09.03 9… en BioA-S116-01 ## 9 430170 2015.09.… 2015.09.03 9… en BioA-T116-01 ## 10 430172 2015.09.… 2015.09.03 9… en PhysA-S116-01 ## # … with 1,092 more rows, and 11 more variables: opdata_username &lt;chr&gt;, ## # Q1MaincellgroupRow1 &lt;dbl&gt;, Q1MaincellgroupRow2 &lt;dbl&gt;, ## # Q1MaincellgroupRow3 &lt;dbl&gt;, Q1MaincellgroupRow4 &lt;dbl&gt;, ## # Q1MaincellgroupRow5 &lt;dbl&gt;, Q1MaincellgroupRow6 &lt;dbl&gt;, ## # Q1MaincellgroupRow7 &lt;dbl&gt;, Q1MaincellgroupRow8 &lt;dbl&gt;, ## # Q1MaincellgroupRow9 &lt;dbl&gt;, Q1MaincellgroupRow10 &lt;dbl&gt; s12_course_data ## # A tibble: 29,711 x 16 ## CourseSectionOr… Bb_UserPK EnrollmentStatus EnrollmentReason Gender ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 2 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 3 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 4 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 5 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 6 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 7 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 8 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 9 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 10 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## # … with 29,701 more rows, and 11 more variables: FinalGradeCEMS &lt;dbl&gt;, ## # Gradebook_Item &lt;chr&gt;, Item_Position &lt;dbl&gt;, Gradebook_Type &lt;chr&gt;, ## # Gradebook_Date &lt;chr&gt;, Grade_Category &lt;chr&gt;, Status &lt;lgl&gt;, ## # Points_Earned &lt;chr&gt;, Points_Attempted &lt;dbl&gt;, Points_Possible &lt;dbl&gt;, ## # last_access_date &lt;time&gt; s12_time_spent ## # A tibble: 598 x 6 ## CourseID CourseSectionID CourseSectionOrigID Bb_UserPK CUPK TimeSpent ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 27 17146 OcnA-S116-01 44638 190682 1383. ## 2 27 17146 OcnA-S116-01 54346 194259 1191. ## 3 27 17146 OcnA-S116-01 57981 196014 3343. ## 4 27 17146 OcnA-S116-01 66740 190463 965. ## 5 27 17146 OcnA-S116-01 67920 191593 4095. ## 6 27 17146 OcnA-S116-01 85355 190104 595. ## 7 27 17146 OcnA-S116-01 85644 190685 1632. ## 8 27 17146 OcnA-S116-01 86349 191713 1601. ## 9 27 17146 OcnA-S116-01 86460 191887 1891. ## 10 27 17146 OcnA-S116-01 87970 194256 3123. ## # … with 588 more rows 6.4 Processing the pre-survey data Often, survey data needs to be processed in order to be (most) useful. Here, we process the self-report items into three scales, for: interest, self-efficacy, and utility value. We do this by Renaming the question variables to something more managable Reversing the response scales on questions 4 and 7 Categorizing each question into a measure Computing the mean of each measure Let’s take these steps in order: Rename the question columns to something much simpler: s12_pre_survey &lt;- s12_pre_survey %&gt;% # Rename the qustions something easier to work with because R is case sensitive # and working with variable names in mix case is prone to error rename( q1 = Q1MaincellgroupRow1, q2 = Q1MaincellgroupRow2, q3 = Q1MaincellgroupRow3, q4 = Q1MaincellgroupRow4, q5 = Q1MaincellgroupRow5, q6 = Q1MaincellgroupRow6, q7 = Q1MaincellgroupRow7, q8 = Q1MaincellgroupRow8, q9 = Q1MaincellgroupRow9, q10 = Q1MaincellgroupRow10 ) %&gt;% # Convert all question responses to numeric mutate_at(vars(q1:q10), list( ~ as.numeric(.))) Let’s take a moment to discuss the dplyr function mutate_at. mutate_at is a version of mutate, which changes the values in an existing column or creates new columns. It’s useful in education datasets because you’ll often need to transform your data before analyzing it. Try this example, where we create a new total_students column by adding the number of male students and female students: # Dataset of students df &lt;- tibble( male = 5, female = 5 ) df %&gt;% mutate(total_students = male + female) ## # A tibble: 1 x 3 ## male female total_students ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 5 10 mutate_at is a special version of mutate, which conveniently changes the values of multiple columns. In our dataset s_12_pre_survey, we let mutate know we want to change the variables q1 through q10. We do this with the argument vars(q1:q10) Next we’ll reverse the scale of the survey responses on questions 4 and 7 so the responses for all questions can be interpreted in the same way. Rather than write a lot of code once to reverse the scales for question 4 then writing it again to reverse the scales on question 7, we’ll build a function that does that job for us. Then we’ll use the same function for question 4 and question 7. This will result in much less code, plus it will make it easier for us to change in the future. # This part of the code is where we write the function: # Function for reversing scales reverse_scale &lt;- function(question) { # Reverses the response scales for consistency # Args: # question: survey question # Returns: a numeric converted response # Note: even though 3 is not transformed, case_when expects a match for all # possible conditions, so it&#39;s best practice to label each possible input # and use TRUE ~ as the final statement returning NA for unexpected inputs x &lt;- case_when( question == 1 ~ 5, question == 2 ~ 4, question == 4 ~ 2, question == 5 ~ 1, question == 3 ~ 3, TRUE ~ NA_real_ ) x } # And here&#39;s where we use that function to reverse the scales # Reverse scale for questions 4 and 7 s12_pre_survey &lt;- s12_pre_survey %&gt;% mutate(q4 = reverse_scale(q4), q7 = reverse_scale(q7)) We’ll accomplish the last two steps in one chunk of code. First we’ll create a column called measure and we’ll fill that column with one of three question categories: int: interest uv: utility value pc: self efficacy After that we’ll find the mean response of each category using mean function. # Add measure variable s12_measure_mean &lt;- s12_pre_survey %&gt;% # Gather questions and responses pivot_longer(cols = q1:q10, names_to = &quot;question&quot;, values_to = &quot;response&quot;) %&gt;% # Here&#39;s where we make the column of question categories mutate( measure = case_when( question %in% c(&quot;q1&quot;, &quot;q4&quot;, &quot;q5&quot;, &quot;q8&quot;, &quot;q10&quot;) ~ &quot;int&quot;, question %in% c(&quot;q2&quot;, &quot;q6&quot;, &quot;q9&quot;) ~ &quot;uv&quot;, question %in% c(&quot;q3&quot;, &quot;q7&quot;) ~ &quot;pc&quot;, TRUE ~ NA_character_ ) ) %&gt;% group_by(measure) %&gt;% summarise(# Here&#39;s where we compute the mean of the responses # Mean response for each measure mean_response = mean(response, na.rm = TRUE), # Percent of each measure that had NAs in the response field percent_NA = mean(is.na(response))) s12_measure_mean ## # A tibble: 3 x 3 ## measure mean_response percent_NA ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 int 4.26 0.171 ## 2 pc 3.65 0.170 ## 3 uv 3.76 0.170 We will use a similar process later to calculate these variables’ correlations. 6.5 Processing the course data We also can process the course data in order to create new variables which we can use in analyses. This led to pulling out the subject, semester, and section from the course ID; variables that we can use later on. # split course section into components s12_course_data &lt;- s12_course_data %&gt;% separate( col = CourseSectionOrigID, into = c(&quot;subject&quot;, &quot;semester&quot;, &quot;section&quot;), sep = &quot;-&quot;, remove = FALSE ) 6.6 Joining the data To join the course data and pre-survey data, we need to create similar keys. In other words, our goal here is to have one variable that matches across both datasets, so that we can merge the datasets on the basis of that variable. For these data, both have variables for the course and the student, though they have different names in each. Our first goal will be to rename two variables in each of our datasets so that they will match. One variable will correspond to the course, and the other will correspond to the student. We are not changing anything in the data itself at this step - instead, we are just cleaning it up so that we can look at the data all in one place. Let’s start with the pre-survey data. We will rename RespondentID and opdata_CourseID to be student_id and course_id, respectively. s12_pre_survey &lt;- s12_pre_survey %&gt;% rename(student_id = RespondentId, course_id = opdata_CourseID) s12_pre_survey ## # A tibble: 1,102 x 16 ## student_id StartDate CompletedDate LanguageCode course_id opdata_username ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 426746 2015.08.… &lt;NA&gt; en FrScA-S1… _80624_1 ## 2 426775 2015.08.… 2015.08.24 1… en BioA-S11… _80623_1 ## 3 427483 2015.08.… &lt;NA&gt; en OcnA-S11… _82588_1 ## 4 429883 2015.09.… 2015.09.02 1… en AnPhA-S1… _80623_1 ## 5 430158 2015.09.… 2015.09.03 9… en AnPhA-S1… _80624_1 ## 6 430161 2015.09.… 2015.09.03 9… en AnPhA-S1… _80624_1 ## 7 430162 2015.09.… 2015.09.03 9… en AnPhA-T1… _80624_1 ## 8 430167 2015.09.… 2015.09.03 9… en BioA-S11… _80624_1 ## 9 430170 2015.09.… 2015.09.03 9… en BioA-T11… _80624_1 ## 10 430172 2015.09.… 2015.09.03 9… en PhysA-S1… _80624_1 ## # … with 1,092 more rows, and 10 more variables: q1 &lt;dbl&gt;, q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, ## # q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;, q7 &lt;dbl&gt;, q8 &lt;dbl&gt;, q9 &lt;dbl&gt;, q10 &lt;dbl&gt; Looks better now! Let’s proceed to the course data. Our goal is to rename two variables that correspond to the course and the student so that we can match with the other variables we just created for the pre-survey data. s12_course_data &lt;- s12_course_data %&gt;% rename(student_id = Bb_UserPK, course_id = CourseSectionOrigID) Now that we have two variables that are consistent across both datasets - we have called them “course_id” and “student_id” - we can join these using the dplyr function, left_join(). Let’s save our joined data as a new object called “dat.” dat &lt;- left_join(s12_course_data, s12_pre_survey, by = c(&quot;student_id&quot;, &quot;course_id&quot;)) dat ## # A tibble: 29,711 x 33 ## course_id subject semester section student_id EnrollmentStatus ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 2 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 3 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 4 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 5 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 6 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 7 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 8 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 9 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 10 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## # … with 29,701 more rows, and 27 more variables: EnrollmentReason &lt;chr&gt;, ## # Gender &lt;chr&gt;, FinalGradeCEMS &lt;dbl&gt;, Gradebook_Item &lt;chr&gt;, ## # Item_Position &lt;dbl&gt;, Gradebook_Type &lt;chr&gt;, Gradebook_Date &lt;chr&gt;, ## # Grade_Category &lt;chr&gt;, Status &lt;lgl&gt;, Points_Earned &lt;chr&gt;, ## # Points_Attempted &lt;dbl&gt;, Points_Possible &lt;dbl&gt;, last_access_date &lt;time&gt;, ## # StartDate &lt;chr&gt;, CompletedDate &lt;chr&gt;, LanguageCode &lt;chr&gt;, ## # opdata_username &lt;chr&gt;, q1 &lt;dbl&gt;, q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, ## # q6 &lt;dbl&gt;, q7 &lt;dbl&gt;, q8 &lt;dbl&gt;, q9 &lt;dbl&gt;, q10 &lt;dbl&gt; Just one more data frame to merge: s12_time_spent &lt;- s12_time_spent %&gt;% rename(student_id = Bb_UserPK, course_id = CourseSectionOrigID) s12_time_spent &lt;- s12_time_spent %&gt;% mutate(student_id = as.integer(student_id)) dat &lt;- dat %&gt;% left_join(s12_time_spent, by = c(&quot;student_id&quot;, &quot;course_id&quot;)) Note that they’re now combined, even though the course data has many more rows: The pre_survey data has been joined for each student by course combination. We have a pretty large data frame! Let’s take a quick look. dat ## # A tibble: 29,711 x 37 ## course_id subject semester section student_id EnrollmentStatus ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 2 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 3 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 4 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 5 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 6 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 7 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 8 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 9 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 10 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## # … with 29,701 more rows, and 31 more variables: EnrollmentReason &lt;chr&gt;, ## # Gender &lt;chr&gt;, FinalGradeCEMS &lt;dbl&gt;, Gradebook_Item &lt;chr&gt;, ## # Item_Position &lt;dbl&gt;, Gradebook_Type &lt;chr&gt;, Gradebook_Date &lt;chr&gt;, ## # Grade_Category &lt;chr&gt;, Status &lt;lgl&gt;, Points_Earned &lt;chr&gt;, ## # Points_Attempted &lt;dbl&gt;, Points_Possible &lt;dbl&gt;, last_access_date &lt;time&gt;, ## # StartDate &lt;chr&gt;, CompletedDate &lt;chr&gt;, LanguageCode &lt;chr&gt;, ## # opdata_username &lt;chr&gt;, q1 &lt;dbl&gt;, q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, ## # q6 &lt;dbl&gt;, q7 &lt;dbl&gt;, q8 &lt;dbl&gt;, q9 &lt;dbl&gt;, q10 &lt;dbl&gt;, CourseID &lt;dbl&gt;, ## # CourseSectionID &lt;dbl&gt;, CUPK &lt;dbl&gt;, TimeSpent &lt;dbl&gt; It looks like we have nearly 30,000 observations from 30 variables. There is one last step to take. Were we interested in a fine-grained analysis of how students performed (according to the teacher) on different assignments (see the Gradebook_Item column), we would keep all (29,711 rows of) the data. But, our goal (for now) is more modest: to calculate the percentage of points students earned as a measure of their final grade (noting that the teacher may have assigned a different grade–or weighted their grades in ways not reflected through the points). dat &lt;- dat %&gt;% group_by(student_id, course_id) %&gt;% mutate(Points_Earned = as.integer(Points_Earned)) %&gt;% summarize( total_points_possible = sum(Points_Possible, na.rm = TRUE), total_points_earned = sum(Points_Earned, na.rm = TRUE) ) %&gt;% mutate(percentage_earned = total_points_earned / total_points_possible) %&gt;% ungroup() %&gt;% left_join(dat) # note that we join this back to the original data frame to retain all of the variables 6.7 Finding distinct cases at the student-level This last step calculated a new column, for the percentage of points each student earned. That value is the same for the same student (an easy way we would potentially use to check this is View(), i.e., View(dat)). But–because we are not carrying out a finer-grained analysis using the Gradebook_Item–the duplicate rows are not necessary. We only want variables at the student-level (and not at the level of different gradebook items). We can do this using the distinct() function. This function takes the name of the data frame and the name of the variables used to determine what counts as a unique case. Another thing to note about distinct() is that it will only return the variable(s) (we note that you can pass more than one variable to distinct()) you used to determine uniqueness, unless you include the argument .keep_all = TRUE. For the sake of making it very easy to view the output, we omit this argument (only for now). Were we to run distinct(dat, Gradebook_Item), what do you think would be returned? distinct(dat, Gradebook_Item) ## # A tibble: 222 x 1 ## Gradebook_Item ## &lt;chr&gt; ## 1 POINTS EARNED &amp; TOTAL COURSE POINTS ## 2 WORK ATTEMPTED ## 3 0-1.1: Intro Assignment - Send a Message to Your Instructor ## 4 0-1.2: Intro Assignment - DB #1 ## 5 0-1.3: Intro Assignment - Submitting Files ## 6 1-1.1: Lesson 1-1 Graphic Organizer ## 7 1-2.1: Explore a Career Assignment ## 8 1-2.2: Explore a Career DB #2 ## 9 PROGRESS CHECK 1 @ 02-18-16 ## 10 1-2.3: Lesson 1-2 Graphic Organizer ## # … with 212 more rows What is every distinct gradebook item is what is returned. You might be wondering (as we were) whether some gradebook items have the same values across courses; we can return the unique combination of courses and gradebook items by simply adding another variable to distinct(): distinct(dat, course_id, Gradebook_Item) ## # A tibble: 1,269 x 2 ## course_id Gradebook_Item ## &lt;chr&gt; &lt;chr&gt; ## 1 FrScA-S216-02 POINTS EARNED &amp; TOTAL COURSE POINTS ## 2 FrScA-S216-02 WORK ATTEMPTED ## 3 FrScA-S216-02 0-1.1: Intro Assignment - Send a Message to Your Instructor ## 4 FrScA-S216-02 0-1.2: Intro Assignment - DB #1 ## 5 FrScA-S216-02 0-1.3: Intro Assignment - Submitting Files ## 6 FrScA-S216-02 1-1.1: Lesson 1-1 Graphic Organizer ## 7 FrScA-S216-02 1-2.1: Explore a Career Assignment ## 8 FrScA-S216-02 1-2.2: Explore a Career DB #2 ## 9 FrScA-S216-02 PROGRESS CHECK 1 @ 02-18-16 ## 10 FrScA-S216-02 1-2.3: Lesson 1-2 Graphic Organizer ## # … with 1,259 more rows It looks like a lot of gradebook items were repeated - likely across the different sections of the same course (we would be curious to hear what you find if you investigate this!). Let’s use what we just did, but to find the unique values at the student-level. Thus, instead of exploring unique gradebook items, we will explore unique students (still accounting for the course, as students could enroll in more than one course.) This time, we will add the keep_all = TRUE argument. dat &lt;- distinct(dat, course_id, student_id, .keep_all = TRUE) This is a much smaller data frame - with one row for each sudnet in the course (instead of the 29,701 rows which we would be interested in were we analyzing this data at the level of specific students’ grades for specific gradebook items). Now that our data are ready to go, we can start to ask some questions of the data, 6.8 Visualizations and Models 6.8.1 The relationship between time spent on course and percentage of points earned One thing we might be wondering is how time spent on course is related to students’ final grade. We note that ggplot2, which we use to create these plots, is discussed further in chapter XXX. dat %&gt;% ggplot(aes(x = TimeSpent, y = percentage_earned)) + # this tells ggplot2 what variables to map to what feature of a plot, here, x- anx y-axis locations geom_point() + # creates a point with x- and y-axis coordinates specified above theme_dataedu() There appears to be some relationship. What if we added a line of best fit - a linear model? dat %&gt;% ggplot(aes(x = TimeSpent, y = percentage_earned)) + geom_point() + # same as above geom_smooth(method = &quot;lm&quot;) + # this adds a line of best fit; method = &quot;lm&quot; tells ggplot2 to make the line linear, whereas the default is a smooth like theme_dataedu() So, it appeares that the more time students spent on the course, the more points they earned. 6.9 Linear model (regression) We can find out exactly what the relationship is using a linear model. We also discuss linear models in walkthrough XXX. Let’s use this technique to model the relationship between the time spent on the course and the percentage of points earned. Here, we predict percentage_earned, or the percentage of the total points that are possible for a student to earn. Here, percentage earned is the dependent, or y-variable, and so we enter it first, after the lm() command, before the tilde (~) symbol. To the right of the tilde is one independent variable, TimeSpent, or the time that students spent on the course. We also pass the data frame, dat. At this point, we’re ready to run the model. Let’s run this line of code and save the results to an object - we chose m_linear, but any name will work, as well as the summary() function on the output. m_linear &lt;- lm(percentage_earned ~ TimeSpent, data = dat) Another way that we can generate table output is with a function from the sjPlot package, tab_model. sjPlot::tab_model(m_linear) percentage earned Predictors Estimates CI p (Intercept) 0.62 0.59 – 0.65 &lt;0.001 TimeSpent 0.00 0.00 – 0.00 &lt;0.001 Observations 598 R2 / R2 adjusted 0.191 / 0.189 This will work well for R Markdown documents (or simply to interpet the model in R). If you want to save the model for use in a Word document, the apaTables package may be helpful; just pass the name of the regression model, like we did with sjPlot::tab_model(), as well as a file name that ends in .doc to the filename argument, i.e.: apaTables::apa.reg.table(m_linear) ## ## ## Regression results using percentage_earned as the criterion ## ## ## Predictor b b_95%_CI beta beta_95%_CI sr2 sr2_95%_CI r ## (Intercept) 0.62** [0.59, 0.65] ## TimeSpent 0.00** [0.00, 0.00] 0.44 [0.36, 0.51] .19 [.14, .24] .44** ## ## ## ## Fit ## ## ## R2 = .191** ## 95% CI[.14,.24] ## ## ## Note. A significant b-weight indicates the beta-weight and semi-partial correlation are also significant. ## b represents unstandardized regression weights. beta indicates the standardized regression weights. ## sr2 represents the semi-partial correlation squared. r represents the zero-order correlation. ## Square brackets are used to enclose the lower and upper limits of a confidence interval. ## * indicates p &lt; .05. ** indicates p &lt; .01. ## Helpfully, you can save the output to a Word document, by adding a filename argument: apaTables::apa.reg.table(m_summary, filename = &quot;regression-table-output.doc&quot;) You might be wondering what else the apaTables package does; we encourage you to read more about the package here: https://cran.r-project.org/web/packages/apaTables/index.html. The vignette is especially helpful. One function that may be useful for writing manuscripts is the following function for creating correlation tables; the function takes, as an input, a data frame with the variables for which you wish to calculate correlations. Before we proceed to the next code chunk, let’s talk about some functions we’ll be using a lot in this book. filter, group_by, and summarise are functions in the dplyr package that you will see a lot in upcoming chapters. filter removes rows from the dataset that don’t match a criteria. Use it for tasks like only keeping records for students in the fifth grade group_by groups records together so you can perform operations on those groups instead of on the entire dataset. Use it for tasks like getting the mean test score of each school instead of a whole school district summarize and summarise reduce your dataset down to a summary statistic. Use it for tasks like turning a datset of student test scores into a datset of grade levels and their mean test score So let’s use these dplyr functions on our survey analysis. We will create the same measures (based on the survey items) that we used earlier to understand how they relate to one another: survey_responses &lt;- s12_pre_survey %&gt;% # Gather questions and responses pivot_longer(cols = q1:q10, names_to = &quot;question&quot;, values_to = &quot;response&quot;) %&gt;% mutate( # Here&#39;s where we make the column of question categories measure = case_when( question %in% c(&quot;q1&quot;, &quot;q4&quot;, &quot;q5&quot;, &quot;q8&quot;, &quot;q10&quot;) ~ &quot;int&quot;, question %in% c(&quot;q2&quot;, &quot;q6&quot;, &quot;q9&quot;) ~ &quot;uv&quot;, question %in% c(&quot;q3&quot;, &quot;q7&quot;) ~ &quot;pc&quot;, TRUE ~ NA_character_ ) ) %&gt;% group_by(student_id, measure) %&gt;% summarise(# Here&#39;s where we compute the mean of the responses # Mean response for each measure mean_response = mean(response, na.rm = TRUE)) %&gt;% filter(!is.na(mean_response)) %&gt;% spread(measure, mean_response) survey_responses ## # A tibble: 920 x 4 ## # Groups: student_id [920] ## student_id int pc uv ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 429883 1.8 3 1 ## 2 430158 NA NA 1 ## 3 430161 NA 1 NA ## 4 431821 4.4 3.5 4 ## 5 431864 4.4 5 3 ## 6 431889 3.8 3 3.67 ## 7 431890 5 4 3.67 ## 8 431909 4 3 3 ## 9 431949 3.8 3.5 3.33 ## 10 431956 4 4 3.67 ## # … with 910 more rows Now that we’ve prepared the survey responses, we can use the apa.cor.table() function: survey_responses %&gt;% apa.cor.table() ## ## ## Means, standard deviations, and correlations with confidence intervals ## ## ## Variable M SD 1 2 3 ## 1. student_id 461015.15 29419.03 ## ## 2. int 4.26 0.63 -.11** ## [-.17, -.05] ## ## 3. pc 3.65 0.72 -.09** .59** ## [-.15, -.02] [.55, .63] ## ## 4. uv 3.76 0.81 -.09** .61** .52** ## [-.15, -.02] [.57, .65] [.47, .56] ## ## ## Note. M and SD are used to represent mean and standard deviation, respectively. ## Values in square brackets indicate the 95% confidence interval. ## The confidence interval is a plausible range of population correlations ## that could have caused the sample correlation (Cumming, 2014). ## * indicates p &lt; .05. ** indicates p &lt; .01. ## The time spent variable is on a very large scale (minutes); what if we transform it to represent the number of hours that students spent on the course? Let’s use the mutate() function we used earlier. We’ll end the variable name in _hours, to represent what this variable means. # creating a new variable for the amount of time spent in hours dat &lt;- dat %&gt;% mutate(TimeSpent_hours = TimeSpent / 60) # the same linear model as above, but with the TimeSpent variable in hours m_linear_1 &lt;- lm(percentage_earned ~ TimeSpent_hours, data = dat) # viewing the output of the linear model sjPlot::tab_model(m_linear_1) percentage earned Predictors Estimates CI p (Intercept) 0.62 0.59 – 0.65 &lt;0.001 TimeSpent_hours 0.00 0.00 – 0.01 &lt;0.001 Observations 598 R2 / R2 adjusted 0.191 / 0.189 The scale still does not seem quite right. What if we standardized the variable to have a mean of zero and a standard deviation of one? # this is to standardize the TimeSpent variable to have a mean of zero and a standard deviation of 1 dat &lt;- dat %&gt;% mutate(TimeSpent_std = scale(TimeSpent)) # the same linear model as above, but with the TimeSpent variable standardized m_linear_2 &lt;- lm(percentage_earned ~ TimeSpent_std, data = dat) # viewing the output of the linear model sjPlot::tab_model(m_linear_2) percentage earned Predictors Estimates CI p (Intercept) 0.76 0.74 – 0.78 &lt;0.001 TimeSpent_std 0.11 0.09 – 0.13 &lt;0.001 Observations 598 R2 / R2 adjusted 0.191 / 0.189 That seems to make more sense. However, there is a different interpretation now for the time spent variable: for every one standard deviation increase in the amount of time spent on the course, the percentage of points a student earns increases by .11, or 11 percentage points. Let’s extend our regression model: what other variables may matter? Perhaps there are differences based on the subject of the course. We can add subject as a variable easily:it l # a linear model with the subject added # independent variables, such as TimeSpent_std and subject, can simply be separated with a plus symbol: m_linear_3 &lt;- lm(percentage_earned ~ TimeSpent_std + subject, data = dat) We can use sjPlot::tab_model() once again to view the results: sjPlot::tab_model(m_linear_3) percentage earned Predictors Estimates CI p (Intercept) 0.70 0.66 – 0.73 &lt;0.001 TimeSpent_std 0.12 0.10 – 0.14 &lt;0.001 subject (BioA) -0.00 -0.08 – 0.07 0.940 subject (FrScA) 0.11 0.07 – 0.16 &lt;0.001 subject (OcnA) 0.00 -0.05 – 0.05 0.963 subject (PhysA) 0.18 0.12 – 0.25 &lt;0.001 Observations 598 R2 / R2 adjusted 0.260 / 0.254 It looks like subject FrSc - forensic science - and subject Ocn - oceanography - are associated with a higher percentage of points earned, overall. This indicates that students in those two classes earned higher grades than students in other science classes in this dataset. 6.10 What is next? In the follow-up to this walkthrough (see Chapter XXX), we will focus on visualizing and then modeling the data using an advanced methodological technique, multi-level models. Before we go, let’s save the data we processed. write_csv(dat, &quot;data/online-science-motivation/processed/dat.csv&quot;) "],
["walkthrough-1b.html", "7 Walkthrough #1B 7.1 But what about different courses? 7.2 Toward multi-level models 7.3 Other groups", " 7 Walkthrough #1B This walkthrough accompanies walkthrough XXX, which focused on preparing the data and beginning to viusualize and model the data. Here, we focus on an extension of the models we ran, one focused on how to address the fact that students in our dataset shared classes. As for the earlier walkthrough using the same data, the purpose for this walkthrough is to explore students’ performance in these online courses, focusing on the time spent in the course (made available throygh the learning management system) and the effects of being in a particular class. First, let’s load the tidyverse and read the data we processed in walkthrough XXX. library(tidyverse) library(dummies) library(sjPlot) library(lme4) library(performance) dat &lt;- read_csv(&quot;data/online-science-motivation/processed/dat.csv&quot;) 7.1 But what about different courses? Are there course-specific differences in how much time students spend on the course as well as in how time spent is related to the percentage of points students earned? There are a number of ways to approach this question. Let’s use our linear model. Specifically, we can dummy-code the groups. Dummy coding means transforming a variable with multiple categories into multiple, new variables, where each variable indicates the presence and absence of only one of the categories. 7.1.1 The role of dummy codes We can see how dummy coding works through using the {dummies} package, though, as we will see, you often do not need to manually dummy code variables like this. Let’s consider the iris data that comes built into R, but, since we are fans of the {tidyverse}, we will first change it into a tibble. iris &lt;- as_tibble(iris) iris ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows As we can see above, the Species variable is a factor. If we consider how we could include a variable such as this in a linear model, things may become a little confusing. Species seems to be made up of, well, words, such as “setosa.” The common way to approach this is through dummy coding, where you create new variables for each of the possible values of Species (such as “setosa”). Then, these new variables have a value of 1 when the row is associated with that level (i.e., the first row in the data frame above would have a 1 for a column named setosa). Let’s return to {dummies}. How many possible values are there for Species? We can check with the levels function. levels(iris$Species) ## [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; When we run the dummy() function on the Species variable, we can see that it returns three variables, one for each of the three levels of Species - “setosa”, “versicolor”, and “virginica”. dummies::dummy(iris$Species) %&gt;% head() ## Warning in model.matrix.default(~x - 1, model.frame(~x - 1), contrasts = FALSE): ## non-list contrasts argument ignored ## {\\n args = commandArgs(TRUE)\\n out = do.call(rmarkdown::render, c(args[1], readRDS(args[2]), list(run_pandoc = FALSE, encoding = &quot;UTF-8&quot;)))\\n out_expected = xfun::with_ext(args[1], &quot;.md&quot;)\\n if (out != out_expected) {\\n file.rename(out, out_expected)\\n attributes(out_expected) = attributes(out)\\n out = out_expected\\n }\\n if (file.exists(args[3])) {\\n res = readRDS(args[3])\\n res[[args[1]]] = out\\n saveRDS(res, args[3])\\n }\\n else saveRDS(setNames(list(out), args[1]), args[3])\\n}setosa ## [1,] 1 ## [2,] 1 ## [3,] 1 ## [4,] 1 ## [5,] 1 ## [6,] 1 ## {\\n args = commandArgs(TRUE)\\n out = do.call(rmarkdown::render, c(args[1], readRDS(args[2]), list(run_pandoc = FALSE, encoding = &quot;UTF-8&quot;)))\\n out_expected = xfun::with_ext(args[1], &quot;.md&quot;)\\n if (out != out_expected) {\\n file.rename(out, out_expected)\\n attributes(out_expected) = attributes(out)\\n out = out_expected\\n }\\n if (file.exists(args[3])) {\\n res = readRDS(args[3])\\n res[[args[1]]] = out\\n saveRDS(res, args[3])\\n }\\n else saveRDS(setNames(list(out), args[1]), args[3])\\n}versicolor ## [1,] 0 ## [2,] 0 ## [3,] 0 ## [4,] 0 ## [5,] 0 ## [6,] 0 ## {\\n args = commandArgs(TRUE)\\n out = do.call(rmarkdown::render, c(args[1], readRDS(args[2]), list(run_pandoc = FALSE, encoding = &quot;UTF-8&quot;)))\\n out_expected = xfun::with_ext(args[1], &quot;.md&quot;)\\n if (out != out_expected) {\\n file.rename(out, out_expected)\\n attributes(out_expected) = attributes(out)\\n out = out_expected\\n }\\n if (file.exists(args[3])) {\\n res = readRDS(args[3])\\n res[[args[1]]] = out\\n saveRDS(res, args[3])\\n }\\n else saveRDS(setNames(list(out), args[1]), args[3])\\n}virginica ## [1,] 0 ## [2,] 0 ## [3,] 0 ## [4,] 0 ## [5,] 0 ## [6,] 0 We can confirm that every row associated with a specific species has a 1 in the column it corresponds to. We can do this by binding together the dummy codes and the iris data and then counting, for each of the three species, how many of the rows for each dummy code were coded with a “1”. For example, when the Species is “setosa”, the variable Speciessetosa always equals 1 - as is the case for the other species (for their respective variables). bind_cols() is a useful tidyverse function for binding together data frames by column. species_dummy_coded &lt;- dummies::dummy(iris$Species) ## Warning in model.matrix.default(~x - 1, model.frame(~x - 1), contrasts = FALSE): ## non-list contrasts argument ignored species_dummy_coded &lt;- as_tibble(species_dummy_coded) iris_with_dummy_codes &lt;- bind_cols(iris, species_dummy_coded) Let’s look at the results. iris_with_dummy_codes %&gt;% count(Species, Speciessetosa, Speciesversicolor, Speciesvirginica) Okay, this covers how dummy codes work: but, how do they work when used in a model, like with the linear model we have been using (through lm())? In the context of using lm() (and many other functions in R) is that the number of levels to be created is always the number of different possible values minus one, because each group will be modeled in comparison to the group without a column, or what is commonly called the reference group. Why can every group not simply have their own dummy-coded column? The reason has to do with how the dummy codes are used. The purpose of the dummy code is to show how different the dependent variable is for all of the observations that are in one group (i.e., all of the flowers that are setosa specimens). In order to represent how different those flowers are, they have to be compared to something else - and the intercept of the model usually represents this “something else.” However, if every level of a factor (such as Species) is dummy-coded, then there would be no cases available to estimate an intercept - in short, the dummy code would not be compared to anything else. For this reason, one group is typically selected as the reference group, to which every other group is compared. 7.1.2 Using dummy codes This may be clearer with an example. Let’s return to our online science class data and consider the effect (for a student) of being in a specific class in the data set. First, let’s determine how many classes there are. We can use the count() function. dat %&gt;% count(course_id) %&gt;% nrow() ## [1] 26 There are 26 distinct courses. We will save this output to m_linear_courses, where the dc stands for dummy code. We will keep the variables we used in our last set of models - TimeSpent and subject - as independent variables. m_linear_dc &lt;- lm(percentage_earned ~ TimeSpent_std + course_id, data = dat) The output will be a bit, well, long, because each group will have its own intercept. Here it is: sjPlot::tab_model(m_linear_dc) percentage earned Predictors Estimates CI p (Intercept) 0.73 0.67 – 0.80 &lt;0.001 TimeSpent_std 0.12 0.10 – 0.14 &lt;0.001 course_id (AnPhA-S116-02) -0.01 -0.11 – 0.09 0.829 course_id (AnPhA-S216-01) -0.09 -0.18 – 0.00 0.062 course_id (AnPhA-S216-02) -0.09 -0.21 – 0.03 0.146 course_id (AnPhA-T116-01) 0.02 -0.12 – 0.16 0.802 course_id (BioA-S116-01) -0.03 -0.13 – 0.06 0.485 course_id (BioA-S216-01) -0.12 -0.30 – 0.07 0.207 course_id (BioA-T116-01) 0.09 -0.21 – 0.39 0.556 course_id (FrScA-S116-01) 0.12 0.03 – 0.20 0.006 course_id (FrScA-S116-02) -0.01 -0.17 – 0.14 0.880 course_id (FrScA-S116-03) 0.02 -0.08 – 0.11 0.696 course_id (FrScA-S116-04) -0.08 -0.28 – 0.11 0.398 course_id (FrScA-S216-01) 0.10 0.02 – 0.18 0.017 course_id (FrScA-S216-02) 0.04 -0.06 – 0.15 0.408 course_id (FrScA-S216-03) -0.14 -0.39 – 0.11 0.273 course_id (FrScA-S216-04) 0.16 -0.03 – 0.36 0.102 course_id (FrScA-T116-01) 0.06 -0.16 – 0.28 0.597 course_id (OcnA-S116-01) 0.02 -0.09 – 0.12 0.775 course_id (OcnA-S116-02) -0.00 -0.13 – 0.12 0.969 course_id (OcnA-S116-03) -0.16 -0.59 – 0.26 0.447 course_id (OcnA-S216-01) -0.10 -0.19 – -0.01 0.038 course_id (OcnA-S216-02) -0.00 -0.12 – 0.11 0.942 course_id (OcnA-T116-01) -0.01 -0.17 – 0.15 0.910 course_id (PhysA-S116-01) 0.17 0.08 – 0.26 &lt;0.001 course_id (PhysA-S216-01) 0.06 -0.07 – 0.18 0.369 course_id (PhysA-T116-01) 0.20 -0.10 – 0.50 0.190 Observations 598 R2 / R2 adjusted 0.300 / 0.268 Wow! That is a lot of effects. In addition to the time spent and subject variables, the model estimated the difference, accounting for the effects of being a student in a specific class. Let’s count how many classes there are. If we count the number of classes, we see that there are 25 - and not 26! One has been automatically selected as the reference group, and every other class’s coefficient represents how different each class is from it. The intercept’s value of 0.74 represents the percentage of points that students in the reference group class, which is automatically the first level of the course_id variable when it is converted to a factor, “course_idAnPhA-S116-01” (which represents an anatomy and physiology course from semester S1 (for the fall) of 2016; this is the first section (01)). We can easily choose another class to serve as a reference group. Imagine, for example, that we want “course_idPhysA-S116-01” (the first section of the physics class offered during this semester and year) to be the reference group. The fct_relevel() function (which is a part of the {tidyverse} suite of packages) makes it easy to do this. This function allows us to re-order the levels within a factor, so that the “first” level will change. We’ll also use mutate again here, which we introduced in the previous chapter. dat &lt;- dat %&gt;% mutate(course_id = fct_relevel(course_id, &quot;course_idPhysA-S116-01&quot;)) ## Warning: Unknown levels in `f`: course_idPhysA-S116-01 We can now see that that group is no longer listed as an independent variable, or a predictor: every coefficient in this model is now in reference to it. # Here we run a linear model again, predicting percentage earned in the course # The predictor variables are the (standardized) amount of time spent and the subject of the course (course_id) m_linear_dc_1 &lt;- lm(percentage_earned ~ TimeSpent_std + course_id, data = dat) sjPlot::tab_model(m_linear_dc_1) percentage earned Predictors Estimates CI p (Intercept) 0.73 0.67 – 0.80 &lt;0.001 TimeSpent_std 0.12 0.10 – 0.14 &lt;0.001 course_id (AnPhA-S116-02) -0.01 -0.11 – 0.09 0.829 course_id (AnPhA-S216-01) -0.09 -0.18 – 0.00 0.062 course_id (AnPhA-S216-02) -0.09 -0.21 – 0.03 0.146 course_id (AnPhA-T116-01) 0.02 -0.12 – 0.16 0.802 course_id (BioA-S116-01) -0.03 -0.13 – 0.06 0.485 course_id (BioA-S216-01) -0.12 -0.30 – 0.07 0.207 course_id (BioA-T116-01) 0.09 -0.21 – 0.39 0.556 course_id (FrScA-S116-01) 0.12 0.03 – 0.20 0.006 course_id (FrScA-S116-02) -0.01 -0.17 – 0.14 0.880 course_id (FrScA-S116-03) 0.02 -0.08 – 0.11 0.696 course_id (FrScA-S116-04) -0.08 -0.28 – 0.11 0.398 course_id (FrScA-S216-01) 0.10 0.02 – 0.18 0.017 course_id (FrScA-S216-02) 0.04 -0.06 – 0.15 0.408 course_id (FrScA-S216-03) -0.14 -0.39 – 0.11 0.273 course_id (FrScA-S216-04) 0.16 -0.03 – 0.36 0.102 course_id (FrScA-T116-01) 0.06 -0.16 – 0.28 0.597 course_id (OcnA-S116-01) 0.02 -0.09 – 0.12 0.775 course_id (OcnA-S116-02) -0.00 -0.13 – 0.12 0.969 course_id (OcnA-S116-03) -0.16 -0.59 – 0.26 0.447 course_id (OcnA-S216-01) -0.10 -0.19 – -0.01 0.038 course_id (OcnA-S216-02) -0.00 -0.12 – 0.11 0.942 course_id (OcnA-T116-01) -0.01 -0.17 – 0.15 0.910 course_id (PhysA-S116-01) 0.17 0.08 – 0.26 &lt;0.001 course_id (PhysA-S216-01) 0.06 -0.07 – 0.18 0.369 course_id (PhysA-T116-01) 0.20 -0.10 – 0.50 0.190 Observations 598 R2 / R2 adjusted 0.300 / 0.268 Using dummy codes is very common - they are used in nearly every case in which you are using a model (such as a linear model, through lm()) and you have variables that are factors. A benefit of using lm() (and many other functions) in R for modeling, such as the lme4::lmer() function we discuss later, is that if you have variables which are not factors, but simply character strings, they will be automatically changed to factors when used in a model. This means, for instance, that if you have a variable for the subject matter of courses labeled “mathematics”, “science”, “english language” (typed like that!), “social studies”, and “art”, and you include this variable in an lm() model, then the function will automatically dummy-code these for you. The only essential step that is not taken for you is choosing which is the reference group. We note that there are cases in which not having a reference group that the other, dummy-coded groups are compared to is desired. In such cases, no intercept is estimated. This can be done by passing a -1 as the first value after the tilde, as follows: # specifying the same linear model as the previous example, but using a &quot;-1&quot; to indicate that there should not be a reference group m_linear_dc_2 &lt;- lm(percentage_earned ~ -1 + TimeSpent_std + course_id, data = dat) sjPlot::tab_model(m_linear_dc_2) percentage earned Predictors Estimates CI p TimeSpent_std 0.12 0.10 – 0.14 &lt;0.001 course_id (AnPhA-S116-01) 0.73 0.67 – 0.80 &lt;0.001 course_id (AnPhA-S116-02) 0.72 0.64 – 0.80 &lt;0.001 course_id (AnPhA-S216-01) 0.65 0.58 – 0.71 &lt;0.001 course_id (AnPhA-S216-02) 0.65 0.54 – 0.75 &lt;0.001 course_id (AnPhA-T116-01) 0.75 0.63 – 0.88 &lt;0.001 course_id (BioA-S116-01) 0.70 0.63 – 0.77 &lt;0.001 course_id (BioA-S216-01) 0.62 0.45 – 0.79 &lt;0.001 course_id (BioA-T116-01) 0.82 0.53 – 1.12 &lt;0.001 course_id (FrScA-S116-01) 0.85 0.80 – 0.90 &lt;0.001 course_id (FrScA-S116-02) 0.72 0.58 – 0.86 &lt;0.001 course_id (FrScA-S116-03) 0.75 0.68 – 0.82 &lt;0.001 course_id (FrScA-S116-04) 0.65 0.46 – 0.84 &lt;0.001 course_id (FrScA-S216-01) 0.83 0.79 – 0.88 &lt;0.001 course_id (FrScA-S216-02) 0.78 0.70 – 0.86 &lt;0.001 course_id (FrScA-S216-03) 0.59 0.35 – 0.83 &lt;0.001 course_id (FrScA-S216-04) 0.90 0.71 – 1.09 &lt;0.001 course_id (FrScA-T116-01) 0.79 0.58 – 1.00 &lt;0.001 course_id (OcnA-S116-01) 0.75 0.67 – 0.83 &lt;0.001 course_id (OcnA-S116-02) 0.73 0.62 – 0.84 &lt;0.001 course_id (OcnA-S116-03) 0.57 0.15 – 0.99 0.007 course_id (OcnA-S216-01) 0.64 0.57 – 0.70 &lt;0.001 course_id (OcnA-S216-02) 0.73 0.63 – 0.83 &lt;0.001 course_id (OcnA-T116-01) 0.72 0.58 – 0.87 &lt;0.001 course_id (PhysA-S116-01) 0.91 0.85 – 0.97 &lt;0.001 course_id (PhysA-S216-01) 0.79 0.69 – 0.89 &lt;0.001 course_id (PhysA-T116-01) 0.94 0.64 – 1.23 &lt;0.001 Observations 598 R2 / R2 adjusted 0.933 / 0.930 This does not work in many cases, and it is much more common to dummy-code factors, and so we emphasized that in this walkthrough. However, we want you to be aware that it is possible (though uncommon) to estimate a model without an intercept. 7.2 Toward multi-level models Dummy-coding is a very helpful strategy. It is particularly useful with a small number of groups (i.e., for estimating the effects of being in one of the five subjects in the online science data set, as in this walkthrough; we note that in addition to these five subjects, we also have multiple sections, or classes, for each subject). With effects such as being a student in a particular class, though, the output seems to be less useful: it is hard to interpret the 25 different effects (and to compare them to the intercept). Additionally, analysts often have the goal not of determining the effect of being in a specific class, per se, but rather of accounting for the fact that students share a class. This is important because linear models (i.e., the model we estimated using lm()) have an assumption that the data points are - apart from sharing levels of the variables that are used in the model - independent, or not correlated. This is what is meant by the “assumption of independence” or of “independently and identically distributed” (i.i.d.) residuals (Field, Miles, &amp; Field, 2012). Multi-level models are a way to deal with the difficulty of interpreting the estimated effects for each of many groups, like classes, and to address the assumption of independence. Multi-level models do this by (still) estimating the effect of being a student in each group, but with a key distinctionfrom linear models: Instead of determining how different the observations in a group are from those in the reference group, the multi-level model “regularizes” (sometimes the term “shrinks” is used) the difference based on how systematically different the groups are. The reason why “shrinkage” is occasionally used is that the group-level estimates (i.e., for classes) that are obtained through multi-level modeling can never be larger than those from a linear model (regression). As described earlier, when there are groups included in the model, a regression effectively estimates the effect for each group independent of all of the others. Through regularization, groups that are comprised of individuals who are consistently different (higher or lower) than individuals on average are not regularized very much - their estimated difference may be close to the estimate from a multi-level model - whereas groups with only a few individuals, or with a lot of variability within individuals, would be regularized a lot. In the former case, the multi-level model considers there to be strong evidence for a group effect, whereas in the latter, the model recognizes that there is less certainty about a group (class) effect for that particular group, in part because that group is small. Multi-level models are very common in educational research for cases such as this: accounting for the way in which students take the same classes, or even go to the same school (see Raudenbush &amp; Bryk, 2002). The way that a multi-level model does this “regularizing” is by considering the groups (and not the data points, in this case) to be samples from a larger population of classes. By considering the effects of groups to be samples from a larger population, the model is able to use information not only particular to each group (as the models created using lm()), but also information across all of the data. Using multi-level models means that the assumption of independence can be addressed; their use also means that individual coefficients for classes do not need to be included (or interpreted, thankfully!), though they are still included in and accounted for in the model. As we describe, the way that information about the groups is reported is usually in the form of the intraclass correlation coefficient (ICC), which explains the proportion of variation in the dependent variable that the groups explain. Smaller ICCs (such as ICCs with values of 0.05, representing 5% of the variation in the dependent variable) mean that the groups are not very important; larger ICCs, such as ICCs with values of 0.10 or larger (values as high as 0.50 are not uncommon!). ICCs that are larger would indicate that groups are important and that they have to do with a lot of the differences observed in the dependent variable (and that not including them may potentially ignore the assumption of independence in a case in which it may be important to recognize it - and lead to bias in the results). That was a lot of technical information about multi-level models; thank you for sticking with us through it! We wanted to include this as multi-level models are common: consider how often the data you collect involves students nested (or grouped) in classes, or classes nested in schools (or even schools nested in districts - you get the picture!). Educational data is complex, and so it is not surprising that multi-level models may be encountered in educational data science analyses, reports, and articles. Fortunately, for all of the complicated details, multi-level models are very easy to use in R. Their requires a new package; one of the most common for estimating these types of models is lme4. We use it very similarly to the lm() function, but we pass it an additional argument about what the groups, in the data are. Such a model is often referred to as a “varying intercepts” multi-level model, because what is different between the groups is the effect of being a student in a class: the intercepts between groups vary. # install.packages(&quot;lme4&quot;) # note that this only needs to be run one time, after which the package will be installed on your local computer m_course &lt;- lme4::lmer(percentage_earned ~ TimeSpent + (1|course_id), data = dat) ## Warning: Some predictor variables are on very different scales: consider ## rescaling sjPlot::tab_model(m_course) percentage earned Predictors Estimates CI p (Intercept) 0.59 0.55 – 0.63 &lt;0.001 TimeSpent 0.00 0.00 – 0.00 &lt;0.001 Random Effects σ2 0.04 τ00 course_id 0.00 ICC 0.09 N course_id 26 Observations 598 Marginal R2 / Conditional R2 0.226 / 0.295 In a way, what is going on above is straightforward (and similar to what we have seen with lm()), but, it is also different and potentially confusing. Parentheses are not commonly used with lm(); there is a term ((1|course_id)) in parentheses. Also, the bar symbol - | - is not commonly used with lm(). As different as these (the parentheses and bar) are, they are used for a relatively straightforward purpose: to model the group (in this case, courses) in the data. With lmer(), these group terms are specified in parentheses - specifically, to the right of the bar. That is what the |course_id part means - it is telling lmer that courses are groups in the data. The left side of the bar tells lmer that what we want to be specified are varying intercepts for each group (1 is used to denote the intercept). That is basically it! That is basically it, but there is potentially more to the story: in addition to the 1, variables which can be specified to have a different effect for each group can also be specified. These variables are referred to not as varying intercepts, but as varying slopes. We will not cover these in this walkthrough, but want you to be aware of them (we recommend the book by West, Welch, and Galecki [2014] provide an excellent walkthrough on how to specify varying slopes using lmer). To say just a bit more, there is a connection between multi-level models and Bayesian methods (Gelman &amp; Hill, 2005; McIlreath, 2015); one way to think about the “regularizing” going on is that estimates for each group (class) are made taking account of the data across all of the groups (classes). The data for all of the classes can be interpreted as a prior for the group estimates. There is another part of the above code to mention. The sjPlot::tab_model() function comparably as it does for lm() models, providing output for the model, including some fit statistics as well as coefficients and their standard errors and estimates. There are two things to note about lmer() output: p-values are not automatically provided, due to debates in the wider field about how to calculate the degrees of freedom for coefficients (run ?lme4::pvalues to see a discussion of the issue as well as solutions; we have found the lmerTest to be helpful as an easy solution, though we note that some of the recommendations available through ?lme4::pvalues may be preferable, as the technique lmerTest implements has some known issues) There As we mentioned earlier, a common way to understand how much variability is at the group level is to calculate the intra-class correlation. This value is the proportion of the variability in the outcome (the y-variable) that is accounted for solely by the groups identified in the model. There is a useful function in the performance package for doing this. # install.packages(&quot;performance&quot;) performance::icc(m_course) ## # Intraclass Correlation Coefficient ## ## Adjusted ICC: 0.089 ## Conditional ICC: 0.069 This shows that nearly 17% of the variability in the percentage of points students earned can be explained simply by knowing what class they are in. There is much more to do with multi-level models. We briefly discuss a common extension to the model we just used, adding additional levels. The data that we are using is all from one school, and so we cannot estimate a “two-level” model. Imagine, however, that instead of 26 classes, we had data from students from 230 classes, and that these classes were from 15 schools. We could estimate a two-level, varying intercepts (where there are now two groups with effects) model very similar to the model we estimated above, but simply with another group added for the school. The model will account for the way in which the classes are nested within the schools automatically (Bates, Maechler, Bolker, &amp; Walker, 2015). # this model would specify a group effect for both the course nad school m_course_school &lt;- lme4::lmer(percentage_earned ~ TimeSpent + (1|course_ID) + (1|school_id), data = dat) Were we to estimate this model (and then use the icc() function), we would see two ICC values representing the proportion of the variation in the dependent variable explained by each of the two groups we added - the course and the school. A common question those using lme4 have is whether it is necessary to explicitly nest the courses within schools; as long as the courses are unique labelled, this is not necessary to do. You can add further still levels to the model, as the {lme4} package was designed for complex multi-level models (and even those with not nested, but crossed random effects; a topic beyond the scope of this walkthrough, but which is described in West, Welch, &amp; Galecki, 2015). 7.3 Other groups In this example (and in many examples in educational research), the groups are classes. But, multi-level models can be used for other cases in which data is associated with a common group. For example, if students respond to repeated measures (such as quizzes) over time, then the multiple quiz responses for each sudent could be considered to be “grouped” within students. In such a case, instead of specifying the model with the course as the “grouping factor”, students could be. Moreover, multi-level models can include multiple groups (as noted above), even if the groups are of very different kinds (i.e., if students from multiple classes responded to multiple quizzes). There is much more that can be done with multi-level models. For further reading, we recommend the sources we cited in the walkthrough: Raudenbush &amp; Bryk (2002) West, Welch, &amp; Galecki (2015) Finally, as noted earlier, multi-level models have similarities to the Bayesian methods which are becoming more common among some R users - and educational data scientists. We recommend the following references for Bayesian methods that directly reference and build on the models that are able to be estimated using {lme4}. Gelman &amp; Hill (2006) Burkner (2019) "],
["walkthrough-02-title-here.html", "8 Walkthrough 02: Title Here 8.1 Introduction 8.2 Data Sources 8.3 Load Packages 8.4 Import Data 8.5 Tidy Data 8.6 Process Data 8.7 Visualize Data 8.8 Model Data", " 8 Walkthrough 02: Title Here 8.1 Introduction There are a variety of data sources to explore in the education field. Student assessment scores can be examined for progress towards goals. The text from a teacher’s written classroom observation notes about a particular learner’s in-class behavior or emotional status can be analyzed for trends. We can tap into the exportable data available from common learning software or platforms popular in the K-12 education space. 8.2 Data Sources This walkthrough goes through a series of analyses using the data science framework (link). The first analysis centers around a ubiquitous K-12 classroom tool: the gradebook. We use an Excel gradebook template Assessment Types - Points and simulated student data. On your first read through of this section try using our simulated dataset found in this book’s data/ folder. 8.3 Load Packages As mentioned in the Foundational Skills chapter, begin by loading the libraries that will be used. We will use the {tidyverse} package mentioned in Walkthrough 1. The {readxl} package is used to read and import Excel spreadsheets since these file types are very common in the education field. Make sure you have installed the packages in R on your computer before starting (see Foundational Skills chapter). Load the libraries, as they must be loaded each time we start a new project. # Load libraries library(tidyverse) library(here) library(readxl) library(janitor) library(dataedu) 8.4 Import Data Recall how the Foundational Skills chapter recommended favoring CSV files, or comma-separated values files, when working with datasets in R. This is because CSV files, with the .csv file extension, are common in the digital world. However, data won’t always come in the preferred file format. Fortunately, R can import a variety of data file types. This walkthrough imports an Excel file because these file types, with the .xlsx or .xls extensions, are very likely to be encountered in the K-12 education world. This code uses the read_excel() function of the {readxl} package to find and read the data of the desired file. Note the file path that read_excel() takes to find the simulated dataset file named ExcelGradeBook.xlsx which sits in a folder on your computer if you have downloaded it. The function getwd() will help locate your current working directory. This tells where on the computer R is currently working with files. # See the current working directory getwd() For example, an R user on Linux or Mac might see their working directory as: /home/username/Desktop. A Windows user might see their working directory as: C:\\Users\\Username\\Desktop. From this location go deeper into files to find the desired file. For example, if you downloaded the book repository from Github to your Desktop the path to the Excel file might look like one of these below: /home/username/Desktop/data-science-in-education/data/gradebooks/ExcelGradeBook.xlsx (on Linux &amp; Mac) C:\\Users\\Username\\Desktop\\data-science-in-education\\data\\gradebooks\\ExcelGradeBook.xlsx (on Windows) In the code below ExcelGradeBook &lt;- read_excel(\"path/to/file.xlsx\", sheet = 1, skip = 10) the part written path/to/file.xlsx is just pseudo code to remind you to swap in your own path to the file you want to import. Recall from the Foundational Skills section of this book that directories and file paths are important for finding files on your computer. After locating the sample Excel file run the code below to run the function read_excel() which reads and saves the data from ExcelGradeBook.xlsx to an object also called ExcelGradeBook. Note the two arguments specified in this code sheet = 1 and skip = 10. This Excel file is similar to one you might encounter in real life with superfluous features that we are not interested in. This file has 3 different sheets and the first 10 rows contain things we won’t need. Thus, sheet = 1 tells read_excel() to just read the first sheet in the file and disregard the rest. While skip = 10 tells read_excel() to skip reading the first 10 rows of the sheet and start reading from row 11 which is where the column headers and data actually start inside the Excel file. Remember to replace path/to/file.xlsx your own path to the file you want to import. # Use readxl package to read and import file and assign it a name ExcelGradeBook &lt;- read_excel( here::here(&quot;data&quot;, &quot;gradebooks&quot;, &quot;ExcelGradeBook.xlsx&quot;), sheet = 1, skip = 10 ) The ExcelGradeBook file has been imported into RStudio. Next, assign the data frame to a new name using the code below. Renaming cumbersome filenames can improve the readability of the code and make is easier for the user to call on the dataset later on in the code. # Rename data frame gradebook &lt;- ExcelGradeBook Your environment will now have two versions of the dataset. There is ExcelGradeBook which was the original dataset imported. There is also gradebook which is currently a copy of ExcelGradeBook. As you progress through this section, we will work primarily with the gradebook dataset. Additionally, while working onward in this section of the book, if you make a mistake and mess up the gradebook data frame and are not able to fix it, you can reset the data frame to return to the same state as the original ExcelGradeBook data frame by running gradebook &lt;- ExcelGradeBook again. This will overwrite your messed up gradebook data frame with the originally imported ExcelGradeBook data frame. Afterwards, just continue running code from this point in the text. 8.5 Tidy Data This walkthrough uses an Excel data file because it is one that we are likely to encounter. Moreover, the messy state of this file mirrors what might be encountered in real life. The Excel file contains more than one sheet, has rows we don’t need, and uses column names that have spaces between words. All these things make the data tough to work with. The data is not tidy. We can begin to overcome these challenges before importing the file into RStudio by deleting the unnecessary parts of the Excel file then saving it as a .csv file. However, if you clean the file outside of R, this means if you ever have to clean it up (say, if the dataset is accidentally deleted and you need to redownload it from the original source) you would have to do everything from the beginning. We recommend cleaning the original data in R so that you can recreate all the steps necessary for your analysis. Also, the untidy Excel file provides realistic practice for tidying up the data programmatically using R itself. First, modify the column names of the gradebook data frame to remove any spaces and replace them with an underscore. Using spaces as column names in R can present difficulties later on when working with the data. Second, we want the column names of our data to be easy to use and understand. The original dataset has column names with uppercase letters and spaces. We can use the janitor package to quickly change them to a more useable format. colnames(gradebook) # look at original column names ## [1] &quot;Class&quot; ## [2] &quot;Name&quot; ## [3] &quot;Race&quot; ## [4] &quot;Gender&quot; ## [5] &quot;Age&quot; ## [6] &quot;Repeated Grades&quot; ## [7] &quot;Financial Status&quot; ## [8] &quot;Absent&quot; ## [9] &quot;Late&quot; ## [10] &quot;Make your own categories&quot; ## [11] &quot;Running Average&quot; ## [12] &quot;Letter Grade&quot; ## [13] &quot;Homeworks&quot; ## [14] &quot;Classworks&quot; ## [15] &quot;Formative Assessments&quot; ## [16] &quot;Projects&quot; ## [17] &quot;Summative Assessments&quot; ## [18] &quot;Another Type 2&quot; ## [19] &quot;Classwork 1&quot; ## [20] &quot;Homework 1&quot; ## [21] &quot;Classwork 2&quot; ## [22] &quot;Homework 2&quot; ## [23] &quot;Classwork 3&quot; ## [24] &quot;Classwork 4&quot; ## [25] &quot;Classwork 5&quot; ## [26] &quot;Classwork 6&quot; ## [27] &quot;Homework 3&quot; ## [28] &quot;Formative Assessment 1&quot; ## [29] &quot;Project 1&quot; ## [30] &quot;Classwork 7&quot; ## [31] &quot;Homework 4&quot; ## [32] &quot;Project 2&quot; ## [33] &quot;Classwork 8&quot; ## [34] &quot;Homework 5&quot; ## [35] &quot;Project 3&quot; ## [36] &quot;Homework 6&quot; ## [37] &quot;Classwork 9&quot; ## [38] &quot;Homework 7&quot; ## [39] &quot;Homework 8&quot; ## [40] &quot;Project 4&quot; ## [41] &quot;Project 5&quot; ## [42] &quot;Formative Assessment 2&quot; ## [43] &quot;Project 6&quot; ## [44] &quot;Classwork 10&quot; ## [45] &quot;Homework 9&quot; ## [46] &quot;Classwork 11&quot; ## [47] &quot;Homework 10&quot; ## [48] &quot;Classwork 12&quot; ## [49] &quot;Classwork 13&quot; ## [50] &quot;Project 7&quot; ## [51] &quot;Classwork 14&quot; ## [52] &quot;Classwork 15&quot; ## [53] &quot;Homework 11&quot; ## [54] &quot;Summative Assessment 1&quot; ## [55] &quot;Classwork 16&quot; ## [56] &quot;Homework 12&quot; ## [57] &quot;Classwork 17&quot; ## [58] &quot;Homework 13&quot; ## [59] &quot;Project 8&quot; ## [60] &quot;Project 9&quot; ## [61] &quot;Project 10&quot; ## [62] &quot;Summative Assessment 2&quot; ## [63] &quot;Assessment | Insert new columns before here&quot; gradebook &lt;- gradebook %&gt;% clean_names() colnames(gradebook) # look at cleaned column names ## [1] &quot;class&quot; ## [2] &quot;name&quot; ## [3] &quot;race&quot; ## [4] &quot;gender&quot; ## [5] &quot;age&quot; ## [6] &quot;repeated_grades&quot; ## [7] &quot;financial_status&quot; ## [8] &quot;absent&quot; ## [9] &quot;late&quot; ## [10] &quot;make_your_own_categories&quot; ## [11] &quot;running_average&quot; ## [12] &quot;letter_grade&quot; ## [13] &quot;homeworks&quot; ## [14] &quot;classworks&quot; ## [15] &quot;formative_assessments&quot; ## [16] &quot;projects&quot; ## [17] &quot;summative_assessments&quot; ## [18] &quot;another_type_2&quot; ## [19] &quot;classwork_1&quot; ## [20] &quot;homework_1&quot; ## [21] &quot;classwork_2&quot; ## [22] &quot;homework_2&quot; ## [23] &quot;classwork_3&quot; ## [24] &quot;classwork_4&quot; ## [25] &quot;classwork_5&quot; ## [26] &quot;classwork_6&quot; ## [27] &quot;homework_3&quot; ## [28] &quot;formative_assessment_1&quot; ## [29] &quot;project_1&quot; ## [30] &quot;classwork_7&quot; ## [31] &quot;homework_4&quot; ## [32] &quot;project_2&quot; ## [33] &quot;classwork_8&quot; ## [34] &quot;homework_5&quot; ## [35] &quot;project_3&quot; ## [36] &quot;homework_6&quot; ## [37] &quot;classwork_9&quot; ## [38] &quot;homework_7&quot; ## [39] &quot;homework_8&quot; ## [40] &quot;project_4&quot; ## [41] &quot;project_5&quot; ## [42] &quot;formative_assessment_2&quot; ## [43] &quot;project_6&quot; ## [44] &quot;classwork_10&quot; ## [45] &quot;homework_9&quot; ## [46] &quot;classwork_11&quot; ## [47] &quot;homework_10&quot; ## [48] &quot;classwork_12&quot; ## [49] &quot;classwork_13&quot; ## [50] &quot;project_7&quot; ## [51] &quot;classwork_14&quot; ## [52] &quot;classwork_15&quot; ## [53] &quot;homework_11&quot; ## [54] &quot;summative_assessment_1&quot; ## [55] &quot;classwork_16&quot; ## [56] &quot;homework_12&quot; ## [57] &quot;classwork_17&quot; ## [58] &quot;homework_13&quot; ## [59] &quot;project_8&quot; ## [60] &quot;project_9&quot; ## [61] &quot;project_10&quot; ## [62] &quot;summative_assessment_2&quot; ## [63] &quot;assessment_insert_new_columns_before_here&quot; Review what the gradebook data frame looks like now. It shows 25 students and their individual values in various columns like projects or formative_assessments. view(gradebook) The data frame looks cleaner now but there still are some things we can remove. For example, there are rows without any names in them. Also, there are entire columns that are unused and contain no data (such as gender). These are called missing values and are denoted by NA. Since our simulated classroom only has 25 learners and doesn’t use all the columns for demographic information, we can safely remove these to tidy our dataset up even more. We can remove the extra columns rows that have no data using the janitor package. The handy remove_empty() removes columns, rows, or both that have no information in them. # Removing rows with nothing but missing data gradebook &lt;- gradebook %&gt;% remove_empty(c(&quot;rows&quot;, &quot;cols&quot;)) Now that the empty rows and columns have been removed, notice there are two columns, absent and late, where it seems someone started putting data into but then decided to stop. These two columns didn’t get removed by the last chunk of code because they technically contained some data in those columns. Since the simulated data enterer of this simulated class decided to abandon using the absent and late columns in this gradebook, we can remove it from our data frame as well. In the foundational skills chapter we introduced the select() function, which tells R which columns we want to keep. Let’s do that again here. This time we’ll use negative signs to say we want the dataset without absent and late. # Remove a targeted column because we don&#39;t use absent and late at this school. gradebook &lt;- gradebook %&gt;% select(-absent, -late) At last, the formerly untidy Excel sheet has been turned into a useful data frame. Inspect it once more to see the difference. view(gradebook) 8.6 Process Data R users transform data to facilitate working with the data during later phases of visualization and analysis. A few examples of data transformation include creating new variables, grouping data, and more. This code chunk first creates a new data frame named classwork_df, then selects particular variables from our gradebook dataset using select(), and finally gathers all the homework data under new variables into new columns. As mentioned previously, select() is very powerful. In addition to explicitly writing out the columns you want to keep, you can also use functions from the package {stringr} within select(). The {stringr} package is within {tidyverse}. Here, we’ll use the function contains() to tell R to select columns that contain a certain string (that is, text). Here, it searches for any column with the string “classwork_”. The underscore makes sure the variables from classwork_1 all the way to classwork_17 are included in classwork_df. pivot_longer() transforms the dataset into tidy data. Note that scores are in character format. We use mutate() to transform them to numerical format. # Creates new data frame, selects desired variables from gradebook, and gathers all classwork scores into key/value pairs classwork_df &lt;- gradebook %&gt;% select( name, running_average, letter_grade, homeworks, classworks, formative_assessments, projects, summative_assessments, contains(&quot;classwork_&quot;) ) %&gt;% mutate_at(vars(contains(&quot;classwork_&quot;)), list(~ as.numeric(.))) %&gt;% pivot_longer( cols = contains(&quot;classwork_&quot;), names_to = &quot;classwork_number&quot;, values_to = &quot;score&quot; ) View the new data frame and note which columns were selected for this new data frame. Also, note how all the classwork scores were gathered under new columns classwork_number and score. The contains() function. We will use this classwork_df data frame later. view(classwork_df) 8.7 Visualize Data Visual representations of data are more human friendly than just looking at numbers alone. This next line of code shows a simple summary of the data by each column similar to what we did in Walkthrough 1. # Summary of the data by columns summary(gradebook) But R can do more than just print numbers to a screen. Use the {ggplot} package within {tidyverse} to graph some of the data to help get a better grasp of what the data looks like. This code uses {ggplot} to graph categorical variables into a bar graph. Here we can see the variable Letter_grade is plotted on the x-axis showing the counts of each letter grade on the y-axis. # Bar graph for categorical variable gradebook %&gt;% ggplot(aes(x = letter_grade, fill = running_average &gt; 90)) + geom_bar() + labs(title = &quot;Bar Graph of Student Grades&quot;, x = &quot;Letter Grades&quot;, y = &quot;Count&quot;, fill = &quot;A or Better&quot;) + theme_dataedu() + scale_fill_dataedu() Using {ggplot}, we can create many types of graphs. Using our classwork_df from earlier, we can see the distribution of scores and how they differ from classwork to classwork using boxplots. We are able to do this because we have made the classworks and scores columns into tidy formats. # Scatterplot of continuous variable classwork_df %&gt;% ggplot(aes(x = classwork_number, y = score, fill = classwork_number)) + geom_boxplot() + labs( title = &quot;Distribution of Classwork Scores&quot;, x = &quot;Classwork&quot;, y = &quot;Scores&quot;, color = &quot;90% or Above&quot; ) + theme_dataedu() + scale_fill_dataedu() + theme(legend.position = &quot;none&quot;, # removes legend axis.text.x = element_text(angle = 45, hjust = 1)) # angles the x axis labels 8.8 Model Data 8.8.1 Deciding an Analysis Using this spreadsheet, we can start to form hypotheses about the data. For example, we can ask ourselves, “Can we predict overall grade using formative assessment scores?” For this, we will try to predict a response variable Y (overall grade) as a function of a predictor variable Y (formative assessment scores). The goal is to create a mathematical equation for overall grade as a function of formative assessment scores when only formative assessment scores are known. 8.8.2 Visualize Data to Check Assumptions It’s important to visualize data to see any distributions, trends, or patterns before building a model. We use {ggplot} to understand these variables graphically. 8.8.2.1 Linearity First, we plot X and Y to determine if we can see a linear relationship between the predictor and response. The x-axis shows the formative assessment scores while the y-axis shows the overall grades. The graph suggests a correlation between overall class grade and formative assessment scores. As the formative scores goes up, the overall grade goes up too. # Scatterplot between formative assessment and grades by percent # To determine linear relationship gradebook %&gt;% ggplot(aes(x = formative_assessments, y = running_average)) + geom_point() + labs(title = &quot;Relationship Between Overall Grade and Formative Assessments&quot;, x = &quot;Formative Assessment Score&quot;, y = &quot;Overall Grade in Percentage&quot;) + theme_dataedu() We can layer different types of plots on top of each other in {ggplot}. Here the scatterplot is layered with a line of best fit, that suggests a positive linear relationship. # Scatterplot between formative assessment and grades by percent # To determine linear relationship # With line of best fit gradebook %&gt;% ggplot(aes(x = formative_assessments, y = running_average)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = TRUE) + labs(title = &quot;Relationship Between Overall Grade and Formative Assessments&quot;, x = &quot;Formative Assessment Score&quot;, y = &quot;Overall Grade in Percentage&quot;) + theme_dataedu() 8.8.2.2 Outliers Now we use boxplots to determine if there are any outliers in formative assessment scores or overall grades. For linear regression, we’re hoping to see no outliers in the data. We don’t see any for these two variables, so we can proceed with the model. # Boxplot of formative assessment scores # To determine if there are any outliers gradebook %&gt;% ggplot(aes(x = &quot;&quot;, y = formative_assessments)) + geom_boxplot() + labs(title = &quot;Distribution of Formative Assessment Scores&quot;, x = &quot;Formative Assessment&quot;, y = &quot;Score&quot;) + theme_dataedu() # Boxplot of overall grade scores in percentage # To determine if there are any outliers gradebook %&gt;% ggplot(aes(x = &quot;&quot;, y = running_average)) + geom_boxplot() + labs(title = &quot;Distribution of Overall Grade Scores&quot;, x = &quot;Overall Grade&quot;, y = &quot;Score in Percentage&quot;) + theme_dataedu() 8.8.3 Correlation Analysis We want to know the strength of the relationship between the two variables formative assessment scores and overall grade percentage. The strength is denoted by the “correlation coefficient.” The correlation coefficient goes from -1 to 1. If one variable consistently increases with the increasing value of the other, then they have a positive correlation (towards 1). If one variable consistently decreases with the increasing value of the other, then they have a negative correlation (towards -1). If the correlation coefficient is 0, then there is no relationship between the two variables. Correlation is good for finding relationships but it does not imply that one variable causes the other (correlation does not mean causation). cor(gradebook$formative_assessments, gradebook$running_average) ## [1] 0.6632553 8.8.4 Build Linear Model In the multi-level modeling walkthrough, we introduced the concept of linear models. Let’s use that same technique here. Now that you’ve checked your assumptions and seen a linear relationship, we can build a linear model, that is, a mathematical formula that calculates your running average as a function of your formative assessment score. This is done using the lm() function, where the arguments are: Your predictor (formative_assessments) Your response (running_average) The data (gradebook) linear_mod &lt;- lm(running_average ~ formative_assessments, data = gradebook) summary(linear_mod) ## ## Call: ## lm(formula = running_average ~ formative_assessments, data = gradebook) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.2814 -2.7925 -0.0129 3.3179 8.5353 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.11511 8.54774 5.863 5.64e-06 *** ## formative_assessments 0.42136 0.09914 4.250 0.000302 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.657 on 23 degrees of freedom ## Multiple R-squared: 0.4399, Adjusted R-squared: 0.4156 ## F-statistic: 18.06 on 1 and 23 DF, p-value: 0.0003018 The formula reads as follows: running_average = 50.11511 + 0.42136*formative_assessments "],
["text-analysis-of-tweets.html", "9 Text Analysis of Tweets 9.1 Vocabulary 9.2 Introduction 9.3 About Our Text Dataset 9.4 Reading the data 9.5 Cleaning the data for the analysis 9.6 Counting words 9.7 Sentiment analysis 9.8 Next steps 9.9 References", " 9 Text Analysis of Tweets 9.1 Vocabulary tokenize token RDS files anti_join set.seed slice sample_n() stop words 9.2 Introduction The ability to work with many kinds of datasets is one of the great features of doing data science with programming. So far we’ve analyzed data in CSV files, but that’s not the only way data is stored. If we can learn some basic techniques for analyzing text, we increase the number of places we can find information to learn about the student experience. When we think about data science in education, our minds tends to go data in spreadsheets. But what can we learn about the student experience from text data? Take a moment to mentally review all the moments in your work day that you generated or consumed text data. In education, we’re surrounded by it. We do our lessons in word processor documents, our students submit assignments online, and the school community expresses themselves on public social media platforms. The text we generate can be an authentic reflection of reality in schools, so how might we learn from it? Even the most basic text analysis techniques will expand your data science toolkit. For example, you can use text analysis to count the number of key words that appear in open ended survey responses. You can analyze word patterns in student responses or message board posts. Analyzing a collection of text is different from analyzing large numerical datasets because words don’t have agreed upon values the way numbers do. The number 2 will always be more than 1 and less than 3. The word “fantastic”, on the other hand, has multiple ambiguous levels of degree depending on interpretation and context. Using text analysis can help to broadly estimate what is happening in the text. When paired with observations, interviews, and close review of the text, this approach can help education staff learn from text data. In this chapter, we’ll learn how to count the frequency of words in a dataset and associate those words with common feelings like positivity or joy. We’ll show these techniques using a dataset of tweets. We encourage you to complete the walkthrough, then reflect on how the skills learned can be applied to other texts, like word processing documents or websites. 9.3 About Our Text Dataset It’s useful to learn these techniques from text datasets that are available for download. Take a moment to do an online search for “download tweet dataset” and note the abundance of tweet datasets available. Since there’s so much, it’s useful to narrow the tweets to a only those that help you answer your analytic questions. Hashtags are text within a tweet that act as a way to categorize content. Here’s an example: RT (???): I’m trying to recreate some Stata code in R, anyone have a good resource for what certain functions in Stata are doing? #RStats #Stata Twitter recognizes any words that start with a “#” as a hashtag. The hashtags “#RStats” and “#Stata” make this tweet conveniently searchable. If Twitter uses search for “#RStats”, Twitter returns all the Tweets containing that hashtag. In this example, we’ll be analyzing a dataset of tweets that have the hashtag #tidytuesday. This hashtag returns tweets about the weekly TidyTuesday ritual, where folks learning R create and tweet data visualizations they made while learning to use tidyverse R packages. You can view the TidyTuesday tweets dataset here: https://github.com/data-edu/data-science-in-education/raw/master/data/tidytuesday/rtweet-processed-tidytuesday-data.rds 9.4 Reading the data Let’s start by getting the data into our programming environment so we can start analyzing it. For this analysis, we’ll be using the tidyverse, here, and dataedu packages. Just a reminder, if you haven’t already installed the dataedu package, you can do so by typing this code: devtools::install_github(&quot;data-edu/dataedu&quot;) library(tidyverse) library(here) library(dataedu) library(tidytext) We’ve conveniently included the raw dataset of TidyTuesday tweets in the dataedu package. You can see the dataset by typing tt_tweets. Let’s start by assigning the name raw_tweets to this dataset: raw_tweets &lt;- tt_tweets Run glimpse(raw_tweets) and notice the number of variables in this dataset. It’s good practice to use functions like glimpse or str to look at the data type of each variable. For this walkthrough, we won’t need all 90 variables so let’s clean the dataset and keep only the ones we want. 9.5 Cleaning the data for the analysis In this section we’ll select the columns we need for our analysis and we’ll transform the dataset so each row represents a word. After that, our dataset will be ready for exploring. First, let’s use select to pick the two columns we’ll need: status_id and text. status_id will help us associate interesting words with a particular tweet and text will give us the text from that tweet. We’ll also change status_id to the character data type since it’s meant to label tweets and doesn’t actually represent a numerical value. tweets &lt;- raw_tweets %&gt;% #filter for English tweets filter(lang == &quot;en&quot;) %&gt;% select(status_id, text) %&gt;% # Convert the ID field to the character data type mutate(status_id = as.character(status_id)) Now the dataset has a column to identify each tweet and a column that shows the text that users tweeted. But each row has the entire tweet in the text variable, which makes it hard to analyze. If we kept our dataset like this, we’d need to use functions on each row to do something like count the number of times the word “good” appears. We can count words more efficiently if each row represented a single word. Splitting sentences in a row into single words in a row is called “tokenizing.” In their book Text Mining With R, Julia Silge and David Robinson describe tokens this way: A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. This one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix. The tidytext package has a convenient function called unnest_tokens that tokenizes vectors of words. Let’s install the tidytext package so we can use it: install.packages(&quot;tidytext&quot;) Let’s use unnest_tokens to take our dataset of tweets and transform it into a dataset of words. tokens &lt;- tweets %&gt;% unnest_tokens(output = word, input = text) tokens ## # A tibble: 131,233 x 2 ## status_id word ## &lt;chr&gt; &lt;chr&gt; ## 1 1163154266065735680 first ## 2 1163154266065735680 tidytuesday ## 3 1163154266065735680 submission ## 4 1163154266065735680 roman ## 5 1163154266065735680 emperors ## 6 1163154266065735680 and ## 7 1163154266065735680 their ## 8 1163154266065735680 rise ## 9 1163154266065735680 to ## 10 1163154266065735680 power ## # … with 131,223 more rows We use output = word to tell unnest_tokens that we want our column of tokens to be called word. We use input = text to tell unnest_tokens to tokenize the tweets in the text column of our tweets dataset. The result is a new dataset where each row has a single word in the word column and a unique ID in the status_id column that tells us which tweet the word appears in. Notice that our tokens dataset has many more rows than our tweets dataset. This tells us a lot about how unnest_tokens works. In the tweets dataset, each row has an entire tweet and its unique ID. Since that unique ID is assigned to the entire tweet, each unique ID only appears once in the dataset. When we used unnest_tokens put each word on its own row, we broke each tweet into many words. This created additional rows in the dataset. And since each word in a single tweet shares the same ID for that tweet, an ID now appears multiple times in our new dataset. We’re almost ready to start analyzing the dataset! There’s one more step we’ll take–removing common words that don’t help us learn about what people are tweeting about. Words like “the” or “a” are in a category of words called “stop words”. Stop words serve a function in verbal communication, but don’t tell us much on their own. As a result, they clutter our dataset of useful words and make it harder to manage the volume of words we want to analyze. The tidytext package includes a dataset called stop_words that we’ll use to remove rows containing stop words. We’ll use anti_join on our tokens dataset and the stop_words dataset to keep only rows that have words not appearing in the stop_words dataset. data(stop_words) tokens &lt;- tokens %&gt;% anti_join(stop_words, by = &quot;word&quot;) Why does this work? Let’s look closer. inner_join matches the observations in one dataset to another by a specified common variable. Any rows that don’t have a match get dropped from the resulting dataset. anti_join does the same thing as inner_join except it drops matching rows and keeps the rows that don’t match. This is convenient for our analysis because we want to remove rows from tokens that contain words in the stop_words dataset. When we call anti_join, we’re left with rows that don’t match words in the stop_words dataset. These remaining words are the ones we’ll be analyzing. One final note before we start counting words: Remember when we first tokenized our dataset and we passed unnest_tokens the argument output = word? We conveniently chose word as our column name because it matches the column name word in the stop_words dataset. This makes our call to anti_join simpler because anti_join knows to look for the column named word in each dataset. 9.6 Counting words Now it’s time to start exploring our newly cleaned dataset of tweets. Computing the frequency of each word and seeing which words showed up the most often is a good start. We can pipe tokens to the count function to do this: tokens %&gt;% count(word, sort = TRUE) ## # A tibble: 15,335 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 t.co 5432 ## 2 https 5406 ## 3 tidytuesday 4316 ## 4 rstats 1748 ## 5 data 1105 ## 6 code 988 ## 7 week 868 ## 8 r4ds 675 ## 9 dataviz 607 ## 10 time 494 ## # … with 15,325 more rows We pass count the argument sort = TRUE to sort the n variable from the highest value to the lowest value. This makes it easy to see the most frequenly occuring words at the top. Not surprisingly, “tidytuesday” was the third most frequent word in this dataset. We may want to explore further by showing the frequency of words as a percent of the whole dataset. Calculating percentages like this is useful in a lot of education scenarios because it helps us make comparisons across different sized groups. For example, you may want to calculate what percentage of students in each classroom receive special education services. In our tweets dataset, we’ll be calculating the count of words as a percentage of all tweets. We can do that by using mutate to add a column called percent. percent will divide n by sum(n), which is the total number of words. Finally, will multiply the result by 100. tokens %&gt;% count(word, sort = TRUE) %&gt;% # n as a percent of total words mutate(percent = n / sum(n) * 100) ## # A tibble: 15,335 x 3 ## word n percent ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 t.co 5432 7.39 ## 2 https 5406 7.36 ## 3 tidytuesday 4316 5.87 ## 4 rstats 1748 2.38 ## 5 data 1105 1.50 ## 6 code 988 1.34 ## 7 week 868 1.18 ## 8 r4ds 675 0.919 ## 9 dataviz 607 0.826 ## 10 time 494 0.672 ## # … with 15,325 more rows Even at 4316 appearances in our dataset, “tidytuesday” represents only about 6 percent of the total words in our dataset. This makes sense when you consider our dataset contains 15335 unique words. 9.7 Sentiment analysis Now that we have a sense of the most frequently appearing words, it’s time to explore some questions in our tweets dataset. Let’s imagine that we’re education consultants trying to learn about the community surrounding the TidyTuesday data visualization ritual. We know from the first part of our analysis that the token “dataviz” appeared frequently relative to other words, so maybe we can explore that further. A good start would be to see how the appearance of that token in a tweet is associated with other positive words. We’ll need to use a technique called sentiment analysis to get at the “positivity” of words in these tweets. Sentiment analysis tries to evaluate words for their emotional association. If we analyze words by the emotions they convey, we can start to explore patterns in large text datasets like our tokens data. The functions we’ll be using for our sentiment analysis are in a package called textdata. Let’s start by installing that. install.packages(&quot;tidytext&quot;) Earlier we used anti_join to remove stop words in our dataset. We’re going to do something similar here to reduce our tokens dataset to only words that have a positive association. We’ll use a dataset called the NRC Word-Emotion Association Lexicon to help us identify words with a positive association. This dataset was published in a work called Crowdsourcing a Word-Emotion Association Lexicon (Saif M. Mohammad and Peter Turney, 2013). To explore this dataset more, we’ll use a tidytext function called get_sentiments to view some words and their associated sentiment. If this is your first time using the NRC Word-Emotion Association Lexicon in the tidytext package, you’ll be prompted to download the NRC lexicon. Respond “yes” to the prompt and the NRC lexicon will download. Note that you’ll only have to do this the first time you use the NRC lexicon. get_sentiments(&quot;nrc&quot;) ## # A tibble: 13,901 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 abacus trust ## 2 abandon fear ## 3 abandon negative ## 4 abandon sadness ## 5 abandoned anger ## 6 abandoned fear ## 7 abandoned negative ## 8 abandoned sadness ## 9 abandonment anger ## 10 abandonment fear ## # … with 13,891 more rows This returns a dataset with two columns. The first is word and contains a list of words. The second is the sentiment column, which contains an emotion associated with each word. This dataset is similar to the stop_words dataset. Note that this dataset also uses the column name word, which will again make it easy for us to match this dataset to our tokens dataset. 9.7.1 Tweets associated with positive Let’s begin working on reducing our tokens dataset down to only words that the NRC dataset associates with positivity. We’ll start by creating a new dataset, nrc_pos, which contains the NRC words that have the positive sentiment. Then we’ll match that new dataset to tokens using the word column that is common to both datasets. Finally, we’ll use count to total up the appearances of each positive word. # Only positive in the NRC dataset nrc_pos &lt;- get_sentiments(&quot;nrc&quot;) %&gt;% filter(sentiment == &quot;positive&quot;) # Match to tokens pos_tokens_count &lt;- tokens %&gt;% inner_join(nrc_pos, by = &quot;word&quot;) %&gt;% # Total appearance of positive words count(word, sort = TRUE) pos_tokens_count ## # A tibble: 644 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 fun 173 ## 2 top 162 ## 3 learn 131 ## 4 found 128 ## 5 love 113 ## 6 community 110 ## 7 learning 97 ## 8 happy 95 ## 9 share 90 ## 10 inspired 85 ## # … with 634 more rows We can visualize this words nicely by using ggplot to show the positive words in a bar chart. There are 644 words total, which is hard to convey in a compact chart. We’ll solve that problem by filtering our dataset to only words that appear 75 times or more. pos_tokens_count %&gt;% filter(n &gt;= 75) %&gt;% ggplot(., aes(x = reorder(word, -n), y = n)) + geom_bar(stat = &quot;identity&quot;) + labs( title = &quot;Count of words associated with positivity&quot;, subtitle = &quot;Tweets with the hashtag #tidytuesday&quot;, caption = &quot;Data: Twitter and NRC&quot;, x = &quot;&quot;, y = &quot;Count&quot; ) + theme_dataedu() Note the use of reorder when mapping the word variable to the x aesthetic. Using reorder here sorts our x axis in descending order by the variable n. Sorting the bars from highest frequency to lowest makes it easier for the reader to identify and compare the most and least common words in the visualization. 9.7.2 TidyTuesday and other positive words Earlier in the analysis we learned that “dataviz” was among the most frequently occuring words in this dataset. We can continue our exploration of TidyTuesday tweets by seeing how many tweets with “dataviz” also had at least one positive word from the NRC dataset. Looking at this might give us some clues about how people in the TidyTuesday learning community view dataviz as a tool. There are a few steps to this part of the analysis, so let’s review our strategy. We’ll need to use the status_id field in the tweets dataset to filter the tweets that have the word dataviz in them. Then we need to use the statis_id field in this new bunch of dataviz tweets to identify the tweets that include at least one positive word. How do we know which status_id values contain the word “dataviz” and which ones contain a positive word? Recall that our tokens dataset only has one word per row, which makes it easy to use functions like filter and inner_join to make two new datasets: one of status_id values that have “dataviz” in the word column and one of status_id values that have a positive word in the word column. We’ll explore the combinations of “dataviz” and any positive words in our tweets dataset using these three ingredients: our tweets dataset, a vector of status_ids for tweets that have “dataviz” in them, and a vector of status_ids for tweets that have positive words in them. Now that we have our strategy, let’s write some code and see how it works. First, we’ll make a vector of status_ids for tweets that have “dataviz” in them. This will be used later to identify tweets that contain “dataviz” in the text. We’ll use filter on our tokens dataset to keep only the rows that have “dataviz” in the word column. Let’s name that new dataset dv_tokens. dv_tokens &lt;- tokens %&gt;% filter(word == &quot;dataviz&quot;) dv_tokens ## # A tibble: 607 x 2 ## status_id word ## &lt;chr&gt; &lt;chr&gt; ## 1 1116518351147360257 dataviz ## 2 1098025772554612738 dataviz ## 3 1161454327296339968 dataviz ## 4 1110711892086001665 dataviz ## 5 1151926405162291200 dataviz ## 6 1095854400004853765 dataviz ## 7 1157111441419395074 dataviz ## 8 1154958378764046336 dataviz ## 9 1105642831413239808 dataviz ## 10 1108196618464047105 dataviz ## # … with 597 more rows The result is a dataset that has status_ids in one column and the word “dataviz” in the other column. We can use $ to extract a vector of status_ids for tweets that have “dataviz” in the text. This vector has 607 values, so we’ll use head to view just the first ten. # Extract status_id head(dv_tokens$status_id) ## [1] &quot;1116518351147360257&quot; &quot;1098025772554612738&quot; &quot;1161454327296339968&quot; ## [4] &quot;1110711892086001665&quot; &quot;1151926405162291200&quot; &quot;1095854400004853765&quot; Now let’s do this again, but this time we’ll we’ll make a vector of status_ids for tweets that have positive words in them. This will be used later to identify tweets that contain a positive word in the text. We’ll use filter on our tokens dataset to keep only the rows that have any of the positive words in the in the word column. If you’ve been running all the code up to this point in the walkthrough, you’ll notice that you already have a dataset of positive words called nrc_pos, which can be turned into a vector of positive words by typing nrc_pos$word. We can use the %in% operator in our call to filter to find only words that are in this vector of positive words. Let’s name this new dataset pos_tokens. pos_tokens &lt;- tokens %&gt;% filter(word %in% nrc_pos$word) pos_tokens ## # A tibble: 4,925 x 2 ## status_id word ## &lt;chr&gt; &lt;chr&gt; ## 1 1163154266065735680 throne ## 2 1001412196247666688 honey ## 3 1001412196247666688 production ## 4 1001412196247666688 increase ## 5 1001412196247666688 production ## 6 1161638973808287746 found ## 7 991073965899644928 community ## 8 991073965899644928 community ## 9 991073965899644928 trend ## 10 991073965899644928 white ## # … with 4,915 more rows The result is a dataset that has status_ids in one column and a positive word from tokens in the other column. We’ll again use $ to extract a vector of status_ids for these tweets. # Extract status_id head(pos_tokens$status_id) ## [1] &quot;1163154266065735680&quot; &quot;1001412196247666688&quot; &quot;1001412196247666688&quot; ## [4] &quot;1001412196247666688&quot; &quot;1001412196247666688&quot; &quot;1161638973808287746&quot; That’s a lot of status_ids, many of which are duplicates. Let’s try and make the vector of status_ids a little shorter. We can get a vector of status_ids by using distinct. distinct is a function that extracts an element from a vector only once. Imagine having a bucket of Halloween candy that has 100 pieces of candy. You know that these 100 pieces are really just a bunch of duplicate pieces from a relatively short list of candy brands. distinct takes that bucket of 100 pieces and returns a dataframe where each element is the name of a candy brand. So we can use distinct to get a dataframe of status_ids, where each status_id only appears once: dataviz_distinct &lt;- dv_tokens %&gt;% distinct(status_id) dataviz_distinct &lt;- as.vector(dataviz_distinct$status_id) Now we have a vector of status_id for tweets containing “dataviz” and another for tweets containing a positive word. Let’s use these to transform our tweets dataset. First we’ll filter tweets for rows that have the “dataviz” status_id. Then we’ll create a new column called positive that will tell us if the status_id is from our vector of positive word status_ids. We’ll name this filtered dataset dv_pos. dv_pos &lt;- tweets %&gt;% # Only tweets that have the dataviz status_id filter(status_id %in% dataviz_distinct) %&gt;% # Is the status_id from our vector of positive word? mutate(positive = if_else(status_id %in% dataviz_distinct, 1, 0)) Let’s take a moment to dissect how we use if_else to create our positive column. We gave if_else three arguments: status_id %in% dataviz_distinct$status_id: a logical statement 1: the value of positive if the logical statement is true 0: the value of positive if the logical statement is false So our new positive column will take the value 1 if the status_id was in our pos_tokens dataset and the value 0 if the status_id was not in our pos_tokens dataset. Practically speaking, positive is 1 if the tweet has a positive word and 0 if it does not have a positive word. And finally, let’s see what percent of tweets that had “dataviz” in them also had at least one positive word: dv_pos %&gt;% count(positive) %&gt;% mutate(perc = n / sum(n)) ## # A tibble: 1 x 3 ## positive n perc ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 605 1 About 55 percent of tweets that have “dataviz” in them also had at least one positive word and about 45 percent of them did not have at least one positive word. It’s worth noting here that this finding doesn’t necessarily mean users didn’t have anything good to say about 45 percent of the “dataviz” tweets. We can’t know precisely why some tweets had positive words and some didn’t, we just know that more dataviz tweets had positive words than not. To put this in perspective, we might have a different impression if 5 percent or 95 percent of the tweets had positive words. Since the point of exploratory data analysis is to explore and develop questions, let’s continue to do that. In this last section we’ll review a random selection of tweets for context. 9.7.3 Close read of randomly selected tweets Let’s review where we are so far as we work to learn more about the TidyTuesday learning community through tweets. So far we’ve counted frequently used words and estimated the number of tweets with positive associations. This dataset is large, so we need to zoom out and find ways to summarize the data. But it’s also useful to explore by zooming in and reading some of the tweets. Reading tweets helps us to build intuition and context about how users talk about TidyTuesday in general. Even though this doesn’t lead to quantitative findings, it helps us to learn more about the content we’re studying and analyzing. Instead of reading all 4418 tweets, let’s write some code to randomly select tweets to review. First, let’s make a dataset of tweets that had positive words from the NRC dataset. Remember earlier when we made a dataset of tweets that had “dataviz” and a column that had a value of 1 for containing positive words and 0 for not containing positive words? Let’s resuse that technique, but instead of applying to a dataset of tweets containing “dataviz”, let’s use it on our dataset of all tweets. pos_tweets &lt;- tweets %&gt;% mutate(positive = if_else(status_id %in% dataviz_distinct, 1, 0)) %&gt;% filter(positive == 1) Again, we’re using if_else to make a new column called positive that takes its value based on whether status_id %in% dataviz_distinct$status_id is true or not. We can use slice to help us pick the rows. When we pass slice a row number, it returns that row from the dataset. For example, we can select the 1st and 3rd row of our tweets datset this way: tweets %&gt;% slice(1, 3) ## # A tibble: 2 x 2 ## status_id text ## &lt;chr&gt; &lt;chr&gt; ## 1 1163154266065735… First #TidyTuesday submission! Roman emperors and their ris… ## 2 1001412196247666… &quot;My #tidytuesday submission for week 8. Honey production da… Again, we’re using if_else to make a new column called positive that takes its value based on whether status_id %in% dataviz_distinct$status_id is true or not. We can use slice to help us pick the rows. When we pass slice a row number, it returns that row from the dataset. For example, we can select the 1st and 3rd row of our tweets datset this way: tweets %&gt;% slice(1, 3) ## # A tibble: 2 x 2 ## status_id text ## &lt;chr&gt; &lt;chr&gt; ## 1 1163154266065735… First #TidyTuesday submission! Roman emperors and their ris… ## 2 1001412196247666… &quot;My #tidytuesday submission for week 8. Honey production da… Randomly selecting rows from a dataset is great technique to have in your toolkit. Random selection helps us avoid some of the biases we all have when we pick rows to review ourselves. Here’s one way to do that using base R: sample(x = 1:10, size = 5) ## [1] 7 5 10 1 2 Passing sample() a vector of numbers and the size of the sample you want returns a random selection from the vector. Try changing the value of x and size to see how this works. dplyr has a version of this called sample_n() that we can use to randomly select rows in our tweets dataset. If we want to specifiy random row numbers for slice to pick for us, we can use sample(1:nrow(.), 10) like this: set.seed(369) pos_tweets %&gt;% sample_n(., size = 10) ## # A tibble: 10 x 3 ## status_id text positive ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 111793685957587… &quot;my attempt to recreate the last graph in https://… 1 ## 2 107981495219247… &quot;1/4 The @R4DScommunity welcomes you back to week … 1 ## 3 114145410803492… &quot;A minimal bar chart of American Crow counts\\nhttp… 1 ## 4 101129662572002… &quot;Welcome to week 13 of #tidytuesday!\\n\\nWe&#39;re expl… 1 ## 5 103952731219738… &quot;Are you a #catperson or a #dogperson? This week i… 1 ## 6 104421685532443… &quot;/1 The @R4DScommunity welcomes you to week 26 of … 1 ## 7 111455350429788… &quot;@BecomingDataSci If you&#39;re stuck on where to find… 1 ## 8 113599065953641… &quot;For my first ever #TidyTuesday contribution, I ma… 1 ## 9 114154765805994… &quot;#TidyTuesday #rstats #dataviz This week, about \\&quot;… 1 ## 10 106223813908584… Choropleths for #TidyTuesday: large reduction in m… 1 That returned ten randomly selected tweets that we can now read through and discuss. Let’s look a little closer at how we did that. We used sample_n(), which returns randomly selected rows from our tweets dataset. We also specified that size = 10, which means we want sample_n() to give us 10 randomly selected rows. A few lines before that, we used set.seed(369). This helps us ensure that, while sample_n() theoretically plucks 10 random numbers, we want our readers to run this code and get the same result we did. Using set.seed(369) at the top of your code makes sample_n() pick the same ten rows every time. Try changing 369 to another number and notice how sample_n() picks a different set of ten numbers, but repeatedly picks those numbers until you change the argument in set.seed(). 9.8 Next steps The purpose of this walkthrough is to share code with you so you can practice some basic text analysis techniques. Now it’s time to make your learning more meaningful by adapting this code to text-based files you regularly see at work. Trying reading in some of these and doing a similar analysis: News articles Procedure manuals Open ended responses in surveys There are also advanced text analysis techniques to explore. Consider trying topic modeling or finding correlations between terms. Both of these are covered in Julia Silge’s and David Robinson’s book, Text mining with R. 9.9 References Silge, J. &amp; Robinson, D. (2017). Text mining with R. O’Reilly Media. This dataset was published in Saif M. Mohammad and Peter Turney. (2013), Crowdsourcing a Word-Emotion Association Lexicon. Computational Intelligence, 29(3): 436-465. "],
["walkthrough-sna.html", "10 16-walkthrough-sna 10.1 Accessing data 10.2 Preparing the data for the analysis 10.3 Extracting mentions 10.4 Putting the edgelist together 10.5 Plotting the network 10.6 Selection and influence models 10.7 Technical Appendix A 10.8 Technical Appendix B 10.9 References", " 10 16-walkthrough-sna In the past, if a teacher wanted advice about how to plan a unit or to design a lesson, they would likely turn to a trusted peer in their building or district (Spillane, Kim, Chong Min, &amp; Frank, 2012). In the present, though, they are as likely to turn to someone in the professional learning network (Trust, Krutka, &amp; Carpenter, 2016). There are a few reasons to be interested in social media. For example, if you work in a school district, you may be interested in who is interacting with the content you share. If you are a researcher, you may wish to investigate what teachers, administrators, and others do through state-based hashtags (e.g., Rosenberg, Greenhalgh, Koehler, Hamilton, &amp; Akcaoglu, 2016). Social media-based data can also be interesting because it provides new contexts for learning to take place, such as learning through informal communities. In this chapter, we focus on a source of data that could be used to understand how one new community functions. That community, #tidytuesday is one sparked by the work of one of the Data Science in Education Using R co-authors, Jesse Mostipak, who created the #r4ds community from which #tidytuesday was created. #tidytuesday is a weekly data visualization challenge. A great place to see examples from past #tidytuesday challenges is this interactive Shiny application. In this walkthrough, we focus on a) accessing data on #tidytuesday from Twitter and b) trying to understand the nature of the interactions that take place through #tidytuesday. We note that while we focused on #tidytuesday because we think it exemplifies the new kinds of learning that a data science toolkit allows an analyst to try to understand (through new data sources), we also chose this because it is straightforward to access data from Twitter, and we think you may find other opportunities to analyze data from Twitter in other cases. 10.1 Accessing data In this chapter, we access data using the rtweet package (Hawksey, 2019). Through rtweet, it is easy to access data from Twitter as long as one has a Twitter account. We will load the tidyverse and rtweet packages to get started. Here is an example of searching the most recent 1,000 tweets which include the hashtag #rstats. When you run this code, you will be prompted to authenticate your access via Twitter. library(tidyverse) library(rtweet) library(dataedu) library(randomNames) library(tidygraph) library(ggraph) rstats_tweets &lt;- search_tweets(&quot;#rstats&quot;) The search term can be easily changed to other hashtags - or other terms. To search for #tidytuesday tweets, we can simply replace #rstats with #tidytuesday. tidytuesday_tweets &lt;- search_tweets(&quot;#tidytuesday&quot;) A key point–and limitation–for how Twitter allows access to their data for the seven most recent days. There are a number of ways to access older data, which we discuss at the end of this chapter, though we focus on one way here: having access to the URLs to (or the status IDs for) tweets. We used this technique, which we describe in this chapter’s Technical Appendix A, along with other strategies for collecting historical data from Twitter. The data that we processed is available in the dataedu R package as the tt-tweets dataset. tt_tweets ## # A tibble: 4,418 x 90 ## user_id status_id created_at screen_name text source ## &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 115921… 11631542… 2019-08-18 18:22:42 MKumarYYC Firs… Twitt… ## 2 107332… 11632475… 2019-08-19 00:33:11 cizzart &quot;El … Twitt… ## 3 107332… 11450435… 2019-06-29 18:57:17 cizzart &quot;Pro… Twitt… ## 4 107332… 11168648… 2019-04-13 00:45:15 cizzart #Arg… Twitt… ## 5 107332… 11228824… 2019-04-29 15:17:02 cizzart &quot;Pes… Twitt… ## 6 107332… 11176387… 2019-04-15 04:00:17 cizzart &quot;Dat… Twitt… ## 7 107332… 11245531… 2019-05-04 05:55:32 cizzart &quot;El … Twitt… ## 8 107332… 11407021… 2019-06-17 19:25:50 cizzart &quot;#da… Twitt… ## 9 107332… 11325299… 2019-05-26 06:12:46 cizzart &quot;El … Twitt… ## 10 107332… 11233585… 2019-04-30 22:48:43 cizzart Visu… Twitt… ## # … with 4,408 more rows, and 84 more variables: display_text_width &lt;dbl&gt;, ## # reply_to_status_id &lt;chr&gt;, reply_to_user_id &lt;chr&gt;, ## # reply_to_screen_name &lt;chr&gt;, is_quote &lt;lgl&gt;, is_retweet &lt;lgl&gt;, ## # favorite_count &lt;int&gt;, retweet_count &lt;int&gt;, quote_count &lt;int&gt;, ## # reply_count &lt;int&gt;, hashtags &lt;list&gt;, symbols &lt;list&gt;, urls_url &lt;list&gt;, ## # urls_t.co &lt;list&gt;, urls_expanded_url &lt;list&gt;, media_url &lt;list&gt;, ## # media_t.co &lt;list&gt;, media_expanded_url &lt;list&gt;, media_type &lt;list&gt;, ## # ext_media_url &lt;list&gt;, ext_media_t.co &lt;list&gt;, ext_media_expanded_url &lt;list&gt;, ## # ext_media_type &lt;chr&gt;, mentions_user_id &lt;list&gt;, mentions_screen_name &lt;list&gt;, ## # lang &lt;chr&gt;, quoted_status_id &lt;chr&gt;, quoted_text &lt;chr&gt;, ## # quoted_created_at &lt;dttm&gt;, quoted_source &lt;chr&gt;, quoted_favorite_count &lt;int&gt;, ## # quoted_retweet_count &lt;int&gt;, quoted_user_id &lt;chr&gt;, quoted_screen_name &lt;chr&gt;, ## # quoted_name &lt;chr&gt;, quoted_followers_count &lt;int&gt;, ## # quoted_friends_count &lt;int&gt;, quoted_statuses_count &lt;int&gt;, ## # quoted_location &lt;chr&gt;, quoted_description &lt;chr&gt;, quoted_verified &lt;lgl&gt;, ## # retweet_status_id &lt;chr&gt;, retweet_text &lt;chr&gt;, retweet_created_at &lt;dttm&gt;, ## # retweet_source &lt;chr&gt;, retweet_favorite_count &lt;int&gt;, ## # retweet_retweet_count &lt;int&gt;, retweet_user_id &lt;chr&gt;, ## # retweet_screen_name &lt;chr&gt;, retweet_name &lt;chr&gt;, ## # retweet_followers_count &lt;int&gt;, retweet_friends_count &lt;int&gt;, ## # retweet_statuses_count &lt;int&gt;, retweet_location &lt;chr&gt;, ## # retweet_description &lt;chr&gt;, retweet_verified &lt;lgl&gt;, place_url &lt;chr&gt;, ## # place_name &lt;chr&gt;, place_full_name &lt;chr&gt;, place_type &lt;chr&gt;, country &lt;chr&gt;, ## # country_code &lt;chr&gt;, geo_coords &lt;list&gt;, coords_coords &lt;list&gt;, ## # bbox_coords &lt;list&gt;, status_url &lt;chr&gt;, name &lt;chr&gt;, location &lt;chr&gt;, ## # description &lt;chr&gt;, url &lt;chr&gt;, protected &lt;lgl&gt;, followers_count &lt;int&gt;, ## # friends_count &lt;int&gt;, listed_count &lt;int&gt;, statuses_count &lt;int&gt;, ## # favourites_count &lt;int&gt;, account_created_at &lt;dttm&gt;, verified &lt;lgl&gt;, ## # profile_url &lt;chr&gt;, profile_expanded_url &lt;chr&gt;, account_lang &lt;lgl&gt;, ## # profile_banner_url &lt;chr&gt;, profile_background_url &lt;chr&gt;, ## # profile_image_url &lt;chr&gt; 10.2 Preparing the data for the analysis Network data, in general, and network data from Twitter, particularly, requires some processing before it can be used in subsequent analyses. In particular, we are going to create an edgelist, a data structure that is especially helpful for understanding the nature of relationships. An edgelist looks like the following, where the sender denotes who is initiating the interaction or relationship, and the receiver is who is the recipient of it: ## # A tibble: 12 x 2 ## sender receiver ## &lt;chr&gt; &lt;chr&gt; ## 1 Luong, Asmita al-Fawaz, Abdus Salam ## 2 el-Hanif, Madeeha al-Basha, Ibtisaama ## 3 el-Hanif, Madeeha Stapleton, Leon ## 4 Liu, Timmy al-Basha, Ibtisaama ## 5 Liu, Timmy al-Fawaz, Abdus Salam ## 6 Liu, Timmy Miller, Katelenn ## 7 el-Kalil, Najwa Stapleton, Leon ## 8 el-Kalil, Najwa el-Hamidi, Shakeela ## 9 el-Kalil, Najwa Miller, Katelenn ## 10 al-Saab, Asad Campos, Sergio ## 11 Scott, Carrie Stapleton, Leon ## 12 Scott, Carrie Campos, Sergio In this edgelist, the sender could indicate, for example, someone who nominates someone else (the receiver) as someone they go to for help. The sender could also indicate someone who interacted with the receiver, such as by recognizing one of their tweets with a favorite (or a mention). In the following steps, we will work to create an edgelist from the data from #tidytuesday on Twitter. 10.3 Extracting mentions Let’s extract the mentions. There is a lot going on in the code below; let’s break it down line-by-line, starting with the mutate(): mutate(all_mentions = str_extract_all(text, regex)): this line uses a regex, or regular expression, to identify all of the usernames in the tweet (note: the regex comes from from this page) mutate(has_mention = ifelse(!is.na(all_mentions), TRUE, FALSE)): this line simply determines there are any mentions at all (or not); it is true if there is one or more mention unnest(all_mentions) this line uses a tidyr function, unnest() to move every mention to its own line, while keeping all of the other information the same (see more about unnest() here: https://tidyr.tidyverse.org/reference/unnest.html) regex &lt;- &quot;@([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\\\.)&quot; tt_tweets &lt;- tt_tweets %&gt;% mutate(all_mentions = str_extract_all(text, regex)) %&gt;% mutate(has_mention = if_else(!is.na(all_mentions), TRUE, FALSE)) %&gt;% unnest(all_mentions) Let’s put these into their own data frame, called mentions. mentions &lt;- tt_tweets %&gt;% filter(has_mention) %&gt;% mutate(all_mentions = str_trim(all_mentions)) %&gt;% select(sender = screen_name, all_mentions) 10.4 Putting the edgelist together An edgelist is a common social network analysis data structure that has columns for the “sender” and “receiver” of interactions, or relations. For example, someone “sends” the mention to someone who is mentioned, who can be considered to “receive” it. This will require one last processing step. Let’s look at our data as it is now. mentions ## # A tibble: 2,447 x 2 ## sender all_mentions ## &lt;chr&gt; &lt;chr&gt; ## 1 cizzart @eldestapeweb ## 2 cizzart @INDECArgentina ## 3 cizzart @ENACOMArgentina ## 4 cizzart @tribunalelecmns ## 5 cizzart @CamaraElectoral ## 6 cizzart @INDECArgentina ## 7 cizzart @tribunalelecmns ## 8 cizzart @CamaraElectoral ## 9 cizzart @AgroMnes ## 10 cizzart @AgroindustriaAR ## # … with 2,437 more rows What needs to happen to these to make them easier to work with in an edgelist? One step is to remove the “@” symbol from the columns we created and to save the results to a new tibble, edgelist. edgelist &lt;- mentions %&gt;% mutate(all_mentions= str_sub(all_mentions, start = 2)) %&gt;% select(sender, receiver = all_mentions) 10.5 Plotting the network Now that we have our edgelist, it is straightforward to plot the network. We’ll use the {tidygraph} and {ggraph} packages to visualize the data. We’ll use the as_tbl_graph() function, which (by default) identified the first column as the “sender” and the second as the “receiver.” Let’s look at the object it creates, too. g &lt;- as_tbl_graph(edgelist) g ## # A tbl_graph: 1340 nodes and 2447 edges ## # ## # A directed multigraph with 130 components ## # ## # Node Data: 1,340 x 1 (active) ## name ## &lt;chr&gt; ## 1 cizzart ## 2 dgwinfred ## 3 davidmasp ## 4 datawookie ## 5 jvaghela4 ## 6 FournierJohanie ## # … with 1,334 more rows ## # ## # Edge Data: 2,447 x 2 ## from to ## &lt;int&gt; &lt;int&gt; ## 1 1 619 ## 2 1 620 ## 3 1 621 ## # … with 2,444 more rows Next, we’ll use the ggraph() function. Run the code below, and then uncomment, one at a time, the next two lines (the two beginning geom_(), running the code after uncommenting each line). g %&gt;% ggraph() + geom_node_point() + # geom_node_text(aes(label = name)) + # geom_edge_link() + theme_graph() ## Using `stress` as default layout Finally, lets size the points based on a measure of centrality, typically a measure of how (potentially) influence an individual may be, based on the interactions observed. g %&gt;% mutate(centrality = centrality_authority()) %&gt;% ggraph() + geom_node_point(aes(size = centrality, color = centrality)) + scale_color_continuous(guide = &#39;legend&#39;) + geom_node_text(aes(label = name)) + geom_edge_link() + theme_graph() ## Using `stress` as default layout There is much more you can do with ggraph (and tidygraph); check out the ggraph tutorial here: https://ggraph.data-imaginist.com/ 10.6 Selection and influence models Behind these visualizations, though, there are also statistical models and methods that can help to understand what is going on with respect to particular relationships in a network in additional ways. One way to consider these models and methods is in terms of two processes at play in our relationships (cite). These two processes are commonly (though not exclusively) the focus of statistical analyses of networks. In addition to not being exclusive, they do not interact independently: they affect each other reciprocally (Xu, Frank, &amp; Penuel, 2018). They are: Selection: the processes regarding who chooses to have a relationship with whom Infuence: the processes regarding how who we have relationships with affects our behavior While these are complex, they can be studied with the type of data collected from asking people about their relationships (and possibly asking them about or studying their behavior–or measuring some outcome). Happily, the use of these methods has expanded along with R: many of the best tools for studying social networks are in the form of long-standing R packages. Additionally, while there are many potential nuances to studying selection and influence, these are models that can fundamentally be carried out with regression, or the linear model (or extensions of it). We describe these in Technical Appendix B for this chapter, as they do not use the tidytuesday dataset and are likely to be of interest to readers only after having mastered preparing and visualizing network data. 10.7 Technical Appendix A 10.7.1 Accessing historical data using status URLs Because the creator of the interactive web application for exploring #tidytuesday content, #tidytuesday.rocks, searched for (and archived) #tidytuesday tweets on a regular basis, a large data set from more than one year of weekly #tidytuesday challenges is available through the GitHub repository for the Shiny application. These Tweets (saved in the data directory) can be read with the following function raw_tidytuesday_tweets &lt;- read_delim( &quot;https://raw.githubusercontent.com/nsgrantham/tidytuesdayrocks/master/data/tweets.tsv&quot;, &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE ) ## Parsed with column specification: ## cols( ## status_url = col_character(), ## screen_name = col_character(), ## created_at = col_datetime(format = &quot;&quot;), ## favorite_count = col_double(), ## retweet_count = col_double(), ## dataset_id = col_character() ## ) Then the URL for the tweet (the status_url column) can be passed to a different rtweet function than the one we used, lookup_statuses(). Before we do this, there is one additional step to take. Because most of the Tweets are from more than seven days ago, Twitter requires an additional authentication step. In short, you need to use keys and tokens for the Twitter API, or application programming interface. This rtweet vignette on accessing keys and tokens explains the process. The end result will be that you will create a token using rtweet that you will use along with your rtweet function (in this case, lookup_statuses()): token &lt;- create_token( consumer_key = &lt; add - your - key - here &gt; , consumer_secret = &lt; add - your - secret - here &gt; ) # here, we pass the status_url variable from raw_tidytuesday_tweets as the statuses to lookup in the lookup_statuses() function, as well as our token tidytuesday_tweets &lt;- lookup_statuses(raw_tidytuesday_tweets$status_url, token = token) The end result will be a tibble, like that above for #rstats, for #tidytuesday tweets. 10.7.2 Accessing historical data when you do not have access to status URLs In the above case, we had access to the URLs for tweets because they were saved for the #tidytuesday.rocks Shiny. But, in many cases, historical data will not be available. There are two strategies that may be helpful. First is TAGS. TAGS is based in, believe it or not, Google Sheets, and it works great for collecting Twitter data over time - even a long period of time The only catch is that you need to setup and start to use a TAGS sheet in advance of the period for which you want to collect data. For example, you can start a TAGS archiver in August of one year, with the intention to collect data over the coming academic year; or, you can start a TAGS archiver before an academic conference for which you want to collect Tweets. A second option is the Premium API through Twitter(see here). This is an expensive option, but is one that can be done through rtweet, and can also access historical data, even if you haven not started a TAGS sheet and do not otherwise have access to the status URLs. 10.8 Technical Appendix B As noted above, there is much more to understanding interactions, and network analysis, beyond creating edgelists and visualizing network data (through the use of an edgelist). Two processes that are particularly important (and able to be studied with network data using R) are for influence and selection. 10.8.1 An example of influence First, let’s look at an example of influence. To do so, let’s create three different data frames. Here is what they should, at the end of the process, contain: A data frame indicating who the nominator and nominee for the relation (i.e., if Stefanie says that José is her friend, then Stefanie is the nominator and José the nominee) - as well as an optional variable indicating the weight, or strength, of their relation. This data frame and its type can be considered the basis for many types of social network analysis and is a common structure for network data: it is an edgelist. Data frames indicating the values of some behavior - an outcome - at two different time points. In this example, we create some example data that can be used to explore questions about how influence works. Let’s take a look at the merged data. What this data now contains is the first data frame, data1, with each nominees’ outcome at time 1 (yvar1). Note that we will find each nominators’ outcome at time 2 later on. data1 &lt;- data.frame( nominator = c(2, 1, 3, 1, 2, 6, 3, 5, 6, 4, 3, 4), nominee = c(1, 2, 2, 3, 3, 3, 4, 4, 4, 5, 6, 6), relate = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) ) data2 &lt;- data.frame(nominee = c(1, 2, 3, 4, 5, 6), yvar1 = c(2.4, 2.6, 1.1, -0.5, -3, -1)) data3 &lt;- data.frame(nominator = c(1, 2, 3, 4, 5, 6), yvar2 = c(2, 2, 1, -0.5, -2, -0.5)) 10.8.2 Joining the data Next, we’ll join the data into one data frame. Note that while this (and data wrangling, in general) is sometimes tedius, especially with large (and messy) sources of network data, it is a key step for being able to carry out network analysis - often, even for creating visualiations that are informative. Next, we’ll join the data into one data frame. Note that while this is sometimes tedius and time-consuming, especially with large sources of network data, it is a key step for being able to carry out network analysis - often, even for creating visualiations that are informative. data &lt;- left_join(data1, data2, by = &quot;nominee&quot;) data &lt;- data %&gt;% mutate(nominee = as.character(nominee)) # this makes merging later easier # calculate indegree in tempdata and merge with data tempdata &lt;- data.frame(table(data$nominee)) tempdata &lt;- tempdata %&gt;% rename(&quot;nominee&quot; = &quot;Var1&quot;, # rename the column &quot;Var1&quot; to &quot;nominee&quot; &quot;indegree&quot; = &quot;Freq&quot;) %&gt;% # rename the column &quot;Freq&quot; to &quot;indegree&quot; mutate(nominee = as.character(nominee)) # makes nominee a character data type, instead of a factor, which can cause problems data &lt;- left_join(data, tempdata, by = &quot;nominee&quot;) 10.8.2.1 Calculating an exposure term This is the key step that makes this model - a regression, or linear, model - one that is special. It is creating an exposure term. The idea is that the exposure term “captures” how your interactions with someone, over some period of time (between the first and second time points) impact some outcome. This model accounts for an individual’s initial report of the outcome, i.e., their time 1 prior value, so it is a model for change in some outcome. # Calculating exposure data &lt;- data %&gt;% mutate(exposure = relate * yvar1) # Calculating mean exposure mean_exposure &lt;- data %&gt;% group_by(nominator) %&gt;% summarize(exposure_mean = mean(exposure)) What this data frame - mean_exposure - contains is the mean of the outcome (in this case, yvar1) for all of the individuals the nominator had a relation with. As we need a final data set with mean_exposure,degree, yvar1, and yvar2 added, we’ll process the data a bit more. data2 &lt;- data2 %&gt;% rename(&quot;nominator&quot; = &quot;nominee&quot;) # rename nominee as nominator to merge these final_data &lt;- left_join(mean_exposure, data2, by = &quot;nominator&quot;) final_data &lt;- left_join(final_data, data3, by = &quot;nominator&quot;) # data3 already has nominator, so no need to change 10.8.2.2 Regression (linear model) Calculating the exposure term is the most distinctive and important step in carrying out influence models. Now, we can simply use a linear model to find out how much relations - as captured by the influence term - affect some outcome. model1 &lt;- lm(yvar2 ~ yvar1 + exposure_mean, data = final_data) summary(model1) ## ## Call: ## lm(formula = yvar2 ~ yvar1 + exposure_mean, data = final_data) ## ## Residuals: ## 1 2 3 4 5 6 ## 0.02946 -0.09319 0.09429 -0.02730 -0.02548 0.02222 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.11614 0.03445 3.371 0.0434 * ## yvar1 0.67598 0.02406 28.092 9.9e-05 *** ## exposure_mean 0.12542 0.03615 3.470 0.0403 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.08232 on 3 degrees of freedom ## Multiple R-squared: 0.9984, Adjusted R-squared: 0.9974 ## F-statistic: 945.3 on 2 and 3 DF, p-value: 6.306e-05 So, the influence model is used to study a key process for social network analysis, but it is one that is useful, because you can quantify, given what you measure and how you measure it, the network effect, something that is sometimes not considered, especially in education (Frank, 2009). It’s also fundamentally a regression. That’s really it, as the majority of the work goes into calculating the exposure term. 10.8.3 An example of selection Selection models are also commonly used - and are commonly of interest not only to researchers but also to administrators and teachers (and even to youth and students). Here, we briefly describe a few possible approaches for using a selection model. At its core, the selection model is a regression - albeit, one that is a generalization of one, namely, a logistic regression (sometimes termed a generalized linear model, because it is basically a regression but is one with an outcome that consists just of 0’s and 1’s). Thus, the most straight-away way to use a selection model is to use a logistic regression where all of the relations (note the relate variable in data1 above) are indicated with a 1. But, here is the important and challenging step: all of the possible relations (i.e., all of the relations that are possible between all of the individuals in a network) are indicated with a 0 in an edgelist. Note that, again, an edgelist is the preferred data structure for carrying out this analysis. This step involves some data wrangling, especially the idea of widening or lengthening a data frame. Once all of the relations are indicated with a 1 or a 0, then a simple linear regression can be used. Imagine that we are interested in whether individuals from the same group are more or less likely to interact than those from different groups; same could be created in the data frame based upon knowing which group both nominator and nominee are from: m_selection &lt;- glm(relate ~ 1 + same, data = edgelist1) While this is a straightforward way to carry out a selection model, there are some limitations to it. Namely, it does not account for individuals who send more (or less) nominations overall–and not considering this may mean other effects, like the one associated with being from the same group, are not accurate. A few extensions of the linear model - including those that can use data for which relationships are indicated with weights, not just 1’s and 0’s, have been developed. One type of model extends the logistic regression. It can be used for data that is not only 1’s and 0’s but also data that is normally distributed . It is the amen package available here. A particularly common one is an Exponential Random Graph Model, or an ERGM. An R package that makes estimating these easy is available here. That R package, ergm, is part of a powerful and often-used collection of packages, including those for working with network data (data that can begin with an edgelist, but may need additional processing that is challenging to do with edgelist data), statnet. A link to the statnet packages is here. 10.9 References Rosenberg, J. M., Greenhalgh, S. P., Koehler, M. J., Hamilton, E., &amp; Akcaoglu, M. (2016). An investigation of State Educational Twitter Hashtags (SETHs) as affinity spaces. E-Learning and Digital Media, 13(1-2), 24-44. http://dx.doi.org/10.1177/2042753016672351 Spillane, J., Kim, Chong Min,Frank, K.A. 2012. “Instructional Advice and Information Providing and Receiving Behavior in Elementary Schools: Exploring Tie Formation as a Building Block in Social Capital Development.” American Educational Research Journal. Vol 49 no. 6 1112-1145 Trust, T., Krutka, D. G., &amp; Carpenter, J. P. (2016). “Together we are better”: Professional learning networks for teachers. Computers &amp; education, 102, 15-34. "],
["walkthrough-5-introduction-to-aggregate-data.html", "11 Walkthrough 5: Introduction to Aggregate Data 11.1 Data 11.2 Selecting Data 11.3 Importing Data 11.4 Reviewing the dataset 11.5 Analyzing spread 11.6 Conclusion 11.7 Appendix", " 11 Walkthrough 5: Introduction to Aggregate Data A common situation encountered when using data for analyzing the education sector, particularly by analysts who are not directly working with schools or districts, is the prevalence of publicly available, aggregate data. Aggregate data refers to numerical or non-numerical information that is (1) collected from multiple sources and/or on multiple measures, variables, or individuals and (2) compiled into data summaries or summary reports, typically for the purposes of public reporting or statistical analysis. Example of publicly available aggregate data include school-level graduation rates, state test proficiency scores by grade and subject, or averaged survey responses. Aggregated datasets are essential both for accountability purposes and for providing useful information about schools and districts to those who are monitoring them. For example, district administrators might aggregate row-level (also known as individual-level or student-level) enrollment reports over time. This allows them to see how many students enroll in each school, in the district overall, and any grade-level variation. Depending on their state, the district administrator might submit these aggregate data to their state education agency for reporting purposes. These datasets might be posted on the state’s department of education website for anybody to download and use. Federal and international education datasets provide additional context for evaluating education systems. In the US, some federal datasets aim to consolidate important metrics from all states. This can be quite useful because each state has its own repository of data and to go through each state to download a particular metric is a significant effort. The federal government also funds assessments and surveys which are disseminated to the public. However, the federal datasets often have more stringent data requirements than the states, so the datasets may be less usable. For education data practitioners, these reports and datasets can be analyzed to answer questions related to their field of interest. However, publicly available, aggregate datasets are large and often suppressed to protect privacy. Sometimes they are already a couple of years old by the time they’re released. Because of their coarseness, they can be difficult to interpret and use. Generally, aggregated data are generally used to surface of broader trends and patterns in education as opposed to diagnosing underlying issues or making causal statements. It is very important that we consider the limitations of aggregate data before analyzing them. In this chapter, we will walk through how running analysis on a publicly available dataset can help education data practitioners understand the landscape of needs and opportunities in the field of education. As opposed to causal analysis, which aims to assess the root cause of an phenomenon or the effects of an intervention, we use analysis on an aggregate dataset to find out whether there is a phenomenon, what it is, and what we’d be trying to solve through interventions. Analysis of aggregate data can help us identify patterns that may not have previously been known. When we have gained new insight, we can create research questions, craft hypotheses around our findings, and make recommendations on how to make improvements for the future. 11.0.1 Disaggregating Aggregated Data Aggregated data can tell us many things, but in order for us to examine subgroups and their information, we must have data disaggregated by the subgroups we hope to analyze. This data is still aggregated from row-level data but provides data on smaller components. Common disaggregations for students include gender, race/ethnicity, socioeconomic status, English learner designation, and whether they are served under the Individuals with Disabilities Education Act (IDEA). 11.0.2 Disaggregating Data and Equity Disaggregated data is essential to monitor equity in educational resources and outcomes. If only aggregate data is provided, we are unable to distinguish how different groups of students are doing and what support they need. With disaggregated data, we can identify where solutions are needed to solve disparities in opportunity, resources, and treatment. It is important to define what equity means to your team so you know whether you are meeting your equity goals. 11.1 Data There are many education-related, publicly available aggregate datasets. On the international level, perhaps the most well known is: Programme for International Student Assessment (PISA), which measures 15-year-old school pupils’ scholastic performance on mathematics, science, and reading. On the federal level, examples include: Civil Rights Data Collection (CRDC), which reports many different variables on educational program and services disaggregated by race/ethnicity, sex, limited English proficiency, and disability. These data are school-level. Common Core of Data (CCD), which is the US Department of Education’s primary database on public elementary and secondary education. EdFacts, which includes state assessments and adjusted cohort graduation rates. These data are school- and district-level. Integrated Postsecondary Education Data System (IPEDS), which is the US Department of Education’s primary database on postsecondary education. National Assessment for Educational Progress (NAEP) Data, an assessment of educational progress in the United States. Often called the “nation’s report card.” The NAEP reading and mathematics assessments are administered to a representative sample of fourth- and eighth-grade students in each state every two years. On the state and district level, examples include: California Department of Education, which is the state department of education website. It includes both downloadable CSV files and “Data Quest”, which lets you query the data online. Minneapolis Public Schools, which is a district-level website with datasets beyond those listed in the state website. 11.2 Selecting Data For the purposes of this walkthrough, we will be looking at a particular school district’s data. This district reports their student demographics in a robust, complete way. Not only do they report the percentage of students in a subgroup, but they also include the number of students in each subgroup. This allows a deep look into their individual school demographics. Their reporting of the composition of their schools provides an excellent opportunity to explore inequities in a system. 11.3 Importing Data # set up libraries library(tidyverse) library(janitor) library(dataedu) ROpenSci created the {tabulizer} package which provides R bindings to the Tabula java library, which can be used to computationaly extract tables from PDF documents. A required package to load {tabulizer} is called {RJava}. Unfortunately, installing {RJava} on Macs can be very tedious. If you find yourself unable to install {tabulizer}, or would like to skip to the data processing, the data pulled from the PDFs is also saved so we can skip the steps requiring RJava. library(tabulizer) tabulizer pulls the PDF data into lists using extract_tables(). # race data race_pdf &lt;- extract_tables(&quot;https://studentaccounting.mpls.k12.mn.us/uploads/mps_fall2018_racial_ethnic_by_school_by_grade.pdf&quot;) It is important to consistently check what we’re doing with the actual PDF’s to ensure we’re getting the data that we need. We then transform the list to a data frame with map(as_tibble). The slice() in map_df() removes unnecessary rows from the PDF. Now we create readable column names using set_names(). race_df &lt;- race_pdf %&gt;% map(as_tibble) %&gt;% map_df( ~ slice(.,-1:-2)) %&gt;% set_names( c( &quot;school_group&quot;, &quot;school_name&quot;, &quot;grade&quot;, &quot;na_num&quot;, # native american number of students &quot;na_pct&quot;, # native american percentage of students &quot;aa_num&quot;, # african american number of students &quot;aa_pct&quot;, # african american percentage &quot;as_num&quot;, # asian number of students &quot;as_pct&quot;, # asian percentage &quot;hi_num&quot;, # hispanic number of students &quot;hi_pct&quot;, # hispanic percentage &quot;wh_num&quot;, # white number of students &quot;wh_pct&quot;, # white percentage &quot;pi_pct&quot;, # pacific islander percentage &quot;blank_col&quot;, &quot;tot&quot; # total number of students ) ) For the Race/Ethnicity table, we want the totals for each district school as we won’t be looking at grade-level variation. When analyzing the PDF, we see the totals have “Total” in the School Name. We clean up this dataset by: Removing unnecessary or blank columns using select(). Negative selections means those columns will be removed. Removing all Grand Total rows (otherwise they’ll show up in our data, when we just want district-level data) using filter(). We keep schools that have “Total” in the name but remove any rows that are Grand Total. Then we trim white space from strings. The data in the “percentage” columns are provided with a percentage sign. This means we will have to remove all of them to be able to do math on these columns (for example, adding them together). Also, we want to divide the numbers by 100 so they are true percentages. race_clean &lt;- race_df %&gt;% select(-school_group,-grade,-pi_pct,-blank_col) %&gt;% #1 filter(str_detect(school_name, &quot;Total&quot;), #2 school_name != &quot;Grand Total&quot;) %&gt;% mutate(school_name = str_replace(school_name, &quot;Total&quot;, &quot;&quot;)) %&gt;% mutate_if(is.character, trimws) %&gt;% #3 mutate_at(vars(contains(&quot;pct&quot;)), funs(as.numeric(str_replace(., &quot;%&quot;, &quot;&quot;)) / 100)) We will import the Free Reduced Price Lunch (FRPL) PDF’s now. FRPL stands for Free/Reduced Price Lunch, [often used as a proxy for poverty]((https://nces.ed.gov/blogs/nces/post/free-or-reduced-price-lunch-a-proxy-for-poverty). Students from a household with an income up to 185 percent of the poverty threshold are eligible for free or reduced price lunch. (Sidenote: Definitions are very important in disaggregated data. FRPL is used because it’s ubiquitous and reporting is mandated but there is debate as to whether it actually reflects the level of poverty among students.) # frpl data frpl_pdf &lt;- extract_tables(&quot;https://studentaccounting.mpls.k12.mn.us/uploads/fall_2018_meal_eligiblity_official.pdf&quot;) Similar to the Race/Ethnicity PDF, there are rows that we don’t need from each page, which we remove using slice(). frpl_df &lt;- frpl_pdf %&gt;% map(as_tibble) %&gt;% map_df( ~ slice(.,-1)) %&gt;% set_names( c( &quot;school_name&quot;, &quot;not_eligible_num&quot;, # number of non-eligible students, &quot;reduce_num&quot;, # number of students receiving reduced price lunch &quot;free_num&quot;, # number of students receiving free lunch &quot;frpl_num&quot;, # total number of students &quot;frpl_pct&quot; # free/reduced price lunch percentage ) ) To clean it up, we remove the rows that are blank. When looking at the PDF, we notice that there are aggregations inserted into the table that are not district-level. For example, they have included ELM K_08, presumably to aggregate FRPL numbers up to the K-8 level. Although this is useful data, we don’t need it for this district-level analysis. There are different ways we can remove these rows but we will just filter them out. frpl_clean &lt;- frpl_df %&gt;% filter( school_name != &quot;&quot;, # remove blanks!school_name %in% c( &quot;ELM K_08&quot;, # filter out the rows in this list &quot;Mid Schl&quot;, &quot;High Schl&quot;, &quot;Alt HS&quot;, &quot;Spec Ed Total&quot;, &quot;Cont Alt Total&quot;, &quot;Hospital Sites Total&quot;, &quot;Dist Total&quot; ) ) %&gt;% mutate(frpl_pct = as.numeric(str_replace(frpl_pct, &quot;%&quot;, &quot;&quot;)) / 100) Because we want to look at race/ethnicity data in conjunction with free/reduced price lunch percentage, we join the two datasets by the name of the school. Use mutate_at() to specify columns to which to apply a function (in this case, as.numeric.) # joined data joined_df &lt;- left_join(race_clean, frpl_clean, by = c(&quot;school_name&quot;)) %&gt;% mutate_at(2:17, as.numeric) Did you notice? The total number of students from the Race/Ethnicity table does not match the total number of students from the FRPL table, even though they’re referring to the same districts in the same year. Why? Perhaps the two datasets were created by different people, who used different rules when aggregating the dataset. Perhaps the counts were taken at different times of the year, when students may have moved around in the meantime. We don’t know but it does require us to make strategic decisions about which data we consider the ‘truth’ for our analysis. Now we move on to the fun part of creating new columns based on the merged dataset. We want to calculate, for each race, the number of students in ‘high poverty’ schools. This is defined by NCES as schools that are over 75% FRPL. When a school has over 75% FRPL, we count the number of students for that particular race under the variable [racename]_povnum. The janitor package has a handy adorn_totals() function that sums the columns for you. This is important because we want a weighted average of students in each category, so we need the total number of students in each group. We create the weighted average of the percentage of each race by dividing the number of students by race by the total number of students. To get FRPL percentage for all schools, we have to recalculate frpl_pct. To calculate the percentage of students by race who are in high poverty schools, we must divide the number of students in high poverty schools by the total number of students in that race. merged_df &lt;- joined_df %&gt;% mutate( hi_povnum = case_when(frpl_pct &gt; .75 ~ hi_num), #2 aa_povnum = case_when(frpl_pct &gt; .75 ~ aa_num), wh_povnum = case_when(frpl_pct &gt; .75 ~ wh_num), as_povnum = case_when(frpl_pct &gt; .75 ~ as_num), na_povnum = case_when(frpl_pct &gt; .75 ~ na_num) ) %&gt;% adorn_totals() %&gt;% mutate( na_pct = na_num / tot, #4 aa_pct = aa_num / tot, as_pct = as_num / tot, hi_pct = hi_num / tot, wh_pct = wh_num / tot, frpl_pct = (free_num + reduce_num) / frpl_num, hi_povsch = hi_povnum / hi_num[which(school_name == &quot;Total&quot;)], #5 aa_povsch = aa_povnum / aa_num[which(school_name == &quot;Total&quot;)], as_povsch = as_povnum / as_num[which(school_name == &quot;Total&quot;)], wh_povsch = wh_povnum / wh_num[which(school_name == &quot;Total&quot;)], na_povsch = na_povnum / na_num[which(school_name == &quot;Total&quot;)] ) To facilitate the creation of ggplots later on, we put this data in tidy format. # tidy data tidy_df &lt;- merged_df %&gt;% gather(category, value,-school_name) Running the above code, particularly the download of the PDFs, takes a lot of time. We’ve saved copies of the merged and tidy data in the data folder and can be accessed by running the code below. # write_csv(tidy_df, here::here(&quot;data&quot;, &quot;agg_data&quot;, &quot;tidy_df.csv&quot;)) tidy_df &lt;- read_csv(here::here(&quot;data&quot;, &quot;agg_data&quot;, &quot;tidy_df.csv&quot;)) ## Parsed with column specification: ## cols( ## school_name = col_character(), ## category = col_character(), ## value = col_double() ## ) # write_csv(merged_df, here::here(&quot;data&quot;, &quot;agg_data&quot;, &quot;merged_df.csv&quot;)) merged_df &lt;- read_csv(here::here(&quot;data&quot;, &quot;agg_data&quot;, &quot;merged_df.csv&quot;)) ## Parsed with column specification: ## cols( ## .default = col_double(), ## school_name = col_character() ## ) ## See spec(...) for full column specifications. 11.4 Reviewing the dataset 11.4.1 Discovering distributions What do the racial demographics in this district look like? For this, a barplot can quickly visualize the different proportion of subgroups. tidy_df %&gt;% filter(school_name == &quot;Total&quot;, str_detect(category, &quot;pct&quot;), category != &quot;frpl_pct&quot;) %&gt;% mutate(category = factor( category, levels = c(&quot;aa_pct&quot;, &quot;wh_pct&quot;, &quot;hi_pct&quot;, &quot;as_pct&quot;, &quot;na_pct&quot;) )) %&gt;% ggplot(aes(x = category, y = value)) + geom_bar(stat = &quot;identity&quot;, aes(fill = category)) + theme(legend.position = &quot;none&quot;) + xlab(&quot;Subgroup&quot;) + ylab(&quot;Percentage of Population&quot;) + scale_x_discrete(labels = c(&quot;Black&quot;, &quot;White&quot;, &quot;Hispanic&quot;, &quot;Asian&quot;, &quot;Native Am.&quot;)) + scale_y_continuous(labels = scales::percent) + scale_fill_dataedu() + theme_dataedu() When we look at these data, the district looks very diverse. Almost 40% of students are Black and around 36% are White. Note: this matches the percentages provided in the original PDF’s. This shows our calculations above were accurate. Hooray! In terms of free/reduced price lunch, we have that calculated under frpl_pct: tidy_df %&gt;% filter(category == &quot;frpl_pct&quot;, school_name == &quot;Total&quot;) %&gt;% knitr::kable() school_name category value Total frpl_pct 0.5685631 56.9% of the students are eligible for FRPL, compared to the US average of 52.1%. This also matches the PDF’s. Great! Now, we dig deeper to see if there is more to the story. 11.5 Analyzing spread Another view of the data can be visualizing the distribution of students with different demographics across schools. Here is a histogram for the percentage of White students within the schools for which we have data. merged_df %&gt;% filter(school_name != &quot;Total&quot;) %&gt;% ggplot(aes(x = wh_pct)) + geom_histogram(breaks = seq(0, 1, by = .1), fill = dataedu_cols(&quot;darkblue&quot;)) + xlab(&quot;White Percentage&quot;) + ylab(&quot;Count&quot;) + scale_x_continuous(labels = scales::percent) + theme(legend.position = &quot;none&quot;) + theme_dataedu() # hist(merged_df$wh_pct)$counts[1:9] # counting number of schools in the first bin (0-10%) 26 of the 74 (35%) of schools have between 0-10% White students. This implies that even though the school district may be diverse, the demographics are not evenly distributed across the schools. More than half of schools enroll fewer than 30% of White students even though White students make up 35% of the district student population. The school race demographics are not representative of the district populations but does that hold for socioeconomic status as well? 11.5.1 Creating categories High-poverty schools are defined as public schools where more than 75% of the students are eligible for FRPL. According to NCES, 24% of public school students attended high-poverty schools. However, different subgroups were overrepresented and underrepresented within the high poverty schools. Is this the case for this district? tidy_df %&gt;% filter(school_name == &quot;Total&quot;, str_detect(category, &quot;povsch&quot;)) %&gt;% mutate(category = factor( category, levels = c( &quot;hi_povsch&quot;, &quot;na_povsch&quot;, &quot;aa_povsch&quot;, &quot;as_povsch&quot;, &quot;wh_povsch&quot; ) )) %&gt;% ggplot(aes(x = category, y = value)) + geom_bar(stat = &quot;identity&quot;, aes(fill = factor(category))) + theme_dataedu() + xlab(&quot;Subgroup&quot;) + ylab(&quot;Percentage in High Poverty Schools&quot;) + scale_x_discrete(labels = c(&quot;Hispanic&quot;, &quot;Native Am.&quot;, &quot;Black&quot;, &quot;Asian&quot;, &quot;White&quot;)) + scale_y_continuous(labels = scales::percent) + theme(legend.position = &quot;none&quot;) + theme_dataedu() + scale_fill_dataedu() 8% of White students attend high poverty schools, compared to 43% of Black students, 39% of Hispanic students, 28% of Asian students, and 45% of Native American students. We can conclude these students are disproportionally attending high poverty schools. 11.5.2 Reveal relationships Let’s explore what happens when we correlate race and FRPL percentage by school. merged_df %&gt;% filter(school_name != &quot;Total&quot;) %&gt;% ggplot(aes(x = wh_pct, y = frpl_pct)) + geom_point(color = dataedu_cols(&quot;green&quot;)) + theme_dataedu() + xlab(&quot;White Percentage&quot;) + ylab(&quot;FRPL Percentage&quot;) + scale_y_continuous(labels = scales::percent) + scale_x_continuous(labels = scales::percent) + theme(legend.position = &quot;none&quot;) ## Warning: Removed 1 rows containing missing values (geom_point). Related to the result above, there is a strong negative correlation between FRPL percentage and the percentage of White students in a school. That is, high poverty schools have a lower percentage of White students and low poverty schools have a higher percentage of White students. 11.6 Conclusion Because of the disaggregated data this district provides, we can go deeper than the average of demographics across the district and see what it looks like on the school level. These views display that (1) there exists a distribution of race/ethnicity within schools that are not representative of the district, (2) that students of color are overrepresented in high poverty schools, and (3) there is a negative relationship between the percentage of White students in a school and the percentage of students eligible for FRPL. According to the Urban Institute, the disproportionate percentage of students of color attending high poverty schools “is a defining feature of almost all Midwestern and northeastern metropolitan school systems.” Among other issues, high poverty schools tend to lack the educational resources—like highly qualified and experienced teachers, low student-teacher ratios, college prerequisite and advanced placement courses, and extracurricular activities—available in low-poverty schools. This has a huge impact on these students and their futures. In addition, research shows that racial and socioeconomic diversity in schools can provide students with a range of cognitive and social benefits. Therefore, this deep segregation that exists in the district can have adverse effects on students. As an education data practicioner, we can use these data to suggest interventions for what we can do to improve equity in the district. In addition, we can advocate for more datasets such as these, which allow us to dig deep. 11.7 Appendix Loeb, S., Dynarski, S., McFarland, D., Morris, P., Reardon, S., &amp; Reber, S. (2017). Descriptive analysis in education: A guide for researchers. (NCEE 2017–4023). Washington, DC: U.S. Department of Education, Institute of Education Sciences, National Center for Education Evaluation and Regional Assistance. National Forum on Education Statistics. (2016). Forum Guide to Collecting and Using Disaggregated Data on Racial/Ethnic Subgroups. (NFES 2017-017). U.S. Department of Education. Washington, DC: National Center for Education Statistics. "],
["exploring-students-with-disabilities-counts-over-time.html", "12 Exploring Students With Disabilities Counts Over Time 12.1 Vocabulary 12.2 Introduction 12.3 Special Education Child Count and Environment Data 12.4 Reading In One Dataset 12.5 Reading In Many Datasets 12.6 Cleaning the Dataset 12.7 How Have Child Counts Changed Over Time? 12.8 Aggregate Data as Context for Student Data 12.9 References", " 12 Exploring Students With Disabilities Counts Over Time 12.1 Vocabulary Read in Aggregate data Vector List Tidy format Subset select_at mutate Statistical model Aggregate data Student-level data 12.2 Introduction We chose to use a publicly available dataset for this walkthrough. Like most public datasets, this one contains aggregates data. This means that someone totaled up the student counts so it doesn’t reveal any private information. 12.3 Special Education Child Count and Environment Data You can download the datasets for this walkthrough on the United States Department of Education website: https://www2.ed.gov/programs/osepidea/618-data/state-level-data-files/index.html#bccee. 12.4 Reading In One Dataset When you are analyzing multiple datasets that all have the same structure, you can read in each dataset using one code chunk. This code chunk will store each dataset as an element of a list. Before doing that, you should explore one of the datasets to see what you can learn about its structure. Clues from this exploration inform how you read in all the datasets at once later on. For example, we can see that the first dataset has some lines at the top that contain no data: ## # A tibble: 16,234 x 31 ## `Extraction Dat… `6/12/2013` X3 X4 X5 X6 X7 X8 X9 X10 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Updated: 2/12/2014 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 Revised: &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 Year State Name SEA … SEA … Amer… Asia… Blac… Hisp… Nati… Two … ## 5 2012 ALABAMA Corr… All … - - - - - - ## 6 2012 ALABAMA Home All … 1 1 57 12 0 2 ## 7 2012 ALABAMA Home… All … - - - - - - ## 8 2012 ALABAMA Insi… All … - - - - - - ## 9 2012 ALABAMA Insi… All … - - - - - - ## 10 2012 ALABAMA Insi… All … - - - - - - ## # … with 16,224 more rows, and 21 more variables: X11 &lt;chr&gt;, X12 &lt;chr&gt;, ## # X13 &lt;chr&gt;, X14 &lt;chr&gt;, X15 &lt;chr&gt;, X16 &lt;chr&gt;, X17 &lt;chr&gt;, X18 &lt;chr&gt;, ## # X19 &lt;chr&gt;, X20 &lt;chr&gt;, X21 &lt;chr&gt;, X22 &lt;chr&gt;, X23 &lt;chr&gt;, X24 &lt;chr&gt;, ## # X25 &lt;chr&gt;, X26 &lt;chr&gt;, X27 &lt;chr&gt;, X28 &lt;chr&gt;, X29 &lt;chr&gt;, X30 &lt;chr&gt;, X31 &lt;chr&gt; The rows containing “Extraction Date:”, “Updated:” and “Revised:” aren’t actually rows. They’re notes the authors left at the top of the dataset to show when the dataset was changed. read_csv uses the first row as the variable names unless told otherwise, so we need to tell read_csv to skip those lines using the skip argument. If we don’t, read_csv assumes the very first line–the one that says “Extraction Date:”–is the correct row of variable names. That’s why calling read_csv without the skip argument results in column names like X4. When there’s no obvious column name to read in, read_csv names them X[...] and let’s you know in a warning message. Try using skip = 4 in your call to read_csv: ## Parsed with column specification: ## cols( ## .default = col_character(), ## Year = col_double() ## ) ## See spec(...) for full column specifications. ## Warning: 3 parsing failures. ## row col expected actual file ## 16228 Year a double ------------------- &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2012.csv&#39; ## 16229 Year a double - Data not available &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2012.csv&#39; ## 16230 Year a double x Data supressed due to small cell size &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2012.csv&#39; ## # A tibble: 16,230 x 31 ## Year `State Name` `SEA Education … `SEA Disability… `American India… ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2012 ALABAMA Correctional Fa… All Disabilities - ## 2 2012 ALABAMA Home All Disabilities 1 ## 3 2012 ALABAMA Homebound/Hospi… All Disabilities - ## 4 2012 ALABAMA Inside regular … All Disabilities - ## 5 2012 ALABAMA Inside regular … All Disabilities - ## 6 2012 ALABAMA Inside regular … All Disabilities - ## 7 2012 ALABAMA Other Location … All Disabilities 7 ## 8 2012 ALABAMA Other Location … All Disabilities 1 ## 9 2012 ALABAMA Parentally Plac… All Disabilities - ## 10 2012 ALABAMA Residential Fac… All Disabilities 0 ## # … with 16,220 more rows, and 26 more variables: `Asian Age 3-5` &lt;chr&gt;, `Black ## # or African American Age 3-5` &lt;chr&gt;, `Hispanic/Latino Age 3-5` &lt;chr&gt;, ## # `Native Hawaiian or Other Pacific Islander Age 3-5` &lt;chr&gt;, `Two or More ## # Races Age 3-5` &lt;chr&gt;, `White Age 3-5` &lt;chr&gt;, `Female Age 3 to 5` &lt;chr&gt;, ## # `Male Age 3 to 5` &lt;chr&gt;, `LEP Yes Age 3 to 5` &lt;chr&gt;, `LEP No Age 3 to ## # 5` &lt;chr&gt;, `Age 3 to 5` &lt;chr&gt;, `Age 6-11` &lt;chr&gt;, `Age 12-17` &lt;chr&gt;, `Age ## # 18-21` &lt;chr&gt;, `Ages 6-21` &lt;chr&gt;, `LEP Yes Age 6 to 21` &lt;chr&gt;, `LEP No Age 6 ## # to 21` &lt;chr&gt;, `Female Age 6 to 21` &lt;chr&gt;, `Male Age 6 to 21` &lt;chr&gt;, ## # `American Indian or Alaska Native Age 6 to21` &lt;chr&gt;, `Asian Age 6 ## # to21` &lt;chr&gt;, `Black or African American Age 6 to21` &lt;chr&gt;, `Hispanic/Latino ## # Age 6 to21` &lt;chr&gt;, `Native Hawaiian or Other Pacific Islander Age 6 ## # to21` &lt;chr&gt;, `Two or more races Age 6 to21` &lt;chr&gt;, `White Age 6 to21` &lt;chr&gt; The skip argument told read_csv to make the line containing “Year”, “State Name”, and so on as the first line. The result is a dataset that has “Year”, “State Name”, and so on as variable names. 12.5 Reading In Many Datasets Will the read_csv and skip = 4 combination work on all our datasets? To find out, we’ll use this strategy: Store a vector of filenames and paths in a list. These paths point to our datasets Pass the list of filenames as arguments to read_csv using purrr::map, including skip = 4, in our read_csv call Examine the new list of datasets to see if the variable names are correct Imagine a widget-making machine that works by acting on raw materials it receives on a conveyer belt. This machine executes one set of instructions on each of the raw materials it receives. You are the operator of the machine and you design instructions to get a widget out of the raw materials. Your plan might look something like this: Raw materials: a list of filenames and their paths Widget-making machine: purrr:map() Widget-making instructions: `read_csv(path, skip = 4) Expected widgets: a list of datasets Let’s create the raw materials first. Our raw materials will be file paths to each of the CSVs we want to read. Use list.files to make a vector of filename paths and name that vector filenames. list.files returns a vector of file names in the folder specified in the path argument. When we set the full.names argument to “TRUE”, we get a full path of these filenames. This will be useful later when we need the file names and their paths to read our data in. # Get filenames from the data folder filenames &lt;- list.files(path = here::here(&quot;data&quot;, &quot;longitudinal_data&quot;), full.names = TRUE) # A list of filenames and paths filenames ## [1] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2012.csv&quot; ## [2] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2013.csv&quot; ## [3] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2014.csv&quot; ## [4] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2015.csv&quot; ## [5] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2016.csv&quot; ## [6] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2017-18.csv&quot; That made a vector of six filenames, one for each year of child count data stored in the data folder. Now pass our raw materials, the vector called filenames, to our widget-making machine called map and give the machine the instructions read_csv(., skip = 4). Name the list of widgets it cranks out all_files: # Pass filenames to map and read_csv all_files &lt;- filenames %&gt;% map(., ~ read_csv(., skip = 4)) ## Parsed with column specification: ## cols( ## .default = col_character(), ## Year = col_double() ## ) ## See spec(...) for full column specifications. ## Warning: 3 parsing failures. ## row col expected actual file ## 16228 Year a double ------------------- &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2012.csv&#39; ## 16229 Year a double - Data not available &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2012.csv&#39; ## 16230 Year a double x Data supressed due to small cell size &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2012.csv&#39; ## Parsed with column specification: ## cols( ## .default = col_character(), ## Year = col_double() ## ) ## See spec(...) for full column specifications. ## Warning: 4 parsing failures. ## row col expected actual file ## 16229 Year a double ------------------- &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2013.csv&#39; ## 16230 Year a double - Data not available &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2013.csv&#39; ## 16231 Year a double x Data suppressed due to small cell size &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2013.csv&#39; ## 16232 Year a double * Data flagged due to questionable data quality. &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2013.csv&#39; ## Parsed with column specification: ## cols( ## .default = col_character(), ## Year = col_double() ## ) ## See spec(...) for full column specifications. ## Warning: 3 parsing failures. ## row col expected actual file ## 16229 Year a double ------------------- &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2014.csv&#39; ## 16230 Year a double - Data not available &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2014.csv&#39; ## 16231 Year a double x Data suppressed due to small cell size &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2014.csv&#39; ## Parsed with column specification: ## cols( ## .default = col_character(), ## Year = col_double() ## ) ## See spec(...) for full column specifications. ## Warning: 4 parsing failures. ## row col expected actual file ## 16229 Year a double ------------------- &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2015.csv&#39; ## 16230 Year a double - Data not available &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2015.csv&#39; ## 16231 Year a double x Data suppressed due to small cell size &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2015.csv&#39; ## 16232 Year a double * Data flagged due to questionable data quality &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2015.csv&#39; ## Warning: Duplicated column names deduplicated: &#39;-&#39; =&gt; &#39;-_1&#39; [6], &#39;-&#39; =&gt; &#39;- ## _2&#39; [7], &#39;-&#39; =&gt; &#39;-_3&#39; [8], &#39;-&#39; =&gt; &#39;-_4&#39; [9], &#39;-&#39; =&gt; &#39;-_5&#39; [10], &#39;-&#39; =&gt; &#39;- ## _6&#39; [11], &#39;-&#39; =&gt; &#39;-_7&#39; [12], &#39;-&#39; =&gt; &#39;-_8&#39; [13], &#39;-&#39; =&gt; &#39;-_9&#39; [14], &#39;-&#39; =&gt; &#39;- ## _10&#39; [15], &#39;-&#39; =&gt; &#39;-_11&#39; [16], &#39;-&#39; =&gt; &#39;-_12&#39; [17], &#39;-&#39; =&gt; &#39;-_13&#39; [18], &#39;-&#39; =&gt; ## &#39;-_14&#39; [19], &#39;0&#39; =&gt; &#39;0_1&#39; [21], &#39;0&#39; =&gt; &#39;0_2&#39; [22], &#39;0&#39; =&gt; &#39;0_3&#39; [23], &#39;0&#39; =&gt; ## &#39;0_4&#39; [24], &#39;0&#39; =&gt; &#39;0_5&#39; [25], &#39;0&#39; =&gt; &#39;0_6&#39; [26], &#39;0&#39; =&gt; &#39;0_7&#39; [27], &#39;0&#39; =&gt; ## &#39;0_8&#39; [28], &#39;4&#39; =&gt; &#39;4_1&#39; [35], &#39;0&#39; =&gt; &#39;0_9&#39; [36], &#39;0&#39; =&gt; &#39;0_10&#39; [40], &#39;77&#39; =&gt; ## &#39;77_1&#39; [41], &#39;1&#39; =&gt; &#39;1_1&#39; [42], &#39;0&#39; =&gt; &#39;0_11&#39; [44], &#39;0&#39; =&gt; &#39;0_12&#39; [45], &#39;0&#39; =&gt; ## &#39;0_13&#39; [47], &#39;0&#39; =&gt; &#39;0_14&#39; [48], &#39;0&#39; =&gt; &#39;0_15&#39; [49] ## Parsed with column specification: ## cols( ## .default = col_character(), ## `2016` = col_double() ## ) ## See spec(...) for full column specifications. ## Warning: 4 parsing failures. ## row col expected actual file ## 16227 2016 a double ------------------- &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2016.csv&#39; ## 16228 2016 a double - Data not available &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2016.csv&#39; ## 16229 2016 a double x Data suppressed due to small cell size &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2016.csv&#39; ## 16230 2016 a double * Data flagged due to questionable data quality &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2016.csv&#39; ## Parsed with column specification: ## cols( ## .default = col_character(), ## Year = col_double() ## ) ## See spec(...) for full column specifications. ## Warning: 4 parsing failures. ## row col expected actual file ## 16228 Year a double ------------------- &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2017-18.csv&#39; ## 16229 Year a double - Data not available &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2017-18.csv&#39; ## 16230 Year a double x Data suppressed due to small cell size &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2017-18.csv&#39; ## 16231 Year a double * Data flagged due to questionable data quality &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2017-18.csv&#39; It is important to think ahead here. The goal is to combine the datasets in all_files into one dataset using bind_rows. But that will only work if all the datasets in our list have the same number of columns and the same column names. We can check our column names by using map and names: We can use identical to see if the variables from two datasets match. We see that the variable names of the first and second datasets don’t match, but the variables from the second and third do. # Variables of first and second dataset don&#39;t match identical(names(all_files[[1]]), names(all_files[[2]])) ## [1] FALSE # Variables of third and second files match identical(names(all_files[[2]]), names(all_files[[3]])) ## [1] TRUE And we can check the number of columns by using map and ncol: all_files %&gt;% map(ncol) ## [[1]] ## [1] 31 ## ## [[2]] ## [1] 50 ## ## [[3]] ## [1] 50 ## ## [[4]] ## [1] 50 ## ## [[5]] ## [1] 50 ## ## [[6]] ## [1] 50 Congratulations on finding an extremely common problem in education data! You’ve discovered that niehter the number of columns nor the column names match. This is a problem because we won’t be able to combine the datasets into one. When we try, bind_rows returns a dataet with 100 columns instead of the expected 50. bind_rows(all_files) ## # A tibble: 97,386 x 100 ## Year `State Name` `SEA Education … `SEA Disability… `American India… ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2012 ALABAMA Correctional Fa… All Disabilities - ## 2 2012 ALABAMA Home All Disabilities 1 ## 3 2012 ALABAMA Homebound/Hospi… All Disabilities - ## 4 2012 ALABAMA Inside regular … All Disabilities - ## 5 2012 ALABAMA Inside regular … All Disabilities - ## 6 2012 ALABAMA Inside regular … All Disabilities - ## 7 2012 ALABAMA Other Location … All Disabilities 7 ## 8 2012 ALABAMA Other Location … All Disabilities 1 ## 9 2012 ALABAMA Parentally Plac… All Disabilities - ## 10 2012 ALABAMA Residential Fac… All Disabilities 0 ## # … with 97,376 more rows, and 95 more variables: `Asian Age 3-5` &lt;chr&gt;, `Black ## # or African American Age 3-5` &lt;chr&gt;, `Hispanic/Latino Age 3-5` &lt;chr&gt;, ## # `Native Hawaiian or Other Pacific Islander Age 3-5` &lt;chr&gt;, `Two or More ## # Races Age 3-5` &lt;chr&gt;, `White Age 3-5` &lt;chr&gt;, `Female Age 3 to 5` &lt;chr&gt;, ## # `Male Age 3 to 5` &lt;chr&gt;, `LEP Yes Age 3 to 5` &lt;chr&gt;, `LEP No Age 3 to ## # 5` &lt;chr&gt;, `Age 3 to 5` &lt;chr&gt;, `Age 6-11` &lt;chr&gt;, `Age 12-17` &lt;chr&gt;, `Age ## # 18-21` &lt;chr&gt;, `Ages 6-21` &lt;chr&gt;, `LEP Yes Age 6 to 21` &lt;chr&gt;, `LEP No Age 6 ## # to 21` &lt;chr&gt;, `Female Age 6 to 21` &lt;chr&gt;, `Male Age 6 to 21` &lt;chr&gt;, ## # `American Indian or Alaska Native Age 6 to21` &lt;chr&gt;, `Asian Age 6 ## # to21` &lt;chr&gt;, `Black or African American Age 6 to21` &lt;chr&gt;, `Hispanic/Latino ## # Age 6 to21` &lt;chr&gt;, `Native Hawaiian or Other Pacific Islander Age 6 ## # to21` &lt;chr&gt;, `Two or more races Age 6 to21` &lt;chr&gt;, `White Age 6 ## # to21` &lt;chr&gt;, `Age 3` &lt;chr&gt;, `Age 4` &lt;chr&gt;, `Age 5` &lt;chr&gt;, `Age 6` &lt;chr&gt;, ## # `Age 7` &lt;chr&gt;, `Age 8` &lt;chr&gt;, `Age 9` &lt;chr&gt;, `Age 10` &lt;chr&gt;, `Age ## # 11` &lt;chr&gt;, `Age 12` &lt;chr&gt;, `Age 13` &lt;chr&gt;, `Age 14` &lt;chr&gt;, `Age 15` &lt;chr&gt;, ## # `Age 16` &lt;chr&gt;, `Age 17` &lt;chr&gt;, `Age 18` &lt;chr&gt;, `Age 19` &lt;chr&gt;, `Age ## # 20` &lt;chr&gt;, `Age 21` &lt;chr&gt;, `2016` &lt;dbl&gt;, Alabama &lt;chr&gt;, `Correctional ## # Facilities` &lt;chr&gt;, `All Disabilities` &lt;chr&gt;, `-` &lt;chr&gt;, `-_1` &lt;chr&gt;, ## # `-_2` &lt;chr&gt;, `-_3` &lt;chr&gt;, `-_4` &lt;chr&gt;, `-_5` &lt;chr&gt;, `-_6` &lt;chr&gt;, ## # `-_7` &lt;chr&gt;, `-_8` &lt;chr&gt;, `-_9` &lt;chr&gt;, `-_10` &lt;chr&gt;, `-_11` &lt;chr&gt;, ## # `-_12` &lt;chr&gt;, `-_13` &lt;chr&gt;, `-_14` &lt;chr&gt;, `0` &lt;chr&gt;, `0_1` &lt;chr&gt;, ## # `0_2` &lt;chr&gt;, `0_3` &lt;chr&gt;, `0_4` &lt;chr&gt;, `0_5` &lt;chr&gt;, `0_6` &lt;chr&gt;, ## # `0_7` &lt;chr&gt;, `0_8` &lt;chr&gt;, `1` &lt;chr&gt;, `2` &lt;chr&gt;, `4` &lt;chr&gt;, `14` &lt;chr&gt;, ## # `22` &lt;chr&gt;, `30` &lt;chr&gt;, `4_1` &lt;chr&gt;, `0_9` &lt;chr&gt;, `7` &lt;chr&gt;, `70` &lt;chr&gt;, ## # `77` &lt;chr&gt;, `0_10` &lt;chr&gt;, `77_1` &lt;chr&gt;, `1_1` &lt;chr&gt;, `76` &lt;chr&gt;, ## # `0_11` &lt;chr&gt;, `0_12` &lt;chr&gt;, `68` &lt;chr&gt;, `0_13` &lt;chr&gt;, `0_14` &lt;chr&gt;, ## # `0_15` &lt;chr&gt;, `9` &lt;chr&gt; We’ll correct this in the next section by selecting and renaming our variables, but it’s good to notice this problem early in the process so you know to work on it later. 12.6 Cleaning the Dataset Transforming your dataset before visualizing it and fitting models is critical. It’s easier to write code when variable names are concise and informative. Many functions in R, especially those in the ggplot2 package, work best when datsets are in a “tidy” format. It’s easier to do an analysis when you have just the variables you need. Any unused variables can confuse your thought process. Let’s preview the steps we’ll be taking: Fix the variable names in the 2016 data Combine the datasets Pick variables Filter for the desired categories Rename the variables Standardize the state names Transform the column formats from wide to narrow using gather Change the data types of variables Explore NAs In real life, data scientists don’t always know the cleaning steps until they dive into the work. Learning what cleaning steps are needed requires exploration, trial and error, and clarity on the analytic questions you want to answer. After a lot of exploring, we settled on these steps for this analysis. When you do your own, you will find different things to transform. As you do more and more data analysis, your instincts for what to transform will improve. 12.6.1 Fix the variable names in the 2016 data When we print the 2016 dataset, we notice that the variable names are incorrect. Let’s verify that by looking at the 2016 dataset, which is the fifth element of all_files: names(all_files[[5]]) ## [1] &quot;2016&quot; &quot;Alabama&quot; ## [3] &quot;Correctional Facilities&quot; &quot;All Disabilities&quot; ## [5] &quot;-&quot; &quot;-_1&quot; ## [7] &quot;-_2&quot; &quot;-_3&quot; ## [9] &quot;-_4&quot; &quot;-_5&quot; ## [11] &quot;-_6&quot; &quot;-_7&quot; ## [13] &quot;-_8&quot; &quot;-_9&quot; ## [15] &quot;-_10&quot; &quot;-_11&quot; ## [17] &quot;-_12&quot; &quot;-_13&quot; ## [19] &quot;-_14&quot; &quot;0&quot; ## [21] &quot;0_1&quot; &quot;0_2&quot; ## [23] &quot;0_3&quot; &quot;0_4&quot; ## [25] &quot;0_5&quot; &quot;0_6&quot; ## [27] &quot;0_7&quot; &quot;0_8&quot; ## [29] &quot;1&quot; &quot;2&quot; ## [31] &quot;4&quot; &quot;14&quot; ## [33] &quot;22&quot; &quot;30&quot; ## [35] &quot;4_1&quot; &quot;0_9&quot; ## [37] &quot;7&quot; &quot;70&quot; ## [39] &quot;77&quot; &quot;0_10&quot; ## [41] &quot;77_1&quot; &quot;1_1&quot; ## [43] &quot;76&quot; &quot;0_11&quot; ## [45] &quot;0_12&quot; &quot;68&quot; ## [47] &quot;0_13&quot; &quot;0_14&quot; ## [49] &quot;0_15&quot; &quot;9&quot; We want the variable names to be Year and State Name. But first, let’s go back and review how to get at the 2016 dataset from all_files. The order of the list elements was set all the way back when we fed map our list of filenames. If we look at filenames again, we see that the 2016 dataset was stored in the fifth element: filenames ## [1] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2012.csv&quot; ## [2] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2013.csv&quot; ## [3] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2014.csv&quot; ## [4] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2015.csv&quot; ## [5] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2016.csv&quot; ## [6] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2017-18.csv&quot; Once we know the 2016 dataset is the fifth element of our list, we can pluck it out by using double brackets: all_files[[5]] ## # A tibble: 16,230 x 50 ## `2016` Alabama `Correctional F… `All Disabiliti… `-` `-_1` `-_2` `-_3` ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2016 Alabama Home All Disabilities 43 30 35 0 ## 2 2016 Alabama Homebound/Hospi… All Disabilities - - - - ## 3 2016 Alabama Inside regular … All Disabilities - - - - ## 4 2016 Alabama Inside regular … All Disabilities - - - - ## 5 2016 Alabama Inside regular … All Disabilities - - - - ## 6 2016 Alabama Parentally Plac… All Disabilities - - - - ## 7 2016 Alabama Residential Fac… All Disabilities 5 3 4 0 ## 8 2016 Alabama Residential Fac… All Disabilities - - - - ## 9 2016 Alabama Separate Class All Disabilities 58 58 98 0 ## 10 2016 Alabama Separate School… All Disabilities 11 20 19 0 ## # … with 16,220 more rows, and 42 more variables: `-_4` &lt;chr&gt;, `-_5` &lt;chr&gt;, ## # `-_6` &lt;chr&gt;, `-_7` &lt;chr&gt;, `-_8` &lt;chr&gt;, `-_9` &lt;chr&gt;, `-_10` &lt;chr&gt;, ## # `-_11` &lt;chr&gt;, `-_12` &lt;chr&gt;, `-_13` &lt;chr&gt;, `-_14` &lt;chr&gt;, `0` &lt;chr&gt;, ## # `0_1` &lt;chr&gt;, `0_2` &lt;chr&gt;, `0_3` &lt;chr&gt;, `0_4` &lt;chr&gt;, `0_5` &lt;chr&gt;, ## # `0_6` &lt;chr&gt;, `0_7` &lt;chr&gt;, `0_8` &lt;chr&gt;, `1` &lt;chr&gt;, `2` &lt;chr&gt;, `4` &lt;chr&gt;, ## # `14` &lt;chr&gt;, `22` &lt;chr&gt;, `30` &lt;chr&gt;, `4_1` &lt;chr&gt;, `0_9` &lt;chr&gt;, `7` &lt;chr&gt;, ## # `70` &lt;chr&gt;, `77` &lt;chr&gt;, `0_10` &lt;chr&gt;, `77_1` &lt;chr&gt;, `1_1` &lt;chr&gt;, ## # `76` &lt;chr&gt;, `0_11` &lt;chr&gt;, `0_12` &lt;chr&gt;, `68` &lt;chr&gt;, `0_13` &lt;chr&gt;, ## # `0_14` &lt;chr&gt;, `0_15` &lt;chr&gt;, `9` &lt;chr&gt; We used skip = 4 when we read in those datasets. That worked for all datasets except the fifth one. In that one, skipping four lines left out the variable name row. To fix it, we’ll read the 2016 dataset again using read_csv and the fifth element of filenames. We’ll assign the newly read dataset to the fifth element of the all_files list: ## Parsed with column specification: ## cols( ## .default = col_character(), ## Year = col_double() ## ) ## See spec(...) for full column specifications. ## Warning: 4 parsing failures. ## row col expected actual file ## 16228 Year a double ------------------- &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2016.csv&#39; ## 16229 Year a double - Data not available &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2016.csv&#39; ## 16230 Year a double x Data suppressed due to small cell size &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2016.csv&#39; ## 16231 Year a double * Data flagged due to questionable data quality &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2016.csv&#39; Try printing all_files now. You can confirm we fixed the problem by checking that the variable names are correct. 12.6.2 Pick variables Now that we know all our datasets have the correct variable names, we simplify our datasets by picking the variables we need. This is a good place to think carefully about which variables to pick. This usually requires a fair amount of trial and error, but here is what we found we needed: Our analytic questions are about gender, so let’s pick the gender variable Later, we’ll need to filter our dataset by disability category and program location so we’ll want SEA Education Environment and SEA Disability Category We want to make comparisons by state and reporting year, so we’ll also pick State Name and Year Combining select and contains is a convenient way to pick these variables without writing a lot of code. Knowing that we want variables that contain the acronym “SEA” and variables that contain “male” in their names, we can pass those characters to contains: all_files[[1]] %&gt;% select( Year, contains(&quot;State&quot;, ignore.case = FALSE), contains(&quot;SEA&quot;, ignore.case = FALSE), contains(&quot;male&quot;) ) ## # A tibble: 16,230 x 8 ## Year `State Name` `SEA Education … `SEA Disability… `Female Age 3 t… ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2012 ALABAMA Correctional Fa… All Disabilities - ## 2 2012 ALABAMA Home All Disabilities 63 ## 3 2012 ALABAMA Homebound/Hospi… All Disabilities - ## 4 2012 ALABAMA Inside regular … All Disabilities - ## 5 2012 ALABAMA Inside regular … All Disabilities - ## 6 2012 ALABAMA Inside regular … All Disabilities - ## 7 2012 ALABAMA Other Location … All Disabilities 573 ## 8 2012 ALABAMA Other Location … All Disabilities 81 ## 9 2012 ALABAMA Parentally Plac… All Disabilities - ## 10 2012 ALABAMA Residential Fac… All Disabilities 6 ## # … with 16,220 more rows, and 3 more variables: `Male Age 3 to 5` &lt;chr&gt;, ## # `Female Age 6 to 21` &lt;chr&gt;, `Male Age 6 to 21` &lt;chr&gt; That code chunk verifies that we got the variables we want, so now we will turn the code chunk into a function called pick_vars. We will then use map to feed our list of datasets, all_files, to the function. In this function, we’ll use a special version of select() called select_at(), which conveniently picks variables based on criteria we give it. The argument vars(Year, contains(\"State\", ignore.case = FALSE), contains(\"SEA\", ignore.case = FALSE), contains(\"male\")) tells R we want to keep any column whose name has “State” in upper or lower case letters, has “SEA” in the title, and has “male” in the title. This will result in a newly transformed all_files list that contains six datasets, all with the desired variables. pick_vars &lt;- function(df) { df %&gt;% select_at(vars( Year, contains(&quot;State&quot;, ignore.case = FALSE), contains(&quot;SEA&quot;, ignore.case = FALSE), contains(&quot;male&quot;) )) } all_files &lt;- all_files %&gt;% map(pick_vars) 12.6.3 Combine six datasets into one Now we’ll turn our attention to combining the datasets in our list all_files into one. We’ll use bind_rows, which combines datasets by adding each one to the bottom of the one before it. The first step is to check and see if our datasets have the same number of variables and the same variable names. When we use names on our list of newly changed datasets, we see that each dataset’s variable names are the same: all_files %&gt;% map(names) ## [[1]] ## [1] &quot;Year&quot; &quot;State Name&quot; ## [3] &quot;SEA Education Environment&quot; &quot;SEA Disability Category&quot; ## [5] &quot;Female Age 3 to 5&quot; &quot;Male Age 3 to 5&quot; ## [7] &quot;Female Age 6 to 21&quot; &quot;Male Age 6 to 21&quot; ## ## [[2]] ## [1] &quot;Year&quot; &quot;State Name&quot; ## [3] &quot;SEA Education Environment&quot; &quot;SEA Disability Category&quot; ## [5] &quot;Female Age 3 to 5&quot; &quot;Male Age 3 to 5&quot; ## [7] &quot;Female Age 6 to 21&quot; &quot;Male Age 6 to 21&quot; ## ## [[3]] ## [1] &quot;Year&quot; &quot;State Name&quot; ## [3] &quot;SEA Education Environment&quot; &quot;SEA Disability Category&quot; ## [5] &quot;Female Age 3 to 5&quot; &quot;Male Age 3 to 5&quot; ## [7] &quot;Female Age 6 to 21&quot; &quot;Male Age 6 to 21&quot; ## ## [[4]] ## [1] &quot;Year&quot; &quot;State Name&quot; ## [3] &quot;SEA Education Environment&quot; &quot;SEA Disability Category&quot; ## [5] &quot;Female Age 3 to 5&quot; &quot;Male Age 3 to 5&quot; ## [7] &quot;Female Age 6 to 21&quot; &quot;Male Age 6 to 21&quot; ## ## [[5]] ## [1] &quot;Year&quot; &quot;State Name&quot; ## [3] &quot;SEA Education Environment&quot; &quot;SEA Disability Category&quot; ## [5] &quot;Female Age 3 to 5&quot; &quot;Male Age 3 to 5&quot; ## [7] &quot;Female Age 6 to 21&quot; &quot;Male Age 6 to 21&quot; ## ## [[6]] ## [1] &quot;Year&quot; &quot;State Name&quot; ## [3] &quot;SEA Education Environment&quot; &quot;SEA Disability Category&quot; ## [5] &quot;Female Age 3 to 5&quot; &quot;Male Age 3 to 5&quot; ## [7] &quot;Female Age 6 to 21&quot; &quot;Male Age 6 to 21&quot; That means that we can combine all six datasets into one using bind_rows. We’ll call this newly combined dataset child_counts: child_counts &lt;- all_files %&gt;% bind_rows() Since we know that a) each of our six datasets had eight variables and b) our combined dataset also has eight variables, we can conclude that all our rows combined together correctly. But let’s use str to verify: str(child_counts) ## Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 97387 obs. of 8 variables: ## $ Year : num 2012 2012 2012 2012 2012 ... ## $ State Name : chr &quot;ALABAMA&quot; &quot;ALABAMA&quot; &quot;ALABAMA&quot; &quot;ALABAMA&quot; ... ## $ SEA Education Environment: chr &quot;Correctional Facilities&quot; &quot;Home&quot; &quot;Homebound/Hospital&quot; &quot;Inside regular class 40% through 79% of day&quot; ... ## $ SEA Disability Category : chr &quot;All Disabilities&quot; &quot;All Disabilities&quot; &quot;All Disabilities&quot; &quot;All Disabilities&quot; ... ## $ Female Age 3 to 5 : chr &quot;-&quot; &quot;63&quot; &quot;-&quot; &quot;-&quot; ... ## $ Male Age 3 to 5 : chr &quot;-&quot; &quot;174&quot; &quot;-&quot; &quot;-&quot; ... ## $ Female Age 6 to 21 : chr &quot;4&quot; &quot;-&quot; &quot;104&quot; &quot;1590&quot; ... ## $ Male Age 6 to 21 : chr &quot;121&quot; &quot;-&quot; &quot;130&quot; &quot;3076&quot; ... 12.6.4 Filter for the desired disabilities and age groups We want to explore gender related variables, but our dataset has additional aggregate data for other subgroups. For example, we can use count to explore all the different disability groups in the dataset: child_counts %&gt;% count(`SEA Disability Category`) ## # A tibble: 16 x 2 ## `SEA Disability Category` n ## &lt;chr&gt; &lt;int&gt; ## 1 All Disabilities 6954 ## 2 Autism 6954 ## 3 Deaf-blindness 6954 ## 4 Developmental delay 4636 ## 5 Developmental delay (valid only for children ages 3-9 when defined by … 2318 ## 6 Emotional disturbance 6954 ## 7 Hearing impairment 6954 ## 8 Intellectual disability 6954 ## 9 Multiple disabilities 6954 ## 10 Orthopedic impairment 6954 ## 11 Other health impairment 6954 ## 12 Specific learning disability 6954 ## 13 Speech or language impairment 6954 ## 14 Traumatic brain injury 6954 ## 15 Visual impairment 6954 ## 16 &lt;NA&gt; 31 Since we will be visualizing and modeling gender variables for all students in the dataset, we’ll filter out all subgoups except “All Disabilities” and the age totals: child_counts &lt;- child_counts %&gt;% filter( `SEA Disability Category` == &quot;All Disabilities&quot;, `SEA Education Environment` %in% c(&quot;Total, Age 3-5&quot;, &quot;Total, Age 6-21&quot;) ) 12.6.5 Rename the variables In the next section we’ll prepare the dataset for visualization and modeling by “tidying” it. When we write code to transform datasets, we’ll be typing the column names a lot, so it’s useful to change them to ones with more convenient names. child_counts &lt;- child_counts %&gt;% rename( year = Year, state = &quot;State Name&quot;, age = &quot;SEA Education Environment&quot;, disability = &quot;SEA Disability Category&quot;, f_3_5 = &quot;Female Age 3 to 5&quot;, m_3_5 = &quot;Male Age 3 to 5&quot;, f_6_21 = &quot;Female Age 6 to 21&quot;, m_6_21 = &quot;Male Age 6 to 21&quot; ) 12.6.6 Clean state names You might have noticed that some state names in our dataset are in upper case letters and some are in lower case letters: child_counts %&gt;% count(state) %&gt;% head() ## # A tibble: 6 x 2 ## state n ## &lt;chr&gt; &lt;int&gt; ## 1 Alabama 8 ## 2 ALABAMA 4 ## 3 Alaska 8 ## 4 ALASKA 4 ## 5 American Samoa 8 ## 6 AMERICAN SAMOA 4 If we leave it like this, R will treat state values like “CALIFORNIA” and “California” as two different states. We can use mutate and tolower to transform all the state names to lowercase letters. child_counts &lt;- child_counts %&gt;% mutate(state = tolower(state)) 12.6.7 Tidy the dataset Visualizing and modeling our data will be much easier if our dataset is in a “tidy” format. In his paper Tidy Data (2014), Hadley Wickham defines tidy datasets where Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. A note on the gender variable in this dataset This dataset uses a binary approach to data collection about gender. Students are described as either male or female. The need for an inclusive approach to documenting gender identity is discussed in a paper by Andrew Park (2016) of The Williams Institute at UCLA. The gender variables in our dataset are spread across four columns, with each one representing a combination of gender and age range. We can use gather to bring the gender variable into one column. In this transformation, we create two new columns: a gender column and a total column. The total column will contain the number of students in each row’s gender and age category. child_counts &lt;- child_counts %&gt;% gather(gender, total, f_3_5:m_6_21) To make the values of the gender column more intuitive, we’ll use case_when to transform the values to either “f” or “m”: child_counts &lt;- child_counts %&gt;% mutate( gender = case_when( gender == &quot;f_3_5&quot; ~ &quot;f&quot;, gender == &quot;m_3_5&quot; ~ &quot;m&quot;, gender == &quot;f_6_21&quot; ~ &quot;f&quot;, gender == &quot;m_6_21&quot; ~ &quot;m&quot;, TRUE ~ as.character(gender) ) ) 12.6.8 Convert data types The values in the total column represent the number of students from a specific year, state, gender, and age group. We know from the chr under their variable names that R is treating these values like characters instead of numbers. While R does a decent job of treating numbers like numbers when needed, it’s much safer to prepare the dataset by changing these character columns to number columns. We’ll use dplyr::mutate_at to change the count columns. child_counts &lt;- child_counts %&gt;% mutate(total = as.numeric(total)) ## Warning: NAs introduced by coercion child_counts ## # A tibble: 2,928 x 6 ## year state age disability gender total ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2012 alabama Total, Age 3-5 All Disabilities f 2228 ## 2 2012 alabama Total, Age 6-21 All Disabilities f NA ## 3 2012 alaska Total, Age 3-5 All Disabilities f 676 ## 4 2012 alaska Total, Age 6-21 All Disabilities f NA ## 5 2012 american samoa Total, Age 3-5 All Disabilities f 45 ## 6 2012 american samoa Total, Age 6-21 All Disabilities f NA ## 7 2012 arizona Total, Age 3-5 All Disabilities f 4743 ## 8 2012 arizona Total, Age 6-21 All Disabilities f NA ## 9 2012 arkansas Total, Age 3-5 All Disabilities f 4605 ## 10 2012 arkansas Total, Age 6-21 All Disabilities f NA ## # … with 2,918 more rows Converting these count columns from character classes to number classes resulted in two changes. First, the chr under these variable names has now changed to dbl, short for “double-precision”. This lets us know that R recognizes these values as numbers with decimal points. Second, the blank values changed to NA. When R sees a character class value like \"4\", it knows to change it to numeric class 4. But there is no obvious number represented by a character class like \"\", so it changes it to NA: # Convert a character to a number as.numeric(&quot;4&quot;) ## [1] 4 # Convert a blank character to number as.numeric(&quot;&quot;) ## [1] NA Similarly, the variable year needs to be changed from the character format to the date format. Doing so will make sure R treats this variable like a point in time when we plot our dataset. The package lubridate has a handy function called ymd that can help us. We just have to use the truncated argument to let R know we don’t have a month and date to convert. child_counts &lt;- child_counts %&gt;% mutate(year = ymd(year, truncated = 2)) 12.6.9 Explore and address NAs You’ll notice that some rows in the total column contain an NA. When we used gather to create a gender column, R created unique rows for every year, state, age, disability, and gender combination. Since the original dataset had both gender and age range stored in a column like Female Age 3 to 5, R made rows where the total value is NA . For example, there is no student count for the age value “Total, Age 3-5” that also has the gender value for female students who were age 6-21. You can see that more clearly by sorting the dataset by year, state, and gender. In our foundational skills chapter, we introduced a dplyr function called arrange to sort the rows of a dataset by the values in a column. Let’s use arrange here to sort the dataset by the year, state and gender columns. When you pass arrange a variable, it will sort by the order of the values in that variable. If you pass it multiple variables, arrange will sort by the first variable, then by the second, and so on. Let’s see what it does on child_counts when we pass it the year, state, and gender variables: child_counts %&gt;% arrange(year, state, gender) ## # A tibble: 2,928 x 6 ## year state age disability gender total ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2012-01-01 alabama Total, Age 3-5 All Disabilities f 2228 ## 2 2012-01-01 alabama Total, Age 6-21 All Disabilities f NA ## 3 2012-01-01 alabama Total, Age 3-5 All Disabilities f NA ## 4 2012-01-01 alabama Total, Age 6-21 All Disabilities f 23649 ## 5 2012-01-01 alabama Total, Age 3-5 All Disabilities m 5116 ## 6 2012-01-01 alabama Total, Age 6-21 All Disabilities m NA ## 7 2012-01-01 alabama Total, Age 3-5 All Disabilities m NA ## 8 2012-01-01 alabama Total, Age 6-21 All Disabilities m 48712 ## 9 2012-01-01 alaska Total, Age 3-5 All Disabilities f 676 ## 10 2012-01-01 alaska Total, Age 6-21 All Disabilities f NA ## # … with 2,918 more rows We can simplify our dataset by removing these rows, leaving us with one row for each category: females age 3-5 females age 6-21 males age 3-5 males age 6-21 Each of these categories will be associated with a state and reporting year: child_counts &lt;- child_counts %&gt;% filter(!is.na(total)) We can verify we have the categories we want by sorting again: child_counts %&gt;% arrange(year, state, gender) ## # A tibble: 1,390 x 6 ## year state age disability gender total ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2012-01-01 alabama Total, Age 3-5 All Disabilities f 2228 ## 2 2012-01-01 alabama Total, Age 6-21 All Disabilities f 23649 ## 3 2012-01-01 alabama Total, Age 3-5 All Disabilities m 5116 ## 4 2012-01-01 alabama Total, Age 6-21 All Disabilities m 48712 ## 5 2012-01-01 alaska Total, Age 3-5 All Disabilities f 676 ## 6 2012-01-01 alaska Total, Age 6-21 All Disabilities f 5307 ## 7 2012-01-01 alaska Total, Age 3-5 All Disabilities m 1440 ## 8 2012-01-01 alaska Total, Age 6-21 All Disabilities m 10536 ## 9 2012-01-01 american samoa Total, Age 3-5 All Disabilities f 45 ## 10 2012-01-01 american samoa Total, Age 6-21 All Disabilities f 208 ## # … with 1,380 more rows 12.7 How Have Child Counts Changed Over Time? In the last section we focused on importing our dataset. In this section, we will turn to exploring it. First, we’ll use visualization to explore the number of students in special education over time. In particular, we’ll compare the count of male and female students. Next, we’ll use what we learn from our visualizations to quantify any differences that we see. 12.7.1 Visualize the Dataset Showing this many states in a plot can be overwhelming, so to start we’ll make a subset of the dataset. We can use a function in the dplyr package called top_n() to help us learn which states have the highest average count of special education students: child_counts %&gt;% group_by(state) %&gt;% summarise(mean_count = mean(total)) %&gt;% top_n(6, mean_count) ## # A tibble: 6 x 2 ## state mean_count ## &lt;chr&gt; &lt;dbl&gt; ## 1 california 180879. ## 2 florida 92447. ## 3 new york 121751. ## 4 pennsylvania 76080. ## 5 texas 115593. ## 6 us, outlying areas, and freely associated states 1671931. These six states have the highest mean count of students in special education over the six years we are examining. For reasons we will see in a later visualization, we are going to exclude outlying areas and freely associated states. That leaves us us with five states: California, Florida, New York, Pennsylvania, and Texas. We can remove all other states but these by using filter(). We’ll call this new dataset high_count: high_count &lt;- child_counts %&gt;% filter(state %in% c(&quot;california&quot;, &quot;florida&quot;, &quot;new york&quot;, &quot;pennsylvania&quot;, &quot;texas&quot;)) Now we can use high_count to do some initial exploration. Our analysis is about comparing counts of male and female students in special education, but visualization is also a great way to explore related curiosities. You may surprise yourself with what you find when visualizing your datasets. You might come up with more interesting hypotheses, find that your initial hypothesis requires more data transformation, or find interesting subsets of the data–we saw a little of that in the surprisingly high mean_count of freely associated states in the state column. Let your curiosity and intuition drive this part of the analysis. It’s one of the activities that makes data analysis a creative process. In that spirit, we’ll start by visualizing specific genders and age groups. Feel free to try these, but also try the other student groups for practice and more exploration. Start by copying and running this code in your console to see what it does: high_count %&gt;% filter(gender == &quot;f&quot;, age == &quot;Total, Age 6-21&quot;) %&gt;% ggplot(aes(x = year, y = total, color = state)) + geom_freqpoly(stat = &quot;identity&quot;) + labs(title = &quot;Count of female students in special education over time&quot;, subtitle = &quot;Ages 6-21&quot;) + theme_dataedu() + scale_color_dataedu() That gives us a plot that has the years in the x-axis and a count of female students in the y-axis. Each line takes a different color based on the state it represents. Let’s look at that closer: We used filter to subset our dataset for students who are female and ages 6 to 21. We used aes to connect visual elements of our plot to our data. We connected the x-axis to year, the y-axis to total, and the color of the line to state. It’s worth calling out one more thing, since it’s a technique we’ll be using as we explore further. Note here that, instead of storing our new dataset in a new variable, we filter the dataset then use the pipe operator %&gt;% to feed it to ggplot. Since we’re exploring freely, we don’t need to create a lot of new variables we probably won’t need later. We can also try the same plot, but subsetting for male students instead. We can use the same code we used for the last plot, but filter for the value “m” in the gender field: high_count %&gt;% filter(gender == &quot;m&quot;, age == &quot;Total, Age 6-21&quot;) %&gt;% ggplot(aes(x = year, y = total, color = state)) + geom_freqpoly(stat = &quot;identity&quot;, size = 1) + labs(title = &quot;Count of male students in special education over time&quot;, subtitle = &quot;Ages 6-21&quot;) + theme_dataedu() + scale_color_dataedu() We’ve looked at each gender separately. What do these lines look like if we visualized the total amount of students each year per state? To do that, we’ll need to add both gender values together and both age group values together. We’ll do this using a very common combination of functions: group_by and summarise. high_count %&gt;% group_by(year, state) %&gt;% summarise(n = sum(total)) %&gt;% ggplot(aes(x = year, y = n, color = state)) + geom_freqpoly(stat = &quot;identity&quot;, size = 1) + theme_dataedu() + scale_color_dataedu() So far we’ve looked at a few ways to count students over time. In each plot, we see that while counts have grown overall for all states, each state has different sized populations. Let’s see if we can summarize that difference by looking at the median student count for each state over the years: high_count %&gt;% group_by(year, state) %&gt;% summarise(n = sum(total)) %&gt;% ggplot(aes(x = state, y = n)) + geom_boxplot() + labs(title = &quot;Median student count&quot;, subtitle = &quot;All ages and genders&quot;) + theme_dataedu() The boxplots show us what we might have expected from our freqpoly plots before it. The highest median student count over time is California and the lowest is Pennsylvania. What have we learned about our data so far? The five states in the US with the highest total student counts (not including outlying areas and freely associated states) do not have similar counts to each other. The student counts for each state also appear to have grown over time. But how can we start comparing the male student count to the female student count? One way is to use a ratio: In mathematics, a ratio is a relationship between two numbers indicating how many times the first number contains the second. Wikpedia We can use the count of male students in each state and divide it by the count of each female student. The result is the number of times male students are in special education more or less than the female students in the same state and year. Our coding strategy will be to Use spread to create separate columns for male and female students. Use mutate to create a new variable called ratio. The values in this column will be the result of dividing the count of male students by the count of female students Note here that we can also accomplish this comparison by dividing the number of female students by the number of male students. In this case, the result would be the number of times female students are in special education more or less than male students. high_count %&gt;% group_by(year, state, gender) %&gt;% summarise(total = sum(total)) %&gt;% # Create new columns for male and female student counts spread(gender, total) %&gt;% # Create a new ratio column mutate(ratio = m / f) %&gt;% ggplot(aes(x = year, y = ratio, color = state)) + geom_freqpoly(stat = &quot;identity&quot;, size = 1) + scale_y_continuous(limits = c(1.5, 2.5)) + labs(title = &quot;Male student to female student ratio over time&quot;, subtitle = &quot;Ages 6-21&quot;) + theme_dataedu() + scale_color_dataedu() By visually inspecting, we can hypothesize that there was no significant change in the male to female ratio between the years 2012 and 2017. But very often we want to understand the underlying properties of our education dataset. We can do this by quantifying the relationship between two variables. In the next section, we’ll explore ways to quantify the relationship between male student counts and female student counts. 12.7.2 Model the Dataset When you visualize your datasets, you are exploring possible relationships between variables. But sometimes visualizations can be misleading because of the way we perceive graphics. In his book Data Visuzliation: A Practical Introduction, Kieran Healy (2019) teaches us that Visualizations encode numbers in lines, shapes, and colors. That means that our interpretation of these encodings is partly conditional on how we perceive geometric shapes and relationships generally. What are some ways we can combat these errors of perception and at the same time draw substantive conclusions about our education dataset? When you spot a possible relationship between variables, the relationship between female and male counts for example, you’ll want to quantify that relationship by fitting a statisitcal model. Practically speaking, this means you are selecting a distribution that represents your dataset reasonably well. This distribution will help you quantify and predict relationships between variables. This is an important step in the analytic process, because it acts as a check on what you saw in your exploratory visualizations. In this example, we’ll follow our intuition about the relationship between male and female student counts in our special education dataset. In particular, we’ll test the hypothesis that this ratio has decreased over the years. Fitting a linear regression model that estimates the year as a predictor of the male to female ratio will help us do just that. 12.7.2.1 Do we have enough information for our model? At the start of this section, we chose to exclude outlying areas and freely associated states. This visualization suggests that there are some states that have a child count so high it leaves a gap in the x-axis values. This can be problematic when we try to interpret our model later. Here’s a plot of female students compared to male students. Note that the relationship appears linear, but there is a large gap in the distribution of female student counts somewhere bewteen the values of 250,000 and 1,750,000: child_counts %&gt;% filter(age == &quot;Total, Age 6-21&quot;) %&gt;% spread(gender, total) %&gt;% ggplot(aes(x = f, y = m)) + geom_point(size = 3, alpha = .5) + geom_smooth() + labs( title = &quot;Comparison of female students to male students in special education&quot;, subtitle = &quot;Counts of students in each state, ages 6-21&quot;, x = &quot;Female students&quot;, y = &quot;Male students&quot;, caption = &quot;Data: US Dept of Education&quot; ) + theme_dataedu() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; If you think of each potential point on the linear regression line as a ratio of male to female students, you’ll notice that we don’t know a whole lot about what happens in states where there are between 250,000 and 1,750,000 female students in any given year. To learn more about what’s happening in our dataset, we can filter it for only states that have more than 500,000 female students in any year: child_counts %&gt;% filter(age == &quot;Total, Age 6-21&quot;) %&gt;% spread(gender, total) %&gt;% filter(f &gt; 500000) %&gt;% select(year, state, age, f, m) ## # A tibble: 6 x 5 ## year state age f m ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2012-01-01 us, outlying areas, and freely associ… Total, Age 6… 1933619 3.89e6 ## 2 2013-01-01 us, outlying areas, and freely associ… Total, Age 6… 1937726 3.88e6 ## 3 2014-01-01 us, outlying areas, and freely associ… Total, Age 6… 1965204 3.92e6 ## 4 2015-01-01 us, outlying areas, and freely associ… Total, Age 6… 2007174 3.98e6 ## 5 2016-01-01 us, outlying areas, and freely associ… Total, Age 6… 2014120 3.97e6 ## 6 2017-01-01 us, outlying areas, and freely associ… Total, Age 6… 2051438 4.02e6 This is where we discover that each of the data points in the upper right hand corner of the plot are from the state value “us, us, outlying areas, and freely associated states”. If we remove these outliers, we have a distribution of female students that looks more complete. child_counts %&gt;% filter(age == &quot;Total, Age 6-21&quot;) %&gt;% spread(gender, total) %&gt;% # Filter for female student counts less than 500,000 filter(f &lt;= 500000) %&gt;% ggplot(aes(x = f, y = m)) + geom_point(size = 3, alpha = .5) + labs( title = &quot;Comparison of female students to male students in special education&quot;, subtitle = &quot;Counts of students in each state, ages 6-21.\\nDoes not include outlying areas and freely associated states&quot;, x = &quot;Female students&quot;, y = &quot;Male students&quot;, caption = &quot;Data: US Dept of Education&quot; ) + theme_dataedu() This should allow us to fit a better model for the relationship between male and female student counts, albeit only the ones where the count of female students takes a value between 0 and 500,000. 12.7.2.2 Male to Female Ratio Over Time Earlier we asked the question Do we have enough data points for the count of female students to learn about the ratio of female to male students? Similarly, we should ask the question Do we have enough data points across our year variable to learn about how this ratio has changed over time? To answer that question, let’s start by making a new dataset that excludes any rows where the f variables has a value that is less than or equal to 500000. We’ll convert the year variable to a factor data type–we’ll see how this helps in a bit. We’ll also add a column called ratio that contains the male to female count ratio. model_data &lt;- child_counts %&gt;% filter(age == &quot;Total, Age 6-21&quot;) %&gt;% mutate(year = as.factor(year(year))) %&gt;% spread(gender, total) %&gt;% # Exclude outliers filter(f &lt;= 500000) %&gt;% # Compute male student to female student ratio mutate(ratio = m / f) %&gt;% select(-c(age, disability)) We can see how much data we have per year by using count: model_data %&gt;% count(year) ## # A tibble: 6 x 2 ## year n ## &lt;fct&gt; &lt;int&gt; ## 1 2012 59 ## 2 2013 56 ## 3 2014 56 ## 4 2015 58 ## 5 2016 57 ## 6 2017 55 Let’s visualize the ratio values across all years as an additional check. Note the use of geom_jitter to spread the points horizontally so we can estimate the quantities better: ggplot(data = model_data, aes(x = year, y = ratio)) + geom_jitter(alpha = .5) + labs(title = &quot;Male to female ratio across years (jittered)&quot;) + theme_dataedu() Each year seems to have data points that can be considered when we fit the model. This means that there are enough data points to help us learn how the year variable predicts the ratio variable. We fit the linear regression model by passing the argument ratio ~ year to the function lm. In R, the ~ usually indicates a formula. In this case, the formula is the variable year as a predictor of the variable ratio. The final argument we pass to lm is data = model_data, which tells R to look for the variables ratio and year in the dataset model_data. The results of the model are called a “model object.” We’ll store the model object in ratio_year: ratio_year &lt;- lm(ratio ~ year, data = model_data) Each model object is filled with all sorts of model information. We can look at this information using the fuction summary: summary(ratio_year) ## ## Call: ## lm(formula = ratio ~ year, data = model_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.44025 -0.10138 -0.02810 0.05343 0.75737 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.03356 0.02200 92.418 &lt;2e-16 *** ## year2013 -0.01205 0.03153 -0.382 0.7027 ## year2014 -0.02372 0.03153 -0.752 0.4524 ## year2015 -0.03104 0.03125 -0.993 0.3213 ## year2016 -0.03964 0.03139 -1.263 0.2075 ## year2017 -0.05760 0.03168 -1.818 0.0699 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.169 on 335 degrees of freedom ## Multiple R-squared: 0.01215, Adjusted R-squared: -0.002594 ## F-statistic: 0.8241 on 5 and 335 DF, p-value: 0.5332 Here’s how we can interpret the Estimate column: The estimate of the (Intercept) is 2.03356, which is the estimated value of the ratio variable when the year variable is “2012”. Note that the value year2012 isn’t present in the in the list of rownames. That’s because the (Intercept) row represents year2012. In linear regression models that use factor variables as predictors, the first level of the factor is the intercept. Sometimes this level is called a “dummy variable”. The remaining rows of the model output show how much each year differs from the intercept, 2012. For example, year2013 has an estimate of -0.01205, which suggests that on average the value of ratio is .01205 less than 2.03356. On average, the ratio of year2014 is .02372 less than 2.03356. The t value column tells us the size of difference between the estimated value of the ratio for each year and the estimated value of the ratio of the intercept. Generally speaking, the larger the t value, the larger the chance that any difference between the coefficient of a factor level and the intercept are significant. Though the relationship between year as a preditor of ratio is not linear (recall our previous plot), the linear regression model still gives us useful information. We fit a linear regression model to a factor variable, like year, as a predictor of a continuous variable, likeratio. In doing so, we got the average ratio at every value of year. We can verify this by taking the mean ratio of ever year: model_data %&gt;% group_by(year) %&gt;% summarise(mean_ratio = mean(ratio)) ## # A tibble: 6 x 2 ## year mean_ratio ## &lt;fct&gt; &lt;dbl&gt; ## 1 2012 2.03 ## 2 2013 2.02 ## 3 2014 2.01 ## 4 2015 2.00 ## 5 2016 1.99 ## 6 2017 1.98 This verifies that our intercept, the value of ratio during the year 2012, is 2.033563 and the value of ratio for 2013 is .01205 less than that of 2012 on average. Fitting the model gives us more details about these mean ratio scores– namely the coefficient, t value, and p value. These values help us apply judgement when deciding if differences in ratio values suggest an underlying difference between years or simply differences you can expect from randomness. In this case, the absence of \"*\" in all rows except the Intercept row suggest that any differences occuring between years are within the range you’d expect by chance. If we use summary on our model_data dataset, we can verify the intercept again: model_data %&gt;% filter(year == &quot;2012&quot;) %&gt;% summary() ## year state f m ratio ## 2012:59 Length:59 Min. : 208 Min. : 443 Min. :1.710 ## 2013: 0 Class :character 1st Qu.: 5606 1st Qu.: 11467 1st Qu.:1.927 ## 2014: 0 Mode :character Median : 22350 Median : 44110 Median :1.994 ## 2015: 0 Mean : 32773 Mean : 65934 Mean :2.034 ## 2016: 0 3rd Qu.: 38552 3rd Qu.: 77950 3rd Qu.:2.093 ## 2017: 0 Max. :198595 Max. :414466 Max. :2.692 The mean value of the ratio column when the year column is 2012 is 2.034, just like in the model output’s intercept row. Lastly, we may want to communicate to a larger audience that there were roughly twice amount of male students in this datset than female students and that this did not change significantly between the years 2012 and 2017. When you are not communicating to an audience of other data scientists, it’s helpful to illustrate your point without the technical details of the model output. Think of yourself as an interpreter: since you can speak the language of model outputs and the language of data visualization, your challenge is to take what you learned from the model output and tell that story in a way that is meaningful to your non-data scientist audience. There are many ways to do this, but we’ll choose boxplots to show our audience that there was roughly twice as many male students in special education than female students between 2012 and 2017. For our purposes, let’s verify this by looking at the median male to female ratio for each year: model_data %&gt;% group_by(year) %&gt;% summarise(median_ratio = median(ratio)) ## # A tibble: 6 x 2 ## year median_ratio ## &lt;fct&gt; &lt;dbl&gt; ## 1 2012 1.99 ## 2 2013 1.99 ## 3 2014 1.98 ## 4 2015 1.98 ## 5 2016 1.97 ## 6 2017 1.96 Now let’s visualize this using our boxplots: model_data %&gt;% gather(gender, students, c(f, m)) %&gt;% ggplot(aes(x = year, y = students, color = gender)) + geom_boxplot() + scale_y_continuous(labels = scales::comma) + labs( title = &quot;Median male and female student counts in special education&quot;, subtitle = &quot;Ages 6-21. Does not include outlying areas and freely associated states&quot;, x = &quot;&quot;, y = &quot;&quot;, caption = &quot;Data: US Dept of Education&quot; ) + theme_dataedu() + scale_color_dataedu() Once we learned from our model that male to female ratios did not change in any meaningful way from 2012 to 2017 and that the median ratio across states was about 2 male students to every female student, we can present these two ideas using this plot. When discussing the plot, it helps to have your model output in your notes so you can reference specific coefficient estimates when needed. 12.8 Aggregate Data as Context for Student Data Education data science is about using data science tools to learn about and improve the lives of our students. So why choose a publicly available aggregate dataset instead of a student-level dataset? We chose to use an aggregate dataset because it reflects an analysis that an education data scientist would typically do. Using student-level data requires that the data scientist is either an employee of the school agency or that works under a memorandum of understanding (MOU) that allows her to access this data. Without either of these conditions, the education data scientist learns about the student experience by working on publicly available datasets, almost all of which are aggregated student-level datasets. Before we discuss the benefits of aggregate data, let’s take some time to understand the differences between aggregate and student-level data. Publicly available data–like the US Federal Government special education student count we used in this walkthrough–is a summary of student-level data. That means that student-level data is totaled up to protect the identities of students before making them publicly available. We can use R to demonstrate this concept. Here are rows in a student-level dataset: # student-level data tibble( student = letters[1:10], school = rep(letters[11:15], 2), test_score = sample(0:100, 10, replace = TRUE) ) ## # A tibble: 10 x 3 ## student school test_score ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 a k 27 ## 2 b l 44 ## 3 c m 58 ## 4 d n 64 ## 5 e o 99 ## 6 f k 56 ## 7 g l 68 ## 8 h m 38 ## 9 i n 91 ## 10 j o 89 Aggregate data totals up a variable–the variable test_score in this case–to “hide” the student-level information. The rows of the resulting dataset represent a group. The group in our example is the school variable: tibble( student = letters[1:10], school = rep(letters[11:15], 2), test_score = sample(0:100, 10, replace = TRUE) ) %&gt;% # Aggregate by school group_by(school) %&gt;% summarise(mean_score = mean(test_score)) ## # A tibble: 5 x 2 ## school mean_score ## &lt;chr&gt; &lt;dbl&gt; ## 1 k 38 ## 2 l 77 ## 3 m 25 ## 4 n 63.5 ## 5 o 30.5 Notice here that this dataset no longer identifies individual students. Student-level data for analysis of local populations. Aggregate data for base rate and context. Longitudinal analysis is typically done with student-level data because educators are interested in what happens to students over time. So if you cannot access student-level data, how do we use aggregate data to offer value to the analytic conversation? Aggregate data is valuable because it allows us to learn from populations that are larger or different from the local student-level population. Think of it as an opportunity to learn from totaled up student data from other states or the whole country. In his book Thinking Fast and Slow, Daniel Kahneman (2011) discusses the importance of learning from larger populations, a context he refers to as the base rate. The base rate fallacy is the tendency to only focus on conclusions we can draw from immediately available information. It’s the difference between computing how often a student at one school is identified for special education services (student-level data) and how often students are identified for special educations services nationally (base rate data). We can use aggregate data to combat the baserate fallacy by putting what we learn from local student data in the context of surrounding populations. For example, consider an analysis of student-level data in a school district over time. Student-level data allows us to ask questions about our local population: One such question is: Are the rates of special education identification for male students different from other gender identitites in our district? This style of question looks inward at your own educational system. Taking a cue from Daniel Kahneman, we should also ask what this pattern looks like in other states or in the country. Aggregate data allows us to ask questions about a larger population: One such question is Are the rates of special education identification for male students different from other gender identities in the United States? This style of question looks for answers outside your own educational system. The combination of the two lines of inquiry are powerful way to generate new knowledge about the student experience. So education data scientists should not despair in situations where they cannot access student-level data. Aggregate data is a powerful way to learn from state level or national level data when an MOU for student-level data is not possible. In situations where student-level data is available, including aggregate data is an excellent way to combat the base rate fallacy. 12.9 References Wickham, H. (2014). Tidy data. Journal of Statistical Software, 59(10). Retrieved from https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf Park, A. (2016). Reachable: data collection methods for sexual orientation and gender identity. Retrieved from https://williamsinstitute.law.ucla.edu/wp-content/uploads/Reachable-Data-collection-methods-for-sexual-orientation-gender-identity-March-2016.pdf Healy, K. (2018). Data visualization: A practical introduction. Princeton, NJ: Princeton University Press. Kahneman, D. (2011). Thinking, fast and slow. New York :Farrar, Straus and Giroux. "],
["introducing-data-science-tools-to-your-education-job.html", "13 Introducing Data Science Tools To Your Education Job 13.1 Introduction 13.2 The Gift of Speed and Scale 13.3 Solving Problems Together 13.4 For K-12 Teachers", " 13 Introducing Data Science Tools To Your Education Job 13.1 Introduction The purpose of this section is to explore the reality of what it is like to take new found data science skills into your work place with the challenge of finding practical ways to use your skills, encouraging your coworkers to be better users of data, and develop analytic routines that are individualized to the needs of your organization. Whether you are helping an education institution as a consultant, an administrator leading teachers at a school, or a university department chair, there are things you can do to transform what you’ve learned in the abstract into more concrete learning objectives in the context of your education work place. We’ll discuss this topic using two areas of focus: bringing your organization the gift of speed and scale, and the importance of connecting well with others. We’ll close this chapter by discussing some of the ways that K-12 teachers in particular might engage a work culture that is bringing on data science as a problem-solving tool. 13.2 The Gift of Speed and Scale The power of doing data a analysis with a programming language like R comes from two improvements over tools like Excel and Google Sheets. These improvements are 1. a massive boost in the speed of your work and 2. a massive boost in the size of the size of the datasets you analyze. Here are some approaches to introducing data science to your education workplace that focus on making the most of these increases in speed and scale. 13.2.1 Working With Data Faster Data analysts who have have an efficient analytic process understand their clients’ questions and participate by rapidly cycling through analysis and discussion. They quickly accumulate skill and experience because their routines facilitate many cycles of data analysis. Roger Peng and Elizabeth Matsui discuss epicycles of analysis in their book The Art of Data Science. In their book R for Data Science, Garrett Grolemund and Hadley Wickham demonstrate a routine for data exploration. When the problem space is not clearly defined, as is often the case with education data analysis questions, the path to get from the initial question to analysis itself is full of detours and distractions. Having a routine that points you to the next immediate analytic step gets the analyst started quickly, and many quick starts results in a lot of data analyzed. But speed gives us more than just an accelerated flow of experience or the thrill of rapidly getting to the bottom of a teacher’s data inquiry. It fuels the creativity required to understand problems in education and the imaginative solutions required to address them. Analyzing data quickly keeps the analytic momentum going at the speed needed to indulge organic exploration of the problem. Imagine an education consultant working with a school district to help them measure the effect of a new intervention on how well their students are learning math. During this process the superintendent presents the idea of comparing quiz scores at the schools in the district. The speed at which the consultant offers answers is important for the purposes of keeping the analytic conversation going. When a consultant quickly answers a teacher’s analytic question about their students’ latest batch of quiz scores, the collaborative analytic process feels more like a fast-paced inspiring conversation with a teammate instead of sluggish correspondence between two people on opposite ends of the country. We’ve all experienced situations where a question like “Is this batch of quiz scores meaningfully different from the ones my students had six months ago?” took so long to answer that the question itself is unimportant by the time the answer arrives! Users of data science techniques in education have wonderful opportunities to contribute in situations like this because speedy answers can be the very thing that sparks more important analytic questions. In our example of the education consultant presented with a superintendent’s curiosity about quiz score results, it is not too hard to imagine many other great questions resulting from the initial answers: How big was the effect of the new intervention, if any? Do we see similar effects across student subgroups, especially the subgroups we are trying to help the most? Do we see similar effects across grade levels? The trick here is to use statistics, programming, and knowledge about education to raise and answer the right questions quickly so the process feels like a conversation. When there’s too much time between analytic questions and their answers, educators lose the momentum required to follow the logical and exploratory path towards understanding the needs of their students. Example: Preparing quiz data to compute average scores # TODO: Add an intervention column to make this example feel more connected to the anecdote Let’s take our example of the education consultant tasked with computing the average quiz scores. Imagine the school district uses an online quiz system and each teacher’s quiz export looks like this: library(tidyverse) ## ── Attaching packages ────────────── ## ✔ ggplot2 3.2.1 ✔ purrr 0.3.3 ## ✔ tibble 2.1.3 ✔ dplyr 0.8.3 ## ✔ tidyr 1.0.0 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ─────────────────────── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() set.seed(45) quizzes_1 &lt;- tibble( teacher_id = 1, student_id = c(1:3), quiz_1 = sample(c(0:100), 3, replace = TRUE), quiz_2 = sample(c(0:100), 3, replace = TRUE), quiz_3 = sample(c(0:100), 3, replace = TRUE) ) quizzes_1 ## # A tibble: 3 x 5 ## teacher_id student_id quiz_1 quiz_2 quiz_3 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 36 95 82 ## 2 1 2 74 38 10 ## 3 1 3 45 57 63 Tools like Excel and Google Sheets can help you compute statistics like mean scores for each quiz or mean scores for each student fairly quickly, but what if you’d like to do that for five teachers using the exact same method? First, let’s tidy the data. This will prepare our data nicely to compute any number of summary statistics or plot results. Using gather to separate the quiz number and its score for each student will get us a long way: quizzes_1 %&gt;% gather(quiz_number, score, -c(teacher_id, student_id)) ## # A tibble: 9 x 4 ## teacher_id student_id quiz_number score ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 1 quiz_1 36 ## 2 1 2 quiz_1 74 ## 3 1 3 quiz_1 45 ## 4 1 1 quiz_2 95 ## 5 1 2 quiz_2 38 ## 6 1 3 quiz_2 57 ## 7 1 1 quiz_3 82 ## 8 1 2 quiz_3 10 ## 9 1 3 quiz_3 63 Note now that in the first version of this dataset, each individual row represented a unique combination of teacher and student. After using gather, each row is now a unique combination of teacher, student, and quiz number. This is often talked about as changing a dataset from “wide” to “narrow” because of the change in the width of the dataset. The benefit to this change is that we can compute summary statistics by grouping values in any of the new columns. For example, here is how we would compute the mean quiz score for each student: quizzes_1 %&gt;% gather(quiz_number, score, -c(teacher_id, student_id)) %&gt;% group_by(student_id) %&gt;% summarise(quiz_mean = mean(score)) ## # A tibble: 3 x 2 ## student_id quiz_mean ## &lt;int&gt; &lt;dbl&gt; ## 1 1 71 ## 2 2 40.7 ## 3 3 55 Again, for one dataset this computation is fairly straight forward and can be done with a number of software tools. But what if the education consultant in our example wants to do this repeatedly for twenty five teacher quiz exports? Let’s look at one way we can do this fairly quickly using R. We’ll start by creating two additional datasets as an example. To make things feel authentic, we’ll also add a column to show if the students participated in a new intervention. # Add intervention column to first dataset quizzes_1 &lt;- quizzes_1 %&gt;% mutate(intervention = sample(c(0, 1), 3, replace = TRUE)) # Second imaginary dataset quizzes_2 &lt;- tibble( teacher_id = 2, student_id = c(4:6), quiz_1 = sample(c(0:100), 3, replace = TRUE), quiz_2 = sample(c(0:100), 3, replace = TRUE), quiz_3 = sample(c(0:100), 3, replace = TRUE), intervention = sample(c(0, 1), 3, replace = TRUE) ) # Third imaginary dataset quizzes_3 &lt;- tibble( teacher_id = 3, student_id = c(7:9), quiz_1 = sample(c(0:100), 3, replace = TRUE), quiz_2 = sample(c(0:100), 3, replace = TRUE), quiz_3 = sample(c(0:100), 3, replace = TRUE), intervention = sample(c(0, 1), 3, replace = TRUE) ) The method we’ll use to compute the mean quiz score for each student is to: Combine all the datasets into one big dataset: Use bind_rows to combine all three quiz exports into one dataset. Remember, this can be done because each teacher’s export uses the same imaginary online quiz system and export feature and thus use the same number of columns and variable names Reuse the code from the first dataset on the new bigger dataset: Paste the code we used in the first example into the script so it cleans and computes the mean on the combined dataset Compute the mean of each student: Now that the data is arranged so that each row is a unique combination of teacher, student, quiz number, and intervention status, we can compute the mean quiz score for each student. # Use `bind_rows` to combine the three quiz exports into one big dataset all_quizzes &lt;- bind_rows(quizzes_1, quizzes_2, quizzes_3) Note there are now nine rows, one for each student in our dataset of three teacher quiz exports: all_quizzes ## # A tibble: 9 x 6 ## teacher_id student_id quiz_1 quiz_2 quiz_3 intervention ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 36 95 82 0 ## 2 1 2 74 38 10 1 ## 3 1 3 45 57 63 0 ## 4 2 4 92 27 15 0 ## 5 2 5 37 80 99 1 ## 6 2 6 67 52 99 1 ## 7 3 7 60 78 13 0 ## 8 3 8 29 1 89 0 ## 9 3 9 93 52 25 1 We’ll combine the cleaning and computation of the mean steps neatly into one this chunk of code: # Reuse the code from the first dataset on the new bigger dataset all_quizzes %&gt;% gather(quiz_number, score, -c(teacher_id, student_id, intervention)) %&gt;% # Compute the mean of each student group_by(student_id, intervention ) %&gt;% summarise(quiz_mean = mean(score)) ## # A tibble: 9 x 3 ## # Groups: student_id [9] ## student_id intervention quiz_mean ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 71 ## 2 2 1 40.7 ## 3 3 0 55 ## 4 4 0 44.7 ## 5 5 1 72 ## 6 6 1 72.7 ## 7 7 0 50.3 ## 8 8 0 39.7 ## 9 9 1 56.7 Note here that our imaginary education consultant from the example is thinking ahead by including the intervention column. By doing so she’s opened the possibility of collaboratively exploring any possible differences in the scores between the students who had the intervention and the students who did not when she reviews and discusses these results with the school staff. Adding these types of details ahead of time is one way to build conversation starters into your collaborations. It is also a way to get faster at responding to curiosities by anticipating useful questions from your clients. The difference in time it takes to do this on three quiz exports using R versus non-programming tools is perhaps not significant. But the speed of computing means across larger volumes of data–say thirty quiz exports–is truly useful to an education consultant looking to help many educators. Summary While getting fast at answering analytic questions is not a silver bullet (but really, what is?), it does have a chain effect often leads to creative solutions. It works something like this: Answering analytic questions faster helps more people Helping more people creates opportunities for more data science practice Helping more people also helps educate those same people about the solutions data science tools can offer Lots of practice combined with a common understanding of the value of data science tools in the education workplace nurtures confidence Confidence leads to the courage required to experiment with interesting solutions for designing the best solutions for students Here are more ways to get faster at answering analytic questions: Recognize when you are using similar chunks of code to do repetitive operations. Store that code in an accessible place and reuse it Keep a notebook of the questions teachers and administrators ask to help you develop an instinct for common patterns of questions. Write your code to anticipate these questions Learn to use functions and packages like purrr to work on many datasets at once Install a prototyping habit by getting comfortable with quickly producing rough first drafts of your analysis. Your audience can give valuable feedback early and feel like you are quickly on the path to developing useful answers to their questions 13.2.2 Working With More Data Improving outcomes in education is about learning, obviously for the students, but just as importantly for the people teaching the students. The more data is available to examine, the more school staff learn about what is working for their students. Using R to prepare and analyze data so it is repeatable and easy to share increases the amount of data you can work with on an order of magnitude compared to tools like Google Sheets. When cleaning and analyzing data is laborious, people tend to generate less data. This can be a problem because less data means less context for the data you do have. Without context, it is difficult to conduct one of the primary cognitive tasks of data analysis: making comparisons. For example, imagine a teacher whose students have an average quiz score of 75 percent. This information is helpful to the teacher because it shows her how close she is to some pre-determined average quiz score goal, say 95 percent. But that data alone doesn’t tell the teacher how unusual that class average is. For that, you need context. Say that line of code used to compute this teacher’s class average quiz score was applied to every classroom and she learned that the school average for the same quiz was 77 percent. From this information the teacher learns that her class average is not very different from everyone else’s. This is more information than just the knowledge that her class’s average was less than her pre-determined goal of 95 percent. This is where using R for data analysis enters the conversation. Working with data past a certain size, say 10,000 rows, is difficult because you have to interact with each row through the graphical user interface. Instead, you can work with larger datasets like using programming languages like R to issue complex instructions for acting on the data rather than using a mouse and keyboard to act on what you can see on the screen. Example: Replacing Many Student Names With Numerical IDs Say, for example, an elementary school administrator wants to replace each student name in a classroom dataset with a unique numerical ID. Doing this in a spreadsheet using good old fashioned data entry is fairly straightforward. Doing this for a whole school’s worth of classrooms though, demands a different approach. Rather than hand enter a unique id into a spreadsheet, the administrator can write an R script that executes the following steps: Use read_csv to store every classroom’s student list into the computer’s memory Use bind_rows to combine the separate lists into one long list Use mutate to replace student names with a randomized and unique numerical ID Use split to separate the data into classrooms again Use purrr and write_csv to create and rename individual spreadsheets to send back to teachers With some initial investment into thoughtful coding on the front end of this problem, the admininistrator now has a script she can use repeatedly in the future when she needs to do this task again. # TODO: More examples of differences in scale Other Ways to Reimagine the Scale of Your Work REFLECT ON YOUR CURRENT SCALE. THEN PUSH TO THE NEXT LEVEL: When you’ve been using the same data analysis tools and routines for a long time, it’s easy to forget to reflect on how you work. The analytic questions we ask, the datasets we use, and the scale of the analytic questions become automatic because for the most part they’ve delivered results. When you introduce data science techniques and R into your education analysis workflow, you also introduce an opportunity to ask yourself: How can I put this analytic question in context by analyzing on a larger scale? When an education client or coworker asks for help answering an analytic question, consider the following: At what level is this question about, student, classroom, school, district, regional, state, or federal? What can we learn by answering the analytic question at the current level, but also at the next level of scale up? If a teacher asks you to analyze the attendance pattern of one student, see what you learn by comparing to the the attendance pattern of the whole classroom or the whole school. If a superintendent of a school district asks you to analyze the behavior referrals of a school, analyze the behavior referrals of every school in the district. One of the many benefits of using programming languages like R to analyze data is that once you write code for one dataset, it can be used with many datasets with a relatively small amount of additional work. LOOK FOR LOTS OF SIMILARLY STRUCTURED DATA: Train your eyes to be alert to repositories that contain many datasets that have the exact same structure, then design ways to act on all those datasets at once. Data systems in education generate standardized data tables all the time. It’s one of the side effects of automation. Software developers design data systems to automatically generate many datasets for many people. The result is many datasets that contain different data, but all have the same number of columns and the same column names. This uniformity creates the perfect condition for R scripts to automatically act on these datasets in a way that is predictable and repeatable. Imagine a student information system that exports a list of students, their teacher, their grade level, and the number of school days attended to date. School administrator’s that have a weekly routine of exporting this data and storing it in a folder on their laptop will generate many uniformly structured datasets. When you train your eyes to see this as an opportunity to act on a lot of data at once, you will find an abundance of chances to transform data on a large scale so school staff can freely explore and ask questions aimed at improving the student experience. CLEANING DATA: Folks who work in education want to look at data about their students with tools like Excel, but the data is frequently not ready for analysis. You can empower these folks to explore data and ask more questions by being alert to opportunities to prepare lots of data for analysis. Offer to clean a dataset! Then do it again and do it fast. When you get into this habit, you not only train your data cleaning skills but you also train your education client’s expectations for how quickly you can prepare data for them. 13.3 Solving Problems Together Steven Spielberg said, “When I was a kid, there was no collaboration; it’s you with a camera bossing your friend around. But as an adult, filmmaking is all about appreciating the talents of the people you surround yourself with and knowing you could never have made any of these films by yourself.” Data science techniques are a powerful addition to an educational organization’s problem-solving capacity. But when you’re the only person who codes or fits statistical models, it’s easy to forget that the best solutions magically arrive when many perspectives come crashing together. Here are some things to think about as you challenge yourself to introduce data science to your education workplace in a lasting and meaningful way. 13.3.1 Data Science in Education and Empathy One definition of empathy is seeing things as others do, which points to a barrier to our mission of discovering ways to use our data science skills to improve the experience of learners–it is all too easy to assume that our coworkers will be inspired by possibilities of data science as you are. In 1990 Elizabeth Newton, then a Stanford University graduate, asked research subjects to “tap” out well-known songs with their fingers and estimate how many people would recognize the songs. She found that they overestimated every time! When we know a subject well, we tend to forget the experience of not knowing that subject. So how do we make use of this knowledge? First, listen carefully to your coworkers as they work with data. As you listen, aim to understand the thinking process they use when making sense of reports, tables, and graphs. This will help you understand the problems and solutions they gravitate towards. Second, ask them if you can “borrow the problem” for a bit. “Borrowing a problem” is not solving it for them, it’s using a little data science magic to get them unstuck so they can continue solving the problem the way they want to. If they’re struggling to make a scatter plot from their pivot table data, offer to help by cleaning and summarizing the dataset before they try again. Third, if your first attempt at borrowing the problem didn’t help, make an effort to learn more. Doing data science together is a conversation, so ask them how it went after you cleaned the dataset. Then listen, understand, and try again. After many rounds of this process, you may find your coworkers willing to try new methods for advancing their goals. A workplace going from not using data science to using data science regularly is a process that takes longer than you think. Responses to new ideas might include excitement and inspiration, but they might just as likely include resistance and fear. Changing the way an organization works requires new skills which often take years to learn. But here we are talking about one part of this change that is easily missed: listening to people and the system and using empathy to determine the unique place in your education organization that your data science skills will help students the most. Introducing data science techniques to your system is as much about having good people skills and empathy as it is about learning how to code and fit models. Data scientists and non-data scientists in education are similar in this regard–they both get excited and inspired by solving meaningful problems for their students. Once we recognize that that is the unifying goal, the exploration of how we do that with a diversity of expertise and tools begins. When we use empathy to connect with our coworkers about the common problems we are solving, we open the door to all kinds of solutions. Data science in education becomes a tool for a student-centered common cause, not an end in and of itself. Here are some reflection questions and exercise to use to inspire connection in your education workplace. Practice these questions both as personal reflections and also as questions you ask your coworkers: What does data analysis in our organiztion look like today? How do I wish data analysis will look like in the future? What is the hardest challenge I face in building my vision of student learning? What is one story about a rewarding experience I had with a student? 13.3.2 Create a Daily Practice Commitment That Answers Someone Else’s Question In his book Feck Perfuction, designer James Victore writes “Success goes to those who keep moving, to those who can practice, make mistakes, fail, and still progress. It all adds up. Like exercise for muscles, the more you learn, the more you develop, and the stronger your skills become.” Doing data science is a skill and like all skills, repetition and mistakes are their fuel for learning. But what happens if you are the first person to do data science in your education workplace? When you have no data science mentors, analytics routines, or examples of past practice, it can feel aimless to say the least. The antidote to that aimlessness is daily practice. Commit to writing code everyday. Even the the simplest three line scripts have a way of adding to your growing programming instincts. Train your ears to be radars for data projects that are usually done in a spreadsheet, then take them on and do them i R. Need the average amount of time a student with disabilities spends in speech and language sessions? Try it in R. Need to rename the columns in a student quiz dataset? Try it in R. The principal is hand assembling twelve classroom attendance sheets into one dataset? You get the picture. Now along the path of data science daily practice you may discover that your non-data science coworkers start kindly declining your offers for help. In my experience there is nothing mean happening here, but rather this is a response to imagining what it’s like to do what you are offering to do using the more commonly found spreadsheet applications. As your programming and statistics skills progress, some of the tasks you offer to help with will be the kind that, if done in a spreadsheet app, are overwhelmingly difficult and time intensive. So in environments where programming is not used for data analysis, declining your offers of help are more perceived acts of kindness to you and probably not statements about the usefulness of your work. As frustrating as these situations might be, they are necessary experiences as an organization learns just how available speed and scale of data analysis are when you use programming as a tool. In fact, these are opportunities you should seize because they serve both as daily practice and as demonstrations of the speed and scale programming for data analysis provides. 13.3.3 Build Your Network It is widely accepted that participating in personal and professional networks is important for survival, thriving, and innovation. The path to connecting to a data science in education network is apparent if your education workplace has an analytics department, but it will take a little more thought if you are the lone data scientist. When looking for allies that will inspire and teach you, the mind immediately searches for other programmers and statisticians and to be sure, these are relationships that will help you and the organization grow in its analytic approach. What the authors argue here is that data science in eduation is not just about bringing programming and statistics, but in the broader view is about evolving the whole approach to analytics. When viewed that way, members of a network broaden beyond just programmers and statisticians. It grows to include administrators and staff who are endlessly curious about the lives of students, graduate students fascinated with unique research methodologies, and designers who create interesting approaches to measurement. Networks for growing data science in education are not limited to the workplace. There are plenty of online and in real life chances to participate in a network that are just as rewarding as the networks you participate in during regular work hours. Here are a few to check out: Communities on Twitter like #RLadies and #rstats Local coding communities Conferences like rstudio::conf and useR! Online forums like RStudio Community 13.4 For K-12 Teachers We’ve used almost all of this chapter to explore what to think about and what to do to help you bring your data science skills to your education workplace. So far the discussion has been from the data scientist’s point of view, but what if you are one of the many who have an interest in analytics but very little interest in programming and statistics? Teachers in elementary and high schools are faced with a mind boggling amount of student data. A study by the Data Quality Campaign estimated that “95 percent of teachers use a combination of academic data (test scores, graduation rates, etc.) and nonacademic data (attendance, classroom, behavior, etc.) to understand their students’ performance”. 57 percent of the teachers in the study said a lack of time was a barrier to using the data they have. Yet the majority of teachers aren’t interested in learning a programming language and statistical methods as a way to get better at analytics. Afterall, most teachers chose their profession because they love teaching, not because they enjoy cleaning datasets and evaluating statistical model output. But to leave them out feels like a glaring ommission in a field where perhaps the most important shared value is the effective teaching of students. If you do happen to be an elementary or high school teacher who wants use programming and statistics to improve how you use data, you will find the approaches in this book useful. But if you are not that person, there is still much to explore that will lead to a rewarding experience as you grow your analytic skill. This book lacks the scope to explore this topic thoroughly, but there are many ways to improve how you use data without requiring a programming language or deep knowledge of statistics. For example, you can explore what is perhaps the most important element of starting a data analysis: asking the correct question. Chapter three of Roger Peng and Elizabeth Matsui’s book, The Art of Data Science provides a useful process for getting better at asking data questions. Given how often data is served to us through data visualizations, it is important to learn the best ways to create and consume these visualizations. Chapter one of Kieran Healy’s book Data Visualization: A Practical Introduction, explores this topic using excellent examples and writing. For pratical applications of a data-informed approach, Learning to Improve: How America’s Schools Can Get Better at Getting Better by Anthony Bryk, Louis Gomez, Alicia Grunow, and Paul LaMahieu offers a thorough explanation of the improvement science process. The book is filled with examples of how data is used to understand problems and trial solutions. The final recommendation for elementary and secondary teachers wanting to get better at analysis is this: find, and partner with, someone who can help you answer the questions you have about how to serve your students better. You have the professional experience to come up with the right ideas and the curiosity to see what these ideas look like in the classroom. Inviting someone who can collaborate with you and help you measure the success of your ideas can be a rewarding partnership for you and your students. "],
["teaching-data-science.html", "14 Teaching Data Science 14.1 The pedagogical principles this book is based upon 14.2 Strategies for teaching data science 14.3 General strategies related to teaching and learning 14.4 Summary", " 14 Teaching Data Science This book is focused on the application of data science to education. In other words, this book focuses on how to apply data science to questions of teaching, learning, and educational systems. The previous chapters have addresses these topics narratively and in the form of walk-throughs for common questions (or problems) and the types of data encountered in education. While this book is focused on applying data science to education, an important consideration for data science is how to teach others about it. This is particularly the case for a book that is by a team of authors who are involved in education. Also, we expect readers of this book - many who will also be involved in education - will be interested in teaching others about data science. This chapter, then, focuses on teaching data science to others. This chapter is organized around three topics, which progress from concrete and specific (with reference to pedagogical approach we used to guide how we wrote this book) to more general ideas and findings from educational research about teaching and learning. The pedagogical principles this book is based upon Strategies for teaching data science General strategies related to teaching and learning 14.1 The pedagogical principles this book is based upon As the authors of a book about data science in education - and readers of books that taught us about data science - we considered what would make it effective for our readers when we set out to write it. The result of this process was a pedagogical framework that consists of three principles: problem-based learning, differentiation, and universal design. We consider each of these in turn. 14.1.1 Problem-based learning Problem-based learning (PBL) is a method of instruction that presents learners with a real-world challenge in which they must apply their skills and knowledge to solve. We applied this principle to the design of this book through including walk-throughs for common questions with respect to data science and education. Whereas some topics may benefit less from such an approach (and the inclusion of walk-throughs), for data science, we believe this is important because we do not have all of the right answers in this text. Moreover, there is not one right statistical model or algorithm, a technique to write code or even software and tools to utilize. Thus, the text features walkthroughs that reflect the types of challenges that educational data scientists may encounter in the course of their work: readers may choose to go about approaching the analysis of the data used in each walkthrough differently. At the end of walkthrough, there exist exercises that provide the opportunity for readers to extend and apply the ideas presented in the chapter. Moreover, the challenges are structured in such a way that readers return to some of them, but with different aims, over the course of the book. 14.1.2 Differentiation Differentiation is a method for providing multiple pathways for learners to engage with, understand, and ultimately apply new content and new skills. To differentiate this text, we first created personas for who we expected to be common groups of readers of the book (see Wilson, 2019, for an example). We then differentiate the book by recognizing and providing background content/skills (either explicitly or through reference to other resources), embedded checks for understanding, and recommendations for where to begin based upon prior expertise. We also provide screenshots that are annotated and reflective of the content in the text to help to show readers how they are able to use what they are reading about. Part of differentiating this book concerns for whom we are differentiating it. We consider inclusivity (in terms of who belongs as a part of the audience for this text and how this broader view of who participates in data science implies the types of challenges, topics, and data that we include in the book) and accessibility (technically, in terms of how a wide audience of readers is able to access and use the book, as well as in terms of the ways in which the content is written based on the unique assets that those in education bring) along with how we differentiate the book. 14.1.3 Universal Design Universal design is a series of principles which guide the creation of spaces that are inclusive and accessible to individuals from all walks of life regardless of age, size, ability, or disability. While traditionally applied to physical spaces, we have extended these principles to the creation of this book in such a way that the text and accompanying materials will be designed for individuals from all walks of life, regardless of educational level, background, ability, or disability. Many of the seven guiding principles of Universal Design are readily transferable to the creation of a text, such as equitable use, flexibility in use (aided in large part through differentiation), simple and intuitive use, perceptible information, and tolerance for error. 14.1.4 Working in the Open We started writing this book in the open, on GitHub. This allowed us to share the book as it developed. Writing the book in the open also allowed others from the wider educational data science and data science community to contribute. These contributions included writing sections of the book in which contributors had specific expertise, asking clarifying questions, and, even, creating a logo for the book which informed our choice of a color palette. We decided to write this book in the open after witnessing the success of other books on data science (such as Wickham’s (2019) Advanced R book. 14.2 Strategies for teaching data science You may be interested in teaching others’ data science. You may be doing this informally (such as by teaching a colleague with whom you work in your school district or organization), in a formal environment (such as a class on data science for educational data scientists or analysts), or in some setting in-between (such as a workshop you are asked to provide). There is some research on teaching data science, as well as practical advice from experienced instructors that can inform these efforts, which we detail in this section. 14.2.1 Provide a home base for learners to access resources (and to learn more) As we discuss in the next section, along with other important factors (such as learners’ motivation and having a supportive atmospher), learning strategies can make a difference for learners. Especially when it comes to learning to do data science, there are many tools and resources to keep track of, such as: How to download and install R How to download and install R Studio How to install packages How to access resources related to the workshop or course (or simply other resources you wish to share) How to contact the instructor How to get help and learn more Having a “home base,” where you can remind learners to first look to for resources, can help to lower some of learners’ demands in terms of remembering how these tools and resources can be accessed. One way to do this is through a personal website. Another is through GitHub pages. For some organizations, a proprietary learning management system - such as Desire2Learn, Blackboard, Moodle, or Canvas - can be helpful (especially if your learners are accustomed to using them). 14.2.2 When it comes to writing code, think early and often It is important to get learners to start writing code early and often. It can be tempting to teach classes or workshops that front-load content about data science and using R. While this information is doubtlessly important, it can mean that those you are teaching do not have the chance to do the things that they want to do, including installing R (and R Studio) and beginning to run analyses. Because of this, we recommend starting with strategies that lower the barrier to writing code for learners. Ways to do this in teaching settings include: Using R Studio Cloud Providing an R Markdown document for learners to work through Providing a data set and ideas for how to begin exploring it While these strategies are especially helpful for courses or workshops, they can be translated to teaching and learning R in tutoring (or “one-on-one”) opportunities for learners. In these cases, being able to work through and to modify an existing analysis (perhaps in R Studio Cloud) can be a way to quickly begin to run analyses - and to use the analysis as a template for analyses associated with other projects. Also, having a data set associated with a project or analysis - and a real need to analyze it using R - can be an outstanding way for an individual to learn to use R. 14.2.3 Don’t touch that keyboard! Resist helping learners to the point of hindering their learning. Wilson (2019) writes about the way in which those teaching others about R - or to program, in general - can find it easier to correct errors in learners’ work. But, by fixing errors, learners’ may perceive themselves to not be capable of carrying out all of the steps needed in an analysis on their own. This strategy relates to a broader issue, as well: issues that have to do with writing code in a way that runs correctly (e.g., with the correct capitalization and syntax) can be minor to those with experience programming, but can be major barriers to using R in an independent way for those new to it. For example, becoming comfortable with where arguments to functions belong and how to separate them, how to use brackets belong in functions or loops, and when it is necessary to use an assignment operator can be completely new to beginners: doing these steps for learners may push their learning later when they do not have as many resources available to help them than when you are teaching them. So, consider taking the additional time needed to help learners to navigate minor issues and errors in their code: it can pay off in increased motivation on their part in the longer-term. 14.2.4 Anticipate issues (and sacrifice accuracy for clarity) Don’t worry about being perfectly accurate early on, especially if doing so would lead to learners who are less interested in the topic you are teaching. For example, there are complicated issues at the heart of why data that is built-in to packages or to R (such as the iris dataset) appear in the environment after they are first used in an R session (see the section on “promises” in Wickham(https://adv-r.hadley.nz/)). Similarly, there are complicated issues that pertain to how functions are evaluated that can explain why it is important to provide the name of packages installed via install.packages() (whereas the names of arguments to othe functions, such as dplyr::select() do not need to be quoted). In these cases, wherein additional details may not be helpful to beginners, it can be valuable to important these questions (and the issues that are assocaited with them), but to have responses or answers that provide more clarity, rather than confusion. For example, 14.2.5 Start lessons or activities with visualizing data There are examples from data science books (Wickham &amp; Grolemund, 2017) and past research (e.g., Lehrer, Kim, &amp; Schauble, 2007; see Lehrer &amp; Schauble, 2015, for more examples) that suggests that starting with visualizing data can be benefician in terms of learners’ ability to work with data. Wickham and Grolemund (2017) write that they begin their book, Data Science Using R, with a chapter on visualization, because doing so allows learners to create something that they can share immediately, whereas tasks such as loading data can be rife with issues - and does not immediately lead learners to have a product they can share. Lehrer, Kim, and Schauble (2007) show how providing students with an opportunity to invent statistics by displaying the data in new ways. This led to (productive) critique among fifth- and sixth-grade students and their teacher. 14.2.6 Consider representation in the data and examples you use One way to think about data is that it is objective, free of decisions about what to value or prioritize. Another is to consider data as a process that is value-laden, from deciding what question to ask (and what data to collect) to interpreting findings with attention to how others will make sense of them (e.g., O’Neill’s Weapons of Math Destruction, and Lehrer, Kim, and Schauble’s (2007) description of data modeling). From this broader view, choosing representative data is a choice, like others, that teachers can make. For example, instructors can choose data that directs attention to issues– equity-related issues in education, for example–that she or he believes would be valuable for students to analyze. We think this consideration is important - particularly when it comes to data-related issues that we consider to be objective, such as how variables are assumed and constructed to be dichotomous (such as variables for individual’s gender) or categorical (such as variables for individual’s race), when the truth may be that such variables are based on decisions that analysts or those collecting data made - decisions that may benefit from questioning. This consideration is also important when it comes to what data is used for teaching and learning. If names of individual’s in data exclusively from individuals from majority racial or ethnic group, for example, some learners may implicitly perceive the content being taught to be designed for others. While we may think that such issues are better left to those we are teaching to decide later on, setting the stage in classes, courses, and other contexts in which data science is taught and learned can set an important precedent for the data our learners use and how they use it. 14.2.7 Draw on other resources In this section of this chapter, we presented some strategies for teaching data science. There are others that go more into depth on this topic from different perspectives, such as the following: GAISE Guidelines: guidelines for teaching statistics Teaching Tech Together: a book on teaching programming Data Science for Undergraduates: a report on undergraduate data science education R Studio Education There are also a number of data science-related curricula (for the K-12 level) which may be helpful: Bootstrap Data Science Exploring CS, unit 5 Chromebook Data Science Oceans of Data Institute Curricula 14.3 General strategies related to teaching and learning The National Academy of Science commissioned a report, How People Learn, that aimed to summarize research from the educational psychology and the learning sciences on teaching and learning. In 2018, the book was updated in How People Learn II, which aimed to emphasize the social and cultural aspects of teaching and learning, which were not as much the focus of the earlier report. In this section, we highlight general strategies related to teaching and learning from the latest report (abbreviated as HPL2), with an emphasis on strategies applicable to teaching and learning data science. 14.3.1 Teaching and learning are complex One principle that HPL2 begins with is that learning is complex. in short, learning is not just about what learners know, or think, but is also about developmental, cultural, contextual, and historical factors - and distinctions between individuals with respect to each of these factors. This is an asset to teachers, as the authors of the report state: learners bring resources that can serve as a starting point for their learning trajectory when it comes to data science. These distinctions also mean that educators need to be concerned with - and to consider within their purview - factors well beyond what learners know, but also what their prior educational experiences have been and what resources and other individuals they have access to at work and at home, for example. 14.3.2 Learners learn many different things (consciously and unconsciously) We often think of learning in terms of objectives for specific lessons, but learners learn many different things at different times. The authors of HPL2 point out that individuals learn in response to different challenges and circumstances, including those in formal learning environements, such as workshops or classes. This principle implies a strategy that involves supporting learners to do data science, however and whenever they learn it. This means that it is both okay - and even to be expected - that learners may take away more from a problem they try to solve on their own, than what they do from a workshop or class (or even a degree!). This also suggests that learners may learn things that we do not anticipate, such as how intructors try to solve problems that arise in class. 14.3.3 Metacognition is important (even though it sounds more sophisticated than it is!) Educators and educational researchers often talk about metacognition, or thinking (and ideas) about thinking, as if it is something only very sophisticated learners do, but it is truly something much more commonplace, as people (and learners) are thinking about what they are learning and doing regularly. One strategy for instructors to support metacognition is to include moments wherein learners are asked to consider what they learned and what they would like to learn more about (exit tickets can be a great way to do this, but brief period in-class can also be used). Another strategy is to help learners to recognize when it is important to ask for help: Often, when doing data science, the right question to the right person (or community) can save hours of work. 14.3.4 Learning strategies matter While teachers are responsible for designing learning opportunities, learners also play an important role - in their own learning! Learning strategies, according to the authors of HPL2, matter, including those that help students to retrieve information and summarize and explain what they have learned (for themseleves and othres). There are many specific strategies documented in chapter four of HPL2. What is important for teachers of data science to know is less the specific strategies, and more the commitment to teaching learners how to learn. In addition to strategies for learners, teaching strategies, such as how content is spaced and sequenced, can also help learners. Design for How People Learn presents these strategies, based largely about instructional design research, that may be helpful to those teaching data science. 14.3.5 Educators can help students to learn Educators know that how motivated learners are matters. According to the authors of HPL2, teachers can make an impact on how motivated learners are. Some specific things that educators can do to support learners include helping learners to set (and to work toward) goals, selecting content that is valuable and interesting to learners, helping learners to have choices and showing them how they are in-control of their learning value, and supporting learners to feel good at and supported in what they are doing. In short, motivation matters, and may be especially important when teaching learners who do not see the value of data science (initially!). 14.4 Summary In this section, we described the pedagogical principles for this book and strategies for teaching data science. We also directed attention to more general strategies for teaching and learning. Teaching data science is a relatively new area, but data science educators are not alone, given resources that are beign developed and those that can be adapted from other disciplines and the wider body of educational research. "],
["learning-more.html", "15 Learning More 15.1 Introduction 15.2 Adopt a growth mindset 15.3 Discover new information 15.4 Ask for help 15.5 Share what you’ve learned 15.6 Welcome others 15.7 References", " 15 Learning More 15.1 Introduction If you’re reading this book cover to cover, you’ve been through quite a journey! So far, you’ve: Learned about the challenges of doing data science in education Practiced some basic coding and statistics techniques Worked through examples of analytic routines using education datasets Reflected on introducing data science to your education organization over time Learned about teaching data science to others We hope this book sparked an interest in data science that you want to nurture. We’ve talked to many people in your shoes–folks who care about educating students and want to help by using their data skills. Indeed we’ve found the common thread in our audience is wanting to use data to improve the experience of learners. It’s important to nurture this passion by keeping the learning going. Surrounding yourself with continuous learning experiences can turn this spark into a specialization that makes a real contribution to the lives of students. There are three reasons we feel these ongoing learning experiences are essential to realizing your vision for data in education. First, developing technical skills is an continuous process. The learning mindset is the same whether you’re taking your first steps toward using data science techniques or you’re a seasoned data scientist trying to make a bigger impact in education: there is always something new to learn about programming and statistics. Setting regular time aside to evolve your craft is a commitment to this mindset. Second, education and data science are like most industries–they are constantly evolving. That means today’s tools and best practices might be tomorrow’s outdated techniques. To keep up with changes, it is important to develop a learning routine that exposes you to the pulse of these two fields. Sometimes this means learning a new technique, sometimes it means deepening expertise in a technique you haven’t mastered, and other times it means revisiting a skill you’ve mastered long ago. And last, when you surround yourself with learning experiences, you inevitably surround yourself with others who are learning. Along your journey, you’ll interact with folks who are struggling through the same concepts as you, folks who are struggling through more complex concepts, and folks who are struggling with concepts you’ve already mastered. Participating in a community of learners has magical properties–it’s a place to learn, teach, inspire, and get inspired all at once. In his book Creative Calling (2019), Chase Jarvis touches on this very point: Whether online or in person, connecting with a community will support your learning efforts. It will also expose you to a diverse set of ideas that will dramatically enrich your perspective on what you’re learning. If you weren’t in love with your new skill before, this step can tip the balance. Passion is infectious. You’ll need to use your intuition to find the areas where you want to deepend your knowledge. When you feel it, go there and dive in. Remember that the learning experience includes all kinds of activities. It’s a combination of reading, doing, discussing, walking away, and coming back. Here are some activities to include in your practice. We hope you take these and construct your own system of rewarding learning experiences. s 15.2 Adopt a growth mindset It’s normal to feel overwhelmed while learning skills like R and data science. This is particularly true when these fields themsleves are learning and growing. The R, data science, and education communities are constantly developing new techniques to move the field forward. It’s part of the beauty of this work! When you’re feeling overwhemed by everything you’re trying to learn, consider adopting a growth mindset. Carol Dweck’s argues that we think of ourselves as being or not being a type of person. For example, we might think of ourselves as “math people” or “reading people”. What matters is whether or not this state is changeable. When we believe we can change, we adopt a desire to learn, choose to be around people who help us learn, and make the effort to learn. When we move from a fixed mindset to a growth mindset, we create the possibility of mastering new techniques and realizing our vision for using data in education. The (nuances of the growth mindset)(https://www.edweek.org/ew/articles/2015/09/23/carol-dweck-revisits-the-growth-mindset.html)) are beyond the scope of this book, but we do encourage the general belief that we can learn how to apply these techniques. We encourage you to adopt a growth mindset as a way to inspire learning and belief that you can introduce data science in your education job. In doing so, you’ll be joining other data scientists who created a way to contribute to their fields. 15.3 Discover new information The content you surround yourself with matters. You can learn a lot and stay inspired by high quality books, blog posts, journals, journalism, and talks. In his book Steal Like An Artist, author Austin Kleon encourages people to surround themselves with great content: “There’s an economic theory out there that if you take the incomes of your five closest friends and average them, the resulting number will be pretty close to your own income. I think the same thing is true of our idea incomes. You’re only going to be as good as the stuff you surround yourself with.” In our Resources chapter, we share books and online resources that inspire us and help us learn. Use these as a starting point and build on them by seeking out authors, data scientists, and educators that inspire you to learn and master your craft. There are lots of ways to do this. Some folks follow data scientists on social media and take note of articles or talks that are getting attention. Others read data informed publications like Fivethirtyeight, The Economist, or The Upshot in the New York Times. Whichever you choose, make sure to stick with something that you’re drawn to and you just might find yourself with a new learning habit that is rewarding and fun. 15.4 Ask for help So far, we’ve discussed learning activities you can do on your own. Data science is a team sport, so at eventually your learning will lead you to others in the data science community. You can do this in many ways, both virtual and in real life. Here are a few examples you can try online. Try these and learn about what you’re comfortable with. Then build on that to surround yourself with many ways to ask and answer questions. 15.4.1 Discussion forums Visiting discussion forums is a common way to learn and participate in the R community. Websites like R Studio Community and Stack Overflow are very popular ways to do this. On these forums you’ll find many years worth of discussion about R and statistics. It’s quite unusual to search these and not find a way to get unstuck. Many discussions include a reproducible example of code that you can copy and paste into your own R console. This is a fantastic way to learn! Consider learning best practices for asking forum questions. Including a reproducible example, or “reprex”, to communiate problems is a widely-accepted norm. (This r-blogger)[https://www.r-bloggers.com/three-tips-for-posting-good-questions-to-r-help-and-stack-overflow/] post about asking Stack Overflow questions and Jenny Bryan’s video about making reproducible examples are great places to learn more. #left off here December 11, 2019 15.4.2 GitHub repositories When you want to learn more about how a package works or engage a package’s online community, consider visiting the its GitHub repository. dplyr’s repository https://github.com/tidyverse/dplyr is a great example. You can start with the README then dive deeper in the vignettes, which contain demonstrations of the package’s functions. You can even browse the code on GitHub to learn more about how the packages work. Don’t worry, you won’t break anything! When you’re ready to see how the community engages a package’s authors, you can read through the Issues page. Each respository’s Issues page contains questions, feature requests, and bugs submitted by the programming community. Visit this page when you want to see if someone’s already submitted the coding challenge you’re working through. If you find you’re working on something that’s not a known problem, you can contribute by adding an issue. And finally, you can contribute to the development of packages by submitting code to the respository–this process is called a pull request. To learn more about contributing to packages, check out Kara Woo’s talk Anyone can play git/R: Tips for first-time contributions to R packages 15.5 Share what you’ve learned If you keep asking questions and finding solutions, you will soon find yourself ready to help others who are just getting started. The adage of learning by teaching applies here–answering someone else’s question also helps you deepen your learning and build empathy for new learners. Adopting a regular sharing routine is a great way to start helping others. A sharing routine encourages participation in the community, invites feedback for improvement, and calls on you to build your craft in a way that others can understand it. So what can you share? Really, what can’t you share? If you’ve built a cool function or visualization that took your project to the next level, you just might help or inspire someone else by sharing it. Maybe you’ve found an R package that really helped you–chances are it will help others. Sharing isn’t always about the output of your work, it can also be about how you work. Consider sharing a workflow you’ve developed or your experience at a recent data science conference. Anything that you learned or found interesting will be relevant to others too! What you share doesn’t have to be perfect. You can decide when you’re ready to share. Some data scientist’s blogs are polished and others are ideas-in-progress or shorter posts. You never know when someone will find value in your work, regardless of whether your work is in a refined state or not. Laslty, you can select your best work from all your sharing and use it as an online work portfolio. 15.5.1 Where to share There are many ways to share your work online. For rapid fire conversational sharing, Twitter. Be sure to use the hashtag #rstats to reach more data scientists. For long form sharing, consider posting to a data science blog. David Robinson’s blog post Advice to aspiring data scientists: start a blog is wonderful inspiration for getting started. If you decide to post to a blog, there are tools to help you post data science content regularly. The R package [blogdown](https://bookdown.org/yihui/blogdown/) is designed to help you create websites using R Markdown and a static website creator called Hugo. Blogdown makes it easy to create, run, and publish code directly from R Studio. Alison Hill at Oregon Health &amp; Science University (OHSU) has a great introduction on getting started with with Blogdown. When you do share a blog post or a tweet, broadcast what you have to say! On Twitter, use hashtags or “at” other community members to include them in the Tweet. On your blog, use blog aggregators that help share your posts to a wider audience. Here are two aggregators to get you started: R Weekly newsletter https://rweekly.org/ R Bloggers https://www.r-bloggers.com Finally, share the love by engaging your fellow data scientists! Retweet others, leave comments, and interact with the vibrant data science and R communities online. 15.6 Welcome others If you find yourself becoming an envangelist for R and data science in education–that’s what happened to us!–welcome folks who are curious and ready to learn. The strength of any community comes from its inclusiveness, safe learning environment, and capacity to welcome new members. The data science community is no exception–many members work hard to create an environment with active participants, engaging conversations, and celebrations for little and big data science wins. Our call to action is this: continue growing this inclusive and positive environment by being the community member you’d want in your own network. Data science in education is a wonderful Venn diagram of communities, with new members joining every day. Welcoming, helping, and teaching new members is a great way to contribute to a positive community and to continue your own learning. What better way to inspire new members than to share your work and how it has impacted the lives of students! 15.7 References Jarvis, C. (2019). Creative calling. New York, NY: HarperCollins Publishers. RStudio community Retrieved from https://community.rstudio.com Stack Overflow questions tagged [r]. Retrieved from https://stackoverflow.com/questions/tagged/r dplyr on github. Retrieved from https://github.com/tidyverse/dplyr Bryan, J. (2018). [Video] reproducible examples and the reprex package. Retrieved from https://community.rstudio.com/t/video-reproducible-examples-and-the-reprex-package/14732 Woo, K. (2018). Anyone can play git/R: tips for first-time contributions to R packages. Retrieved from https://speakerdeck.com/karawoo/r-tips-for-first-time-contributions-to-r-packages Robinson, D. (2017). Advice to aspiring data scientists: start a blog. Retrieved from http://varianceexplained.org/r/start-blog/ Hill, A. (2017). Up &amp; Running with blogdown. Retrieved from https://alison.rbind.io/post/up-and-running-with-blogdown/ "],
["additional-resources.html", "16 Additional Resources 16.1 Data science courses 16.2 R workshops 16.3 Education resources 16.4 Data visualization 16.5 Books related to data science in education 16.6 Articles related to data science in education 16.7 Programming with R 16.8 Package vignettes and descriptions of packages 16.9 Statistics 16.10 Software and R Packages 16.11 A career in data science 16.12 Places to share your work 16.13 Help forums 16.14 Cheat Sheets 16.15 Learning communities", " 16 Additional Resources 16.1 Data science courses Data science for social scientists by Landers (2019) University of Oregon Data Science Specialization for the College of Education by Anderson (2019) 16.2 R workshops Workshop on using R at the Association for Educational Communications and Technology 16.3 Education resources Bryk et al (2015). Learning to improve: How America’s schools can get better at getting better. Cambridge, MA: Harvard Education Press. Penuel et al (2016, April). Findings from a national study on research use among school and district leaders. Retrieved from http://ncrpp.org/assets/documents/NCRPP_Technical-Report-1_National-Survey-of-Research-Use.pdf. A survey of 733 school principals and district leaders within US mid-sized and large school districts, focused on how educational leaders use research to inform their decision-making. Geller et al (2019, October). Education data done right: lessons from the trenches of applied data science. Independently published. 16.4 Data visualization Tufte, E. (2006). Beautiful evidence. Cheshire, CT: Graphics Press LLC. Healy, K. (2018). Data visualization: A practical introduction. Princeton, NJ: Princeton University Press. Chang, W. (2013). R graphics cookbook. Sebastopol, CA: O’Reilly. 16.5 Books related to data science in education Krumm, A., Means, B., &amp; Bienkowski, M. (2018). Learning analytics goes to school: A collaborative approach to improving education. Routledge. Powers, K., &amp; Henderson, A. E. (Eds.). (2018). Cultivating a data culture in higher education. Routledge. Williamson, B. (2017). Big data in education: The digital future of learning, policy and practice. Sage. Lawson, J. (2015). Data Science in Higher Education: A Step-by-Step Introduction to Machine Learning for Institutional Researchers. CreateSpace. Swing, R. L. (2018). The Analytics Revolution in Higher Education: Big Data, Organizational Learning, and Student Success. Stylus Publishing, LLC. 16.6 Articles related to data science in education Williamson, B. (2017). Who owns educational theory? Big data, algorithms and the expert power of education data science. E-Learning and Digital Media, 14(3), 105-122. Liu, M. C., &amp; Huang, Y. M. (2017). The use of data science for education: The case of social-emotional learning. Smart Learning Environments, 4(1), 1. Rosenberg, J. M., Lawson, M. A., Anderson, D. J., Rutherford, T., &amp; Jones, R. S. (accepted pending minor revisions). Making Data Science “Count”: Data Science and Learning, Design, and Technology Research. In E. Romero-Hall (Ed.), Research Methods in Learning Design &amp; Technology. Routledge: New York, NY. 16.7 Programming with R Wickham, H. &amp; Grolemund, G. (2017). R for data science. Sebastopol, CA: O’Reilly. Teetor, P. (2011). R cookbook. Sebastopol, CA: O’Reilly. Bryan, J. &amp; Hestor, J. Happy git and github for the useR. Retrieved from https://happygitwithr.com Hill, A. (2017). Big magic with R: Creating learning beyond fear. Retrieved from https://speakerdeck.com/apreshill/big-magic-with-r-creative-learning-beyond-fear 16.8 Package vignettes and descriptions of packages Introduction to dplyr. Retreived from https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html A short introduction to the caret package. Retrieved from https://cran.r-project.org/web/packages/caret/vignettes/caret.html tidy data. Retrieved from https://tidyr.tidyverse.org/articles/tidy-data.html Wickham et al. (2019). Welcome to the Tidyverse. Journal of Open Source Software, 4(43). 1686-1691. https://joss.theoj.org/papers/10.21105/joss.01686 16.9 Statistics 16.9.1 Introductory Bruce, P. &amp; Bruce, A. (2017). Practical statistics for data scientists. Sebastopol, CA: O’Reilly. Navarro, D. (2019). Learning Statistics With R. https://learningstatisticswithr.com/ Field, A., Miles, J., &amp; Field, Z. (2012). Discovering statistics using R. Sage publications. Ismay, C., &amp; Kim, A. Y. (2019). ModernDice: Statistical inference via data science. CRC Press. https://moderndive.com/ James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2015). An introduction to statistical learning with applications in R. New York, NY: Springer. Peng, R. D. (2019). R programming for data science. Leanpub. https://leanpub.com/rprogramming Peng, R. D., &amp; Matsui, E. (2018). The art of data science. Leanpub. https://leanpub.com/artofdatascience 16.9.2 Advanced Gelman, A., &amp; Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press. Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction. Springer Science &amp; Business Media. West, B. T., Welch, K. B., &amp; Galecki, A. T. (2014). Linear mixed models: a practical guide using statistical software. Chapman and Hall/CRC. McElreath, R. (2018). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC. [see also https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/] 16.10 Software and R Packages Peng, R. D. (2019). Mastering software development in R. Leanpub. https://leanpub.com/msdr Wickham, H. (2015). R packages: Organize, test, document, and share your code. O’Reilly. http://r-pkgs.had.co.nz/ 16.11 A career in data science Robinson, E., &amp; Nolis, J. (2020). Building a career in data science. Manning. https://www.manning.com/books/build-a-career-in-data-science?a_aid=buildcareer&amp;a_bid=76784b6a 16.12 Places to share your work Twitter: twitter.com - Especially through the hashtags we mentioned below LinkedIn: linkedin.com &lt;– !Say more here about how to share here? –&gt; Medium : medium.com &lt;– !Say more here about how to share here? –&gt; 16.13 Help forums RStudio community. Retrieved from https://community.rstudio.com Stack Overflow. Retrieved from http://stackoverflow.com/ 16.14 Cheat Sheets R Studio Cheat Sheets. https://rstudio.com/resources/cheatsheets/ see especially the dplyr, tidyr, purrr, ggplot2, and other cheat sheets 16.15 Learning communities Here some online communities and community resources we recommend: #rstats #tidyverse #RLadies #tidytuesday Here are two resources by co-author Mostipak related to the #r4ds community (from which #tidytuesday came): Mostipak, J. (2017). R4DS: the next iteration. Retrieved fromhttps://medium.com/(???) Mostipak, J. (2019). R4DS online learning community Improvements to self-taught data science &amp; the critical need for diversity, equity, and inclusion. Retrieved from https://resources.rstudio.com/rstudio-conf-2019/r4ds-online-learning-community-improvements-to-self-taught-data-science-and-the-critical-need-for-diversity-equity-and-inclusion-in-data-science-education "],
["conclusion-to-where-next.html", "17 Conclusion: To where next? 17.1 Take on challenges 17.2 Know when to turn to something else", " 17 Conclusion: To where next? At the conclusion of this book, we hope that you feel ready to take on the data- related question or problem you have wanted to address. We think that the work that you will do is important. In education, particularly, there is a lot of promise for how data can be used - by teachers, administrators, parents, and even by students. There is also some peril, though, especially if data is used that does not align with the goals and values of educators - and individually and collectively learning and changing. As you work toward that goal, we wish to offer some closing thoughts. 17.1 Take on challenges Many of the things that you surely want to do are likely beyond the realm of what is possible now. These challenges, from the social (the problem you are addressing is incredibly hard) and organizational (if your organization is not ready for the types of work you are trying to do) to the technical (perhaps what you want to do has literally never been done before), can be really difficult to tackle. One small suggestion we have is to tell a friend. When you’re frustrated, having a group you can turn to can be valuable. Relatedly, when R tells you that NULL is not TRUE (or any one of innumerable pain points), try to laugh. It is so common when doing data science to run into these frustrations that social media accounts have emerged to chronicle them (see (???)(https:/twitter.com/accidentalart))! Everyone, from the most experienced R coder (Hadley Wickham has mentioned this many times) to the person using R for the first time constantly runs into difficulties when trying something new - you are not alone! So, take on challenges: a joy of doing data science is that it is a field at this time, especially in education, is that the promise and peril of using data are both very real, and those taking on challenges have the chance to shape the direction of how data is used 17.2 Know when to turn to something else In addition to taking on challenges (and turning to friends and laughing), we want to acknowledge everyone needs a break. Sometimes, instead of writing one last line of R code, it can be better to do something else! An implication of this is that leaving off so that you can pick back up where you stopped is important. But, by doing so, you can live to code another day. We need as many data scientists working in different corners of education as possible, and we don’t want to lose you to burn-out! Similarly, when working on any project - even (mand aybe particularly those you are passionate about - it can be important to switch to something else (data science-related or otherwise) to retain the curiosity, passion, and interest that brought you to the work in the first place. Consider working on a different project or even taking a break from a project to pursue another hobby or interest. That curiosity, passion, and interest for educational and social (and data science in education-related) topics is important to cultivate - perhaps as important as the capabilities that this book focused on. "]
]
