[
["index.html", "Data Science in Education Using R 1 Introduction: Data Science in Education–You’re Invited! 1.1 Learning Data Science in Education 1.2 Making the Path a Little Clearer", " Data Science in Education Using R Emily A. Bovee, Ryan A. Estrellado, Jesse Mostipak, Joshua M. Rosenberg, and Isabella C. Velásquez 1 Introduction: Data Science in Education–You’re Invited! Dear Data Scientists, Educators, and Data Scientists who are Educators: This book is a warm welcome and an invitation. If you’re a data scientist in education or an educator in data science, your role isn’t exactly straightforward. This book is our contribution to a growing movement to merge the paths of data analysis and education. We wrote this book to make your first step on that path a little clearer and a little less scarier. Whether you’re a data scientist using your skills in an education job or an educator who wants to learn data science skills, we invite you to read this book and put these techniques to work in the real world. We think that your work in the eduation community will help decide how education and data science come together going forward. 1.1 Learning Data Science in Education Over the coming chapters we’ll be learning together about what data science in education can look like. But to understand why we were compelled to write about the topic, we need to talk about why data science in education is not such a straightforward thing. Learning data science in education is challenging because there isn’t a universal vision for that role yet. Data science in education isn’t straightforward because the role itself is not straightforward. If education were a building, it would be multi-storied with many rooms. There are privately and publicly funded schools. There are more than eighteen possible grade levels. Students can learn alone or with others in a classroom. This imaginary building we call education also has rooms most residents never see: Business and finance staff plan the most efficient use of limited funds. The transportation department plans bus routes across vast spaces. University administrators search for the best way to measure career readiness. Education consultants study how students perform on course work and even how they feel about class materials. There are a lot of ways one could do data science in education, but building consensus on ways one should do data science in education is just getting started. The data science in education community is still working out how it all fits together. And for someone just getting started, it can all seem very overwhelming. Even if we did have perfectly clarity on the topic, there’s still the issue of helping education systems learn to leverage these new analytic tools. In many education settings, school administrators and their staff may have never had someone around [who understands education, knows how to code, and uses statistical techniques (Conway 2010) all at once. 1.2 Making the Path a Little Clearer As data science in education grows, the way we talk about and conceptualize it (Rosenberg et al. 2020) also needs to grow. We begin this book by offering a primer for data science in education, including a discussion of unique challenges and foundational skills in the programming language R. Next, you’ll take what you’ve learned and apply it in our data analysis in education walkthroughs. The walkthroughs in this book are our contribution towards a more example-driven approach to learning. They’re meant to make the ambiguous path of learning data science in education a little clearer by way of recognizable and actionable demonstrations. These examples fall into three different education data themes, with walkthroughs for each theme: Student perceptions of learning Walkthrough 1: The Education Dataset Science Pipeline Walkthrough 5: Text Analysis With Social Media Data Walkthrough 7: The Role (and Usefulness) of Multi-Level Models Analyze student performance data Walkthrough 2: Approaching Gradebook Data From a Data Science Perspective Walkthrough 8: Predicting Students’ Final Grades Using Machine Learning Methods Get value from publicly available data Walkthrough 3: Introduction to Aggregate Data Walkthrough 4: Longitudinal Analysis With Federal Students With Disabilities Data We’ll end the book by discussing how to bring data science skills into your education job. We hope after reading this book you’ll feel like you’re not alone in learning to do data science in education. We hope your experience with this book is challenging, but fun. And finally, we hope you’ll take what you learned and share it with others who are looking to start this journey. References "],
["c02.html", "2 How to Use this Book 2.1 Different Strokes for Different Data Scientists in Education 2.2 A Note on Statistics 2.3 What This Book Is Not About 2.4 Contributing to the Book", " 2 How to Use this Book We’ve heard it from fellow data scientists and experienced it many times - learning a programming language is hard. Learning a programming language, like learning a foreign language, is not just about mastering vocabulary. It’s also about learning norms, the language’s underlying structure, and the metaphors that hold the whole thing together. The beginning of the learning journey is particularly challenging because it feels slow. If you have experience as an educator or consultant, you already have efficient solutions you use in your day-to-day work. Introducing code to your workflow slows you down at first because you won’t be as fast as you are with your favorite spreadsheet software. But learning how to analyze data using R is like investing in your own personal infrastructure–it takes time while you’re building the initial skills, but the investment pays off when you start solving complex problems faster and at scale. One person we spoke with shared this story about their learning journey: “The first six months were hard. I knew how quickly I could do a pivot table in Excel. It took longer in R because I had to go through the syntax and take the book out. I forced myself to do it though. In the long-term, I’d be a better data scientist. I’m so glad I thought that way, but it was hard the first few months.” Our message is this: learning R for your education job is doable, challenging, and rewarding all at once. We wrote this book for you because we do this work every day. We’re not writing as education data science masters. We’re writing as people who learned R and data science after we chose education. And like you, learning to use R and data science to improve the lives of students is our daily practice. Join us in enjoying all that comes with that–both the challenge of learning and the joy of solving problems in creative ways. 2.1 Different Strokes for Different Data Scientists in Education It’s tough to define data science in education because people are educated in all kinds of settings and in all kinds of age groups. Education organizations require different roles to make it work, which creates different kinds of data science uses. A teacher’s approach to data analysis is different from an administrator’s or an operations manager. We also know that learning data science and R is not in the typical job description. Most readers of this book are educators working with data and looking to expand their tools. You might even be an educator who doesn’t work with data, but who’s discovered a love for learning about the lives of students through data. Either way, learning data science and R is probalby not in your job description. Does this describe your situation? You’ve got a full work schedule and challenging demands in the name of improving the student experience. Your busy workday doesn’t include regular professional development time or self-driven learning. You also have a life outside of work, including family, hobbies, and relaxation. We struggle with this ourselves, so we’ve designed this book to be used in lots of different ways. The important part in learning this materials is to establish a routine that allows you to engage and practice the content every day, even if for a few minutes at a time. That will make the content ever present in your mind and will help you shift your mindset so you start seeing even more opportunities for practice. We want all readers to have a rewarding experience and so we believe there should be different ways to use this book. Here are some of those ways: 2.1.1 Read the Book Cover to Cover (and How to Keep Going) We wrote this book assuming you’re at the start of your journey learning R and using data science in your education job. The book takes you from installing R to practicing more advanced data science skills like text analysis. If you’ve never written a line of R code, we welcome you to the community! We wrote this book for you. Consider reading the book cover to cover and doing all the analysis walkthroughs. Remember that you’ll get more from a few minutes of practice every day than you will from long hours of practice every once in awhile. Typing code everyday, even if it doesn’t always run, is a daily practice that invites learning and a-ha moments. We know how easy it is to avoid coding when it doesn’t feel successful, so we’ve designed the book to deliver frequent small wins to keep the momentum going. But even then, we all eventually hit a wall in our learning. When that happens, take a break and then come back and keep coding. When daily coding becomes a habit, so does the learning. If you get stuck in an advanced chapter and you need a break, try reviewing an earlier chapter. You’ll be surprised at how much you learn from reviewing old material with the benefit of new experience. That kind of back-to-basics attitude is sometimes what we need to get a fresh perspective on new challenges. 2.1.2 Pick a Chapter of Interest and Start There We interviewed R users in education as research for this book. We chose people with different levels of experience in R, the education field, and statistics. We asked each interviewee to rate their level of experience on a scale from 1 to 5, with 1 being no experience and 5 being very experienced. You can try this now–take a moment to rate your level of experience in: Using R Education as a field Statistics If you rated yourself as a 1 in Using R, we recommend reading the book from beginning to end as part of a daily practice. If you rated yourself higher than a 1, consider reviewing the table of contents and skimming across all the chapters first. If a particular chapters call to you, feel free to start your daily practice there. Eventually, we do hope you choose to experience the whole book even if you start somewhere in the middle. For example, you might be working through a specific use case in your education job–analyzing student quiz scores, evaluating a school program, introducing a data science technique to your teammates, or designing data dashboards for example. If this describes your situation, feel free to find a section in the book that inspires you or shows you techniques that apply to your project. # LEFT OFF December 2, 2019 2.1.3 Read Through the Walkthroughs and Run the Code If you’re experienced in data analysis using R, you may be interested in starting with the walkthroughs. Each walkthrough is designed to demonstrate basic analytic routines using datasets that look familiar to educators. In this approach, we suggest readers be intentional about what they want to learn from the walkthroughs. For example, readers may seek out examples of aggregated datasets, exploratory data analysis, the {ggplot2} package, or the gather() function. Read the walkthrough and run the code in your R console as you go. After you successfully run the code, experiment with the functions and techniques you learned by changing the code and seeing new results (or new error messages!). After running the code in the walkthroughs, reflect on how what you learned can be applied to the datasets, problems, and analytic routines in your education work. One last note on this approach to the book: we believe that doing data science in R is, at its heart, an endeavor aimed at improving the student experience. This endeavor involves complex problems and collaboration. Be sure to read other areas of the book that give context to why and how we do this work. Chapter Twelve in particular explores ways to introduce these skills to your education job and invite others into analytic activities. The skills taught in the walkthroughs are only one part of doing data science in education using R. 2.2 A Note on Statistics It’s been said that data science is the intersection between content expertise, programming, and statistics. You’ll want to grow all three of these as you learn more about using data science in your education job. Your education knowledge will lead you to the right problems, your statistics skills will bring rigor to your analysis, and your programming skills will scale your analysis to reach more people. What happens when we remove one of these pieces? Consider a data scientist working in education who is an expert programmer and statistician but has not learned about the real life conditions that generate education data. She might make analysis decisions that ignore the nuances in the data. Or consider a data scientist who is an expert statistician and an education veteran, but has not learned to code. She will find it difficult to scale her analysis and have the largest possible influence on improving the student experience. And finally, consider a data scientist who is an expert programmer and an education veteran. She can only scale surface level analysis and might miss chances to draw causal relationships or predict student outcomes. In this book we will spend a lot of time learning R by way of recognizable education data examples. But doing a deep dive into statistics and how to use them responsibly is better covered by books dedicated solely to the topic. It’s hard to understate how important this part of the learning is on the lives of students and educators. One education data scientist we spoke to said this about the difference between building a model for an online retailer and building a model in education: “It’s ok if I get shown 1000 brooms but if I got my model wrong and we close a school that will change someone else’s world.” We want this book to be your go-to R reference as you start integrating data science tools into your education job. Our aim is to help you learn R by teaching it in two contexts: data science techniques and workaday education datasets. We’ll demonstrate statistics techniques like hypothesis testing and model building and how to run these operations in R. But the explanations stop short of a complete discussion about the statistics themselves. We wrote within these boundaries because we believe that the technical and ethical use of statistics techniques deserve their own space. We hope that you’ll take a satisfying leap forward in your learning by successfully using R to run the models and experiencing the model interpretations in our walkthroughs. We encourage you to explore other excellent books like Navarro’s Learning Statistics With R (https://learningstatisticswithr.com/) as you learn the required nuances of applying statistical techniques to scenarios outside our walkthroughs. 2.3 What This Book Is Not About While we wrote Data Science in Education Using R to be a wide-ranging introduction to the topic, there is a great deal that this book is not about. Some of these topics are those that we would like to have been able to include but did not because they did not fit our intention (providing a solid foundation in doing data science in education). We chose to not include others because, frankly, excellent resources already exist. We detail some of what we had to not include in the book here. git/GitHub: Using git and GitHub are parts of the educational data scientists workflow; however, these can be a challenge to us (and are not needed to get started). Moreover, an outstanding introduction to their use exists in Bryan (2020) freely-available Happy git with R (https://happygitwithr.com/). Building R packages: If you are carrying out the same analyses many times, it may be helpful to create your own package, such as the roomba (for tidying complex, nested lists) and tidyLPA (for carrying out Latent Profile Analysis) packages that authors of this book created. However, building an R package is not the focus of this book, and Wickham has made the helpful - and freely-available - R packages (http://r-pkgs.had.co.nz/) book. Advanced statistical methodologies: As noted above, there are other excellent books for learning statistics; while we do discuss statistical methods (including some that can be considered to be advanced), this is not, primarily, a statistics text, and we especially do not consider this to be an advanced statistical methods book; one that we think is excellent as an advanced text from a machine learning perspective is James et al. (2013) Introduction to Statistical Learning. Creating a website (or book): While this book is created using the bookdown package (for creating a book through R), and many of us have created our personal websites using the blogdown R package (for cerating a website through R), this book does not describe how to do these; there are excellent, freely available books (see Xie, Thomas, and Hill (2019)‘s Blogdown (https://bookdown.org/yihui/blogdown/) and Xie (2019)’ Bookdown (https://bookdown.org/yihui/bookdown/)). 2.4 Contributing to the Book We designed this book to be useful and practical for our readers in education and by doing so almost certainly left out much. We did this to create a reference that is not intimidating to new users and indeed creates frequent small wins while learning to use R. But how do we expand the work as data science in education itself expands? We wrote this book in the open on GitHub so that community members can help us evolve the work. We hope that as the book evolves it grows to reflect the changing needs of data scientists in education. We want this to be the book new data scientists in education have with them as they grow their craft. To do that, it’s important to us that the stories and examples in the book are based on your stories and examples. And so we’ve built ways for you to share with us. Here’s how you can can contribute: Submit a pull request to our GitHub repository that describes a data science problem that is unique to the education setting Submit a pull request to share a solution for the problems discussed in the book to the education setting Share an anonymized dataset References "],
["c03.html", "3 What is a Data Scientist in Education? 3.1 Data Roles in Education 3.2 Common Activities of Data Scientists 3.3 Who We Are and What We Do", " 3 What is a Data Scientist in Education? You can think of a data scientist as someone who combines three skills to do data analysis: programming, statistics, and content knowledge (Conway 2010). Though if you Google “what is a data scientist,” you won’t find a simple answer. If we substitute “education experience” for “content knowledge”, we start to imagine what skills a data scientist in education has. Even then, there’s still a lot more to talk about when we try to describe what someone in this role does on a day-to-day basis. Not having a consistent definition of what a data scientist in education does makes it hard to share with others in specific terms. In this chapter we’ll try and define what being a data scientist in education means by sharing some of the roles folks occupy in this line of work. We’ll also share some of the common day-to-day activities a data scientist in education does. 3.1 Data Roles in Education We learned from talking with data scientists in the education field that their roles and specializations can be very different from each other. Hearing about all the different ways that people contribute to education organizations was inspiring and eye opening. It inspired hope and excitement that people have niche skills and passions can find ways to add value to their organization’s data culture. Here are some of the roles and specializations data scientists in education might take on. 3.1.1 Building Systems That Get Data to the Right People School staff and leadership can’t make data-informed decisions unless they have good data. Data scientists in education who specialize in data engineering and data warehousing build systems that organize data in one place, keep the data secure to protect the information of students and staff, and distribute datasets to the folks who need it. In this area of data science, you might also find folks who specialize in data governance, which are policies used to keep data collection, documentation, security, and communication to a high standard. 3.1.2 Measuring the Impact of Our Work on the Student Experience Education systems that continually strive to improve the way they serve students value the scientific evaluation of their work’s impact. Measuring the impact of instructional interventions is so important for allocating time, money, and attention to ways to improve education systems. Data scientists who specialize in measuring impact know how to use statistical techniques to isolate the effect of an intervention and estimate its value. For example, an education system may choose to work with their data analysts to quantify gains in student attendance that result from a new intervention aimed at chronic absenteeism. 3.1.3 Looking for Patterns in Student Data Now more than ever, students and school staff are generating data as they go about their day learning and teaching. Online quizzes generate quiz data. Student systems collect attendance, discipline, behavior, and native language data. Online individualized education program (IEP) systems house information about students with disabilities. Statewide testing assessments are scored, stored in a database, and reported back to families. Much of this data gets reported to the state education agency (SEA) for processing and publishing online as a part of an accountability system. School systems that learn to explore this data as its generated get a lot of value from it. The patterns they identify inform where they put their time, money, and attention. Data analysts are experts at systematically exploring these data and finding useful ways to compare data across different categories. This technique, called exploratory data analysis, is used to generate plausible hypotheses about relationships between variables in the data. These hypotheses can be tested and become the material educational organizations use to learn about what’s happening with their students. For example, one way for school systems to strive for equity in student outcomes is to constantly explore any differences in outcomes among student subgroups. 3.1.4 Improving How We Use Statistical Models in Education When we spoke with data scientists in education during the research phase of this book, we learned about the way they use tried and true methods for analysis in schools. But we also learned that they are innovators. They take techniques that are commonly found in other industries like business, and explore how these techniques can improve the state of analytics in education. In particular, the data scientists that we spoke with talked about going beyond exploratory data analysis and introducing more advanced techniques like inferential statistics and predictive modeling to the data culture of schools. This work is not only about improving how well we do our current practices, but also exploring our curiosities about how we might apply new techniques to the task of improving the learning experience of our students. 3.2 Common Activities of Data Scientists Now let’s explore what a data scientist in education can expect to do on a typical day at the office. In this section we’ll preview some common activities data scientists do daily. Later in the book we’ll learn and practice these techniques and more. 3.2.1 Processing Data Processing data, or cleaning data, is the act of taking data in its raw form and preparing it for analysis. When you start a data analysis, the data you have is in the same state it was in when it was generated and stored. It very often isn’t designed to support the specific analysis that that you’re tasked with doing. Here are some examples of common things you’ll need to do to prepare your data: The variable names have to be reworked so they’re convenient to reference in your code. It’s common for raw datasets to have generic variable names that don’t describe the values in that dataset’s column. These variable names should be changed into something that intuitively represents the values in that column. There are also format-related problems with variables: things like spaces between words, lengthy variable names, or symbols in the variables names can cause inconvenience in your code or make it hard to keep track of the steps in a complicated analysis. Datasets also have to be filtered to the subset that you’re interested in analyzing. It’s possible that the dataset you’re given contains a larger group of students than you need for your project. For example, a principal at a school site may give you a dataset of every student and the number of days they’ve missed this school year. Now imagine she asks you to do an analysis of attendance patterns in first, second, and third graders. One step you’ll need to take before you start the analysis is filtering the dataset so that it only contains first, second, and third graders. Sometimes you’ll need to do computations to get the summary figures your stakeholders are asking for. Imagine that the director of curriculum and instruction asks you to report the percentage of students that have scored in the proficient range on a statewide assessment. Now imagine that the datasets you’re given is a list of students, the school they attend, and their test score. To produce the requested report, you’ll need to convert the number of students who scored above a threshold on the test to the percentage of students who met that threshold at each school. 3.2.2 Doing Analysis This is the part of our workflow that most people associate with data science. Analysis is when you apply statistics techniques to identify the underlying structure of the dataset. This means that you are making educated guesses about the real life conditions that generated the dataset. We realize this may be the first time you’ve heard data analysis described this way. We choose to describe it this way because in the end, data analysis in education is about understanding what the data tells us about the student experience. If we can understand the underlying structure of a dataset, we can improve our understanding of the students who’s actions generated the numbers. Let’s look at a concrete examples of this. Imagine that you are an education consultant and your client is a school district superintendent. The superintendent has asked you to evaluate the impact of a new teacher coaching initiative the school district has been using for a year. After preparing a dataset that contains teachers, the number of hours they’ve spent in coaching sessions, and the change in quiz scores, you set out to explore and fit a statistical model. Your initial visualization of the dataset suggests there might be a linear relationship between the number of hours spent in coaching and a positive change in quiz scores. Using a statistical model to analyze this dataset is one way of understanding the underlying structure of the dataset. Specifically, a statistical model can help estimate how much the hours a teacher spent in coaching sessions influences the change in test scores, and how much some other factor (even random chance!) accounts for the change. Data scientists in education are fundamentally interested in the people who generated the numbers. In this case, that’s the students who took the quizzes and the teachers who participated in the coaching sessions. If you can analyze the dataset and understand its underlying structure, you might learn more about what’s happening with the student experience. 3.2.3 Sharing Results One lesson we’ve learned from our daily practice is that each step in a data analysis is its own step with its own audience. When that audience changes, the way you create output must also change. So far we’ve discussed processing data and analyzing data. At these stages, the audiences for your output are usually you, other data scientists, or stakeholders who are in a position to give feedback about the process so far. But when you’ve sorted through your findings and have selected conclusions you want to share, the audience shifts to a wider group of people. Now you’re tasked with communicating with leadership, staff, parents, the community, or some combination. Your thought process and techniques for sharing with a wider audience are different from the ones you use when processing and analyzing data. This third activity includes developing visualizations that clearly communicate a finding, writing narratives that give context and story to your analysis, and developing presentations that spark conversations about the student experience. 3.3 Who We Are and What We Do We’ve talked about the roles of data scientists in education and the daily tasks they can be expected to do. Now let’s talk about how who data scientists in education are. In some fields the relationship between what you do today and how you got there is prescribed. If you want to help people by performing cardiac surgery, you have to go to medical school first. If you want to hear trials in court, you have to go to law school first. To prepare for this book, we talked to lots of folks who do data analysis in the education field. We found that there’s quite a bit of variety in how people work with data in education. There’s also variety in the journey that people took to arrive at their education data science role. We see this as good news for folks who want to start working with data in education in a more formalized way. You don’t need a Ph.D. to do this kind of work, though many people we talked to did. You don’t need to be an expert in statistical modeling, though many people we talked to were. We talked to people who are consultants that came to the education field. We also talked to teachers and administrators who became consultants. We talked to folks who are the lone data scientist in their education organizations and we talked to folks who are part of an analytics team. Even our own paths toward doing data science in education are very different. Here’s a little about us and how we practice data science: Leading Office Culture Toward a Data-Driven Approach Jesse, a director at an education non-profit in Texas, is setting up a database to house student achievement data. This project requires a number of data science skills we’ll discuss in chapter five, including cleaning data into a consistent format. Once the data is prepared, Jesse builds dashboards to help her teammates explore the data. But not all of Jesse’s work can be found in a how-to manual for data scientists. She manages a team and serves as the de facto project manager for IT initiatives. And given her expertise and experience in data science, she’s leading the charge towards a more data-driven approach within the organization. Helping School Districts Plan to Meet Their Goals Ryan, a special education administrator in California, uses data science to reproduce the state department of education’s special education compliance metrics, then uses the results to build an early warning system for compliance based on local datasets. In this case, Ryan uses foundational data science skills like data cleaning, visualization, and modeling to help school districts monitor and meet their compliance requirements. Doing and Empowering Research On Data Scientists in Education Joshua, Assistant Professor of STEM Education at University of Tennessee in Knoxville, researches how students do data science and helps teachers teach the next generation of data-informed citizens. He makes this work possible by building R packages - self-contained groups of data tools - that he and other researchers use to analyze datasets efficiently. Supporting Student Success with Data Emily, a dental education administrator in Wisconsin, guides faculty members on best practices in assessing student learning. Like Jesse, Emily works on merging multiple data sources together to get a better understanding of the educational experience. For example, she merges practice national board exam scores with actual national board performance data. Later, Emily conducts statistical analyses to help identify a practice national board score threshold at which students are ready to sign up for the real national board exam. All this is possible because of R! Placing Schools and Districts in Context Isabella, a data analyst at a large philanthropy, uses publicly available aggregate data to understand what schools and districts look like, how they’ve changed over time, and other contextual information needed to better understand the field of education. These datasets are often in messy formats (or, PDFs!), and sometimes data from the same agency are organized in a slightly different way every year. Using R allows the downloading and cleaning process to be reproducible when new data comes in. The code clearly shows the decision rules made to make the aggregate data useful in models or visualizations. Packages and projects allow the entire process to be shared and reused across the analytics team. These are examples of how one might apply statistics and programming to create new knowledge in the education field. But that’s as far as we can go when looking for commonalities in their day-to-day work. We hope this book is part of a movement to develop common norms and expectations for how it all works together as the relationship between data science and education grows. Because this relationship is still young, it is important that the people growing data science in education understand the culture and unique challenges in their education job. After all, the defining feature that will differentiate data science in education from data science in general will be doing data science that meets the unique needs of students, staff, and administration in education. As you progress through this book, we hope you begin to understand where your particular data interests and passions are. There is more variety in educational backgrounds and in the daily work of education data analysis than one might think. You can bring your own unique background and interests to this field. We hope this book will help you combine your unique experiences with some new learning so you can create a practice that improves the education experience of students using your unique gifts. References "],
["c04.html", "4 Special Considerations 4.1 Things to Consider when Doing Data Science in any Domain 4.2 Things to Consider when Doing Data Science in Education 4.3 Conclusion", " 4 Special Considerations Data science in education is a new domain. It presents opportunities, like those discussed in the previous chapter, but also some challenges. These challenges vary a lot: we consider doing data science in education to include not only access, processing, and modeling data, but also social and cultural factors, like the training and support that educational data scientists have available to them. These considerations range from the very general (and common to all domains in which data science is carried out) to very particular to the field of education. Because data science in educational settings is a relatively new phenomenon, it’s understandable that school staff may be wary of how data is collected and analyzed. It’s common for school staff to question how data is used, particularly if the data is used to describe staff and student performance. One of the biggest challenges that can arise is if individuals begin to feel that they are being evaluated by metrics that they feel are unclear or even unfair. Usually, “data-driven” efforts mean different things to administrators as compared to educators. To an administrator, a data-driven effort might be an endeavor to better understand the strengths and weaknesses of pre-existing systems, with an eye to eventually proposing new systems that are more efficient. To an educator, a data-driven effort might feel like an approach that masks the individuality of students by reducing them to numbers. Perhaps neither perspective is exactly correct. Whereas maximizing efficiency and preserving students’ individual needs should certainly be goals of educators and educational administrators, data science is a versatile tool that can be leveraged to help answer a variety of meaningful questions. This chapter will present subjects to consider when adopting data science in educational contexts. 4.1 Things to Consider when Doing Data Science in any Domain 4.1.1 A New Field One consideration for educational data scientists is common to data scientists in other domains: combining content knowledge, programming, and statistics to solve problems is a fairly new idea. In particular, the amount of data now available means that programming is often not only helpful, but necessary, for stakeholders to effectively use data. Programming is powerful but challenging, and many of us in education do not have prior experience with it. Despite this challenge and the difficulty of writing the first few lines of code, we know that those of us without prior programming experience can learn. It is not necessary to have a computer science background or even any formal or informal training related to coding. The great thing about entering a field as flexible as data science is that you are joining a vast crowd of individuals who are self-taught. You will find that there is a very supportive online community to help as you learn through this book. 4.1.2 Addressing Ambiguity: A Reproducible Approach One way to address concerns of educators feeling wary of ambiguous data processes is to build analytic processes that are open about what data is collected, how it is collected, how it is analyzed, and how it is considered alongside other data when used in decision-making conversations. This can be achieved through a number of activities, including regular conversations about analytic methods, written reports describing data collection, and receiving input about analytic goals from staff members. One such process for achieving openness in data collection and analysis is called reproducible research. The concept of reproducible work (Wikipedia 2020) is the idea that a completed analysis should come with all the necessary materials, including a description of methodology and programming code, needed for someone else to run the analysis and achieve the same results. If school staff are apprehensive about how school data is collected and used, it follows that a more transparent method for using data could go some way towards putting school staff - the consumers of school data - at ease. An additional benefit of a reproducible approach is that it can ease in transition periods. If a data-science-in-school advocate leaves their original position, ideally, they would leave behind not just descriptions of the analyses that they did, but also the specific files needed to run the analyses again. The new individual who takes their place will be able to seamlessly transition into the new role. If asked to run “the same report I always got from your predecessor,” the new person will understand immediately what files were needed to create that original report and will be able to request all necessary data to generate a new version of the report. To implement a reproducible approach in your organization, you can start by keeping all files related to each project you do in their own folders. As you create reports from the data, keeping notes in the files will help you to easily generate similar reports in the future. Many educators find that even though changing administration might mean changing requests, having careful documentation of past processes allows for more efficiency in the way they use data to answer those requests. 4.2 Things to Consider when Doing Data Science in Education 4.2.1 Addressing organizational resistance: A self-driven analytic approach One consideration when adopting data science strategies in educational contexts is that in some environments, there is a lack of precedent - it is not common, for example, for a teacher to be conducting regression analyses on data. However, it’s not necessary to wait for a district-wide or state-wide initiative to begin to implement the techniques you will learn in this book. An organization should encourage their staff to do their own data analyses primarily for the purpose of testing their own hypotheses. In a school, for example, a teacher might wonder about student learning in their classroom and might want to utilize data to directly guide decisions about how they deliver instruction. There are at least two benefits to this approach. First, staff begin to realize the value of doing data analysis as an ongoing inquiry into their outcomes, instead of a special event once a year ahead of school board presentations. Second - and more important for the idea of reducing apprehension around data analysis in schools - school staff begin to demystify data analysis as a process. When school staff collect and analyze their own data, they know exactly how it is collected and exactly how it is analyzed. The long-term effect of this self-driven analytic approach might be more openness to analysis, whether it is self-driven or conducted by the school district. Building and establishing a data governance system that advocates for an open and transparent analytic process is difficult and long-term work, but the likely result will be less apprehension about how data is used and more channels for school staff to participate in the analysis. Here are more practical steps a school district can take towards building a more open approach to analysis: Make technical write-ups of data analyses available so interested parties can learn more about how data was collected and analyzed Make datasets available to staff within the organization, to the extent that privacy laws and policies allow Establish an expectation that analysts present their work in a way that is accessible to many levels of data experience Hold regular forums to discuss how the organization collects and uses data By adopting a self-driven analytic approach, individuals can help their educational organization to embrace the potential of utilizing data to anticipate and possibly forestall problems in the future. 4.2.2 Lack of Processes and Guidelines One challenge is the ambiguity around the process and practice of doing data science in the educational context specifically. While there is a body of past research on students’ work with data (see Lee &amp; Wilkerson, 2018, for a review), there is limited information from case- or design-based research on how others in education –teachers, administrators, and data scientists– use data in their work. In other words, we do not have a good idea of what best practices in our field are. This challenge is reflected in part in the variability in the roles of those who work with data. Many districts employ data analysts and research associates; some are now advertising and hiring for data scientist positions. Education is a field that is rich with data: survey, assessment, written, and policy and evaluation data, just for a few examples. Nevertheless, sometimes, there is a lack of processes and procedures in place for school districts and those working in them to share data with each other in order to build knowledge and context. In academic and research settings, there are not often structures in place to facilitate the analysis of data and sharing of results. This means that there can be silos of information - one group could do a survey, and another group could not hear about the results or even hear about the fact that the survey happened. The good news about this is that many educational organizations are curious and passionate about supporting student success, so it is likely that even if many separate data collection efforts are being implemented (rather than one unified strategy), you will not be dealing with the problem of “I don’t have enough data to work with.” As a pioneer for data science in your organization, you can help to clarify these redundant processes and can offer your skills to help make sense of the wealth of information already being gathered. 4.2.3 Limited Training and Educational Opportunities Educational data science is new. At the present time, there are limited opportunities for those working in education to build their capabilities in educational data science (though this is changing to an extent; see Anderson and colleagues’ work to create an educational data science certificate program at the University of Oregon and Bakers’ educational data mining Massive Open Online Course offered through Coursera). Many educational data scientists have been trained in fields other than statistics, business analytics, or research. Moreover, the training in terms of particular tools and approaches that educational data scientists utilize are highly varied. However, we believe this diversity of training creates unique opportunities for the development of data science in education as a field. Indeed, educators’ diverse training and backgrounds positions them to tackle educational challenges creatively. 4.2.4 Advancing Equity We can use data science to inform decisions that reduce inequities that exist in the education system. However, data science can also be a tool that exacerbates marginalization of students we want to serve. For a data scientist in education, it is crucial that before beginning an analysis, we fully understand how we/our organization defines equity and our equity goals and how we will continuously check against our biases. After defining equity and our equity goals, we can then work to ensure our data science lifecycle reflects what we are trying to learn so we can meet our outcomes. If our organization hopes to decrease the opportunity gap between students affected by poverty and students not affected by poverty, then it is important that (1) we define what ‘affected by poverty’ means, (2) what project design will help us understand if we’re moving towards our goals, and (3) whether our data collection allows us to disaggregate these demographics (see Walkthrough 03). We must then make sure our analyses take these disaggregations into account. By getting in front of our project design and data collection, we increase our data’s utility to actually move us toward our equity goals. The report should clearly detail whether we are moving toward our equity goal(s). Our analysis can highlight any potential blind spots that we are missing, as all data is biased and can only ever tell a partial story. When reporting on equity measures, it is prevalent in education to use deficit-based framing. We suggest using asset-based framing to empower the students, people, and communities we are hoping to serve in our work. 4.2.5 The Complex and Messy Nature of Education Data Another consideration concerns the complex nature of education data. Education data are often hierarchical, in that data at multiple “levels” is collected. These levels include classrooms, schools, districts, states, and countries - quite the hierarchy! In addition to the hierarchical nature of education data, by their nature, these data often require linking with other data, such as data that provides context at each of the aforementioned levels. For example, when data is collected on students at the school level, it is often important to know about the training of the teachers in the school; data at the district level needs to be interpreted in the context of the funding provided by the community in terms of per-pupil spending. A final aspect concerns the type of data collected. Often, education data is numeric, but just as often, it is not. Education data involves characteristics of students, teachers, and other individuals that are categorical (a descriptive type of variable with multiple levels for which a the levels do not signify quantity but instead signify groups, such as sex or grade level), open-ended responses that are strings (a type of variable used to store text), or even recordings that consist of audio and video data. All of these present challenges to the educational data scientist. As with the diversity of training for educational data scientists, though, the complexity of educational data also presents opportunities for educators to creatively approach their tasks. If you are faced with a large and complicated dataset, you might begin by asking yourself what you are curious about and carving out just a couple variables that you can use to answer your question. Your colleague might be interested in an entirely different question, and might consider different variables from the same dataset in their analysis. The complexity of education data need not discourage educators from pursuing their interests. 4.2.6 Ethical and Legal Concerns Related to the complex and messy nature of education data is its confidential nature. At the K-12 level, most data requires safeguards because youth are a protected population. A closely related issue concerns the aims of education within predetermined constraints. Those working in education often seek to improve it and often work to do so with a scarcity of school and community resources. These ethical, legal, and even values-related concerns may become amplified as the role of data in education increases. They should be carefully considered and emphasized from the outset by those involved in educational data science. If you feel resistance in your organization as you begin to adopt the principles you learn in this book, you might begin by offering to analyze “de-identified” or “anonymous” data. In this way, you could show your administration what is possible and foster additional buy-in further down the road. 4.2.7 Analytic Considerations Due to the particular nature of education data, analyzing education data can be difficult, too. The data is often not ready to be used: It may be in a format that is difficult to open without specialized software or it may need to be “cleaned” before it is usable. In data science, “cleaning” data refers to reorganizing or restructuring the dataset to make it easier to analyze. This process would be analogous to if you received an Excel spreadsheet but the columns were in an order that didn’t make sense to you, or one column was duplicated in the spreadsheet. The process you’d go through to reorganize the data to make it logical is data cleaning. Closely related to the ethical and legal challenges discussed above, educational data scientists should be conscious of potential racial and gender biases in school models, and challenge not reinforce them. Because of the different types of data, the educational data scientist must often use a variety of analytic approaches, such as multi-level models, models for longitudinal data, or even models and analytic approaches for text data. In later chapters of this book, you will learn more specifics about building models. 4.3 Conclusion While there are many challenges to working with education data, there are many opportunities as well. Once they unlock the power of data science to reveal insights about their organizational context (their students, their teaching, etc.), many educators will become more interested in gathering more data and continuing on this path. Data science becomes a useful tool to help connect with the purpose of your job. Once you begin to rely on data science, it can be hard to stop! As an educational professional, remember that you are more closely acquainted with your context than any outside analyst could ever be. This affords you the unique opportunity to become the data and analysis guru in your area. In summary, educators that want to evolve their data analysis processes into something practical and meaningful to student progress will need to address some unique challenges in order to help all stakeholders to understand the benefits of the questions being answered with data. That hard work will pay off. References "],
["c05.html", "5 Getting Started with R and RStudio 5.1 Chapter overview 5.2 Getting Started 5.3 Downloading R and RStudio 5.4 Getting to know R through RStudio 5.5 Diving a little deeper into R with swirl 5.6 Introduction to Help Documentation 5.7 Downloading and accessing the data sets used in this book", " 5 Getting Started with R and RStudio 5.1 Chapter overview This chapter is designed to take you from installing R and RStudio all the way through the very basics of data loading and manipulation using the tidyverse (???). We will be covering the following topics in this chapter: Installing R and RStudio RStudio environment and pane layout Basics of customizing your RStudio environment Introduction to help documentation Steps for working through new and unamiliar content Downloading and accessing the data sets used in this book 5.2 Getting Started First, you will need to download the latest versions of R and RStudio. R is a free environment for statistical computing and graphics using the programming language R. RStudio is a set of integrated tools that allows for a more user-friendly experience for using R. Although you will likely use RStudio as your main console and editor, you must first install R, as RStudio uses R behind-the-scenes. Both R and RStudio are freely-available, cross-platform, and open-source. 5.3 Downloading R and RStudio 5.3.1 To download R: Visit this page to download R: https://cran.r-project.org/ Find your operating system (Mac, Windows, or Linux) Download the ‘latest release’ on the page for your operating system and download and install the application Don’t worry; you will not mess anything up if you download (or even install!) the wrong file. Once you’ve installed R, you can get started. 5.3.2 To download RStudio: Visit this page to download RStudio: https://www.rstudio.com/products/rstudio/download/ Under the column called “RStudio Desktop FREE”, click Download Find your operating system (Mac, Windows, or Linux) Download the ‘latest release’ on the page for your operating system and download and install the application If you do have issues, consider this page, and then reach out for help. Another excellent place to get help is the RStudio Community. 5.4 Getting to know R through RStudio Now that we’ve installed both R and RStudio, we will be accessing R through RStudio. One of the most reliable ways to tell if you’re opening R or RStudio is to look at the icons: [IMG] RStudio is an Integrated Development Environment (IDE), and comes with built-in features that make using R a little easier. If you’d like more information on the difference between R and RStudio, we recommend the Getting Started section of the Modern Dive textbook. You do not have to use RStudio to access R, and many people don’t! Other IDEs that work with R include: - Jupyter notebook - VisualStudio - VIM - IntelliJ IDEA - EMACS Speaks Statistics (ESS) This is a non-exhaustive list, and most of these options require a good deal of familiarity with a given IDE. However we bring up alternative IDEs – particularly ESS – because RStudio, as of this writing, is not fully accessible for learners who utilize screen readers. We have chosen to use RStudio in this text in order to standardize the experience, but encourage you to choose the IDE that best suits your needs! When we open RStudio for the first time, we’re likely to see this: These three “panes” are referred to as the console pane, the environment pane, and the files pane. The large square on the left is the console, the pane in the top right is the environment pane, and the square in the bottom right is the files pane. When we create a new file, such as an R script, an R Markdown file, or a Shiny app, RStudio will open a fourth pane, known as the source pane. You can try this out by going to File -&gt; New File -&gt; R Script. When we type out code, we do so in either the console or source pane. It is generally better to type code in an R script, which saves as an .R file, than to type your code in the console. This is because anything you type in the console will be lost as soon as you close R, whereas you can save everything in an .R script and see/use it again later. Running code in an R Script There are several ways to run code in an R script: Highlight the line(s) of code you’d like to run and press Ctrl+Enter Highlight the line(s) of code you’d like to run and click the Run button in the R script pane To run every line of code in your file you can press Ctrl+Shift+Enter 5.4.1 Getting to Know R: Challenge Questions Script vs. Console Use the console as a calculator Create an .R script (File -&gt; New File -&gt; R script) and use it as a calculator What are the similarities and differences between coding in the console versus coding in an .R script? Assignment operator What is the assignment operator, and how does it work? Commenting code When might we want to comment out code? How do we comment out code in an .R script? What are two ways to comment code? Exploring the workspace For each of the challenges below, you’ll need to go to Tools -&gt; Global Options Explore the various themes available to you in RStudio by going to Tools -&gt; Global Options -&gt; Appearance Choose a theme that works best for you and apply it Change some of the default options in RStudio by going to Tools -&gt; Global Options -&gt; General and ensure that the following options are selected/deselected: Lastly we’ll add in our margin indicator by going to Tools -&gt; Global Options -&gt; Code and then selecting/deselecting the following: 5.5 Diving a little deeper into R with swirl If you’re eager to get started exploring everything that R can do, we recommend installing and learning through swirl. swirl is set of packages (more on those shortly!) that you can download, providing an interactive method for learning R by using R. We are not affiliated with swirl in any way, nor is it required to progress through this text. 5.6 Introduction to Help Documentation Very few - if any - people in the world know everything there is to know about R. This means that we all need to look things up, sometimes every few minutes! Thankfully there are some excellent built-in resources that we can leverage as we use R. From within RStudio we can access the Help documentation by using ? or ?? in the console. For example, if I wanted to look up information on the data() function, I can type ?data or data() next to the carat &gt; in the Console and hit Enter. You should see the Help panel on the bottom right side of your RStudio environment populate with documentation on the data() function. This works because the data() function is part of something called base R - that is, all of the functions included with R when you first install it. As you use R throughout this book, we’ll be asking you to install additional packages. These packages extend the functionality of base R by providing us with access to new functions. This means that instead of writing a function to do a common data analysis task, such as creating a new variable out of existing variables, someone has written that function and made it available for you to use at no charge. 5.7 Downloading and accessing the data sets used in this book "],
["c06.html", "6 Foundational Skills 6.1 Chapter overview 6.2 Foundational Skills Framework 6.3 Setting up your Project 6.4 Code for Foundational Skills 6.5 Packages and Functions 6.6 Installing and loading a package 6.7 Using functions to import data 6.8 Datasets and variables 6.9 The pipe operator 6.10 Finicky functions (rename this section) 6.11 SECTION ON c() and %in% 6.12 Creating objects 6.13 Creating Projects 6.14 Include at end of chapter! 6.15 Loading Data from Excel, SAV, and Google Sheets 6.16 Processing Data 6.17 Communicating / Sharing Results 6.18 Old text - keep until reincorporated//readdressed 6.19 May not need", " 6 Foundational Skills 6.1 Chapter overview This chapter is designed to give you the skills and knowledge necessary to get started in any of the walk through chapters. Our goal is to get you working with R using the RStudio Integrated Development Environment (IDE) through a series of applied examples. If you have not yet installed R and/or RStudio, please go through the steps outlined in Chapter 05A before beginning this chapter. Please note that this chapter is not intended to be a full and complete introduction to programming with R, nor for using R for data science. There are many excellent resources available which provide this kind of instruction, and we’ve listed them for you on page [AAA] in the Resources section [link for bookdown version]. 6.2 Foundational Skills Framework No two data science projects are the same, and rather than be overly prescriptive, this chapter errs on the side of creating a general framework for you to use as a home base as you work through this text. The four basic concepts we will use to build our framework are: Data Projects Packages Functions You’re likely using this text because you have some data that you’d like to do something with, and you’d like to try doing said thing using R. The framework we’ll use assumes that your data exists externally from R in a .csv file, and needs to be brought into R so that we can work with it. There are a multitude of file types that data can be stored in. We’ve provided additional resources for loading data from Excel, SAV, and Google Sheets at the end of this chapter. While it is possible to connect directly to a database from within R, we do not cover those skills in this text. For those curious as to how to accomplish this, we recommend the following resources [AAA]. [We’ll say more about each component later in this chapter, but for now let’s break down this image: [INSERT ILLUSTRATION]. [Illustration of framework] We have data that we bring into a Project within RStudio. RStudio is the interface that we use to access and manipulate R. Sometimes you’ll hear RStudio referred to as an IDE, or “Interactive Development Environment.” We use an IDE - in this case RStudio, although you could use others - because it adds features that make our analytical lives a little (and sometimes a lot!) easier. You can read more about everything that RStudio offers here (I couldn’t find a great resource on this - does anyone have one? Might be worth creating a blog post on it!) Within RStudio we set up a Project. This is the home for all of the files, images, reports, and code that we’ll create for a single project. While there are a myriad of ways to set up the files in your Project, the method we’ll use in this text is: [IMAGE with src, data (sub: raw, for use), results, images] It’s OK (and totally normal) for your initial projects to consist of only one or two files. As your skill set grows and develops you’ll need more and more files, and laying the groundwork for a file structure now will make future you very happy. We use Projects because they create a self-contained file for a given analysis in R. This means that if you want to share your Project with a colleague they will not have to reset file paths (or even know anything about file paths!) in order to re-run your analysis. And even if the only person you ever collaborate with is a future version of yourself, using a Project for each of your analyses will mean that you can move the Project folder around on your computer, or even move it to a new computer, and remain confident that the analysis will run in the future, at least in terms of file path structures. You may run into issues with packages being out of date and functions being deprecated - in which case you may be interested in learning more about how to set up a Project within Docker [AAA]. 6.3 Setting up your Project note: how in-depth into file structures do we want to go? How do I create a Project? Creating a Project is one of the first steps in working on an R-based data science project in RStudio (if we were using GitHub for this we’d absolutely recommend setting up your repository first!) To create a Project you will need to be in RStudio. From within RStudio, follow these steps: Click on File Select New Project Choose New Directory Click on New Project Enter your Project’s name in the box that says “Directory name” Choose where to save your Project by clicking on “Browse” next to the box labeled “Create project as a subdirectory of:” Click “Create Project” If you are looking for more resources, including information on using a git repository, whether or not you should open in a new session, etc., please see AAA and BBB resources. 6.3.1 Coding in RStudio – import from 05A ## Packages “Packages” are shareable collections of R code that provide functions (i.e., a command to perform a specific task), data and documentation,. Packages increase the functionality of R by improving and expanding on base R (basic R functions). 6.3.2 Installing and Loading Packages To download a package, you must call install.packages(): install.packages(&quot;dplyr&quot;, repos = &quot;http://cran.us.r-project.org&quot;) You can also navigate to the Packages pane, and then click “Install”, which will work the same as the line of code above. This is a way to install a package using code or part of the RStudio interface. Usually, writing code is a bit quicker, but using the interface can be very useful and complimentary to use of code. After the package is installed, it must be loaded into your RStudio session using library(): library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union We only have to install a package once, but to use it, we have to load it each time we start a new R session. A package is a like a book, a library is like a library; you use library() to check a package out of the library. - Hadley Wickham, Chief Scientist, RStudio 6.3.3 Running Functions from Packages Once you have loaded the package in your session, you can run the functions that are contained within that package. To find a list of all those functions, you can run this in the RStudio console: help(package = dplyr) The documentation should tell you what the function does, what arguments (i.e., details) needed for it to successfully run, examples, and what the output should look like. If you know the specific function that you want to look up, you can run this in the RStudio console: ??dplyr::filter Once you know what you want to do with the function, you can run it in your code: dat &lt;- # example data frame data.frame(stringsAsFactors=FALSE, letter = c(&quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;), number = c(1L, 2L, 3L, 4L, 5L)) dat ## letter number ## 1 A 1 ## 2 A 2 ## 3 A 3 ## 4 B 4 ## 5 B 5 filter(dat, letter == &quot;A&quot;) # using dplyr::filter ## letter number ## 1 A 1 ## 2 A 2 ## 3 A 3 – end import Import your data Talk about previous chapter - which goes into different data sources. Why are we skimming over it here? Should we instead focus on data types and structures? R comes with built in data sets, the list of which you can see by running data() in the Console (go ahead, try it! You can do this by typing data() in the Console window and hitting Enter.) You may be familiar with some of the data sets, such as mtcars or iris, which often get used in vignettes (link out//define) and other various examples. Using built-in data sets are a great way to work with R code using a data set that “just works.” Think of any data science project as consisting of data and code. We use code to manipulate data, which means that when we are troubleshooting we need to think about whether or not our code is the source of the error, if our data is the source of the error, or if the error comes from the interaction of the code with the data. For this chapter we’ll be using data from the Massachusetts Public School System that was originally downloaded from Kaggle on October 23, 2019. Access the data by ???. Save your data to the data --&gt; raw folder, make a copy of your data, and put the new copy into the data --&gt; for_use folder. When working with downloaded data (in other words, data files, as opposed to pulling your data directly into R through an API, database connection, or webscraping) it’s good practice to keep the initial copy of your data in a folder and to never open that data, especially if that data is in an .xls* or .csv format! Excel has an overly aggressive way of trying to be helpful with your data, and when you open a file you may find that your numeric values have been converted to dates. At best you need to re-download the file, and at worst you’ve made a significant amount of work for yourself. 6.4 Code for Foundational Skills The following code comprises all of the code that we’ll use in this chapter. We present it here in its entirety, and then break it down into chunks in order to explain what’s happening. This code may resemble the start of an analytical workflow, but does not walk you through a complete analysis. Instead, the code chosen below is used to highlight key features and foundational skills that comprise the interaction of packages and functions with data. We recommend the text R for Data Science for those looking for a more in-depth look into completing an analytical workflow in R. We do not recommend running the following block of code all at once! It intentionally contains errors that will prevent it from running in its entirety. Instead we recommend creating a new .R script and adding chunks of code as we discuss them in the book, running each chunk of code as we go along in order to better see what is happening in a given line of code. (define a code chunk) 6.4.1 See Ryan’s work on prefacing functions - need to preface dplyr::arrange()! # Installing packages # install.packages(&quot;tidyverse&quot;) # install.packages(&quot;janitor&quot;) # install.packages(&quot;skimr&quot;) # install.packages(&quot;here&quot;) # Setting up your environment library(tidyverse) library(janitor) library(skimr) library(here) # Importing data read_csv(here(&quot;zz_jesse_practice_scripts/data/for_use&quot;, &quot;MA_Public_Schools_2017.csv&quot;)) # change path once finalized in book read_csv(here(&quot;zz_jesse_practice_scripts/data/for_use&quot;, &quot;MA_Public_Schools_2017.csv&quot;)) -&gt; my_data ma_data_init &lt;- read_csv(here(&quot;zz_jesse_practice_scripts/data/for_use&quot;, &quot;MA_Public_Schools_2017.csv&quot;)) # Exploring and manipulating your data names(ma_data_init) glimpse(ma_dat_init) glimpse(ma_data_init) summary(ma_data_init) glimpse(ma_data_init$Town) summary(ma_data_init$Town) glimpse(ma_data_init$`AP_Test Takers`) glimpse(ma_data_init$`AP_Test Takers`) summary(ma_data_init$`AP_Test Takers`) ma_data_init %&gt;% group_by(`District Name`) %&gt;% count() ma_data_init %&gt;% group_by(`District Name`) %&gt;% count() ma_data_init %&gt;% group_by(`District Name`) %&gt;% count() %&gt;% filter(n &gt; 10) # ma_data_init %&gt;% # group_by(`District Name`) %&gt;% # count() %&gt;% # filter(n &gt; 10) %&gt;% # arrange(desc(n) ma_data_init %&gt;% group_by(`District Name`) %&gt;% count() %&gt;% filter(n &gt; 10) %&gt;% arrange(desc(n)) ma_data_init %&gt;% group_by(`District Name`) %&gt;% count() %&gt;% filter(n = 10) %&gt;% arrange(desc(n)) ma_data_init %&gt;% group_by(`District Name`) %&gt;% count() %&gt;% filter(n == 10) %&gt;% arrange(desc(n)) ma_data_init %&gt;% rename(district_name = `District Name`, grade = Grade) %&gt;% select(district_name, grade) ma_data %&gt;% clean_names() 01_ma_data &lt;- ma_data_init %&gt;% #&#39; intentional error clean_names() $_ma_data &lt;- ma_data_init %&gt;% #&#39; intentional error clean_names() ma_data_01 &lt;- ma_data_init %&gt;% clean_names() 6.5 Packages and Functions From the outside in Packages are a collection of functions, and most packages are designed for: a specific data set, a specific field, a specific set of tasks. Functions are individual components within a package, and functions are what we use to interact with our data. From the inside out To put it another way, an R user might write a series of functions that they find themselves needing to use repeatedly in a variety of projects. Instead of re-writing (or copying and pasting) the functions each time they need to use them, an R user can collect all of these individual functions inside a package. The R user can then load the package any time that they want to use the functions, using a single line of code instead of tens to tens of thousands of lines of code for each function. Most of the packages we’ll be working with in this book are available on the Comprehensive R Archive Network, or [CRAN]9https://cran.r-project.org/), which means that we can install them using the same set of commands from within R. The process of submitting a package and having it published through CRAN is beyond the scope of this book, and it’s important to point out that you - yes, you! - can create a package for yourself and never submit it for publication. (why?) Lastly, some packages are available directly from developers via GitHub. [image] The Tidyverse, a package of packages The tidyverse is a single package that contains additional packages, and within each of those individual packages are a set of functions. The tidyverse is an excellent tool that has a cohesive syntax across all functions, and the packages allow you do to the bulk of an analytical workflow. As of this writing, the tidyverse is the only known package of packages. [image] 6.6 Installing and loading a package It is entirely possible to do all of your work in R without ever using a package, however we do not recommend that approach due to the wealth of packages available that help reduce both the learning curve associated with R as well as the amount of time spent on any given analytical project. In order to access the functions within a package, you must first install the package on your computer. If the package is on CRAN we can install it by running the following code: install.packages(\"package_name\") Once a package is installed on your computer you do not have to re-install it in order to use the functions in the package (until you update R - is this still the case, or am I thinking of something else?). It is common to load a package into your R environment if you know that you’ll be using the functions from within the package. We load a package using the following code: library(package_name) Sometimes you’ll see require() instead of library(). We strongly advocate for the use of library(), as it forces R to load the package, and if the package is not installed or there are issues with the package, will give you an error message. require() on the other hand will not give an error if the package is not available or if there are issues with the package. Using library() will help to eliminate sources of confusion later on. 6.6.1 Recognizing a function Functions in R can be spotted by the use of a word adjacent to a set of parentheses, like so: word() The word (or set of words) represent the name of the function, and the parenthesis are where we can provide arguments to a function, if arguments are needed for the function to run. Many functions in R packages do not require arguments, and will use a set of default arguments unless you provide something different from the default. 6.6.2 Running code in R In order to run code in R you need to type your code either in the Console or within an .R (or .Rmd) script. For the purposes of this chapter we recommend creating an .R script to type all of your code because you can add comments and then save your .R script for reference, whereas anything that you type in the Console will disappear as soon as you restart or close R. To run code in the Console, you type your code and hit ‘Enter’. To run code in an R script, you can run a single line by highlighting it (or placing your cursor at the end of the line) and hitting Ctrl + Enter (get Mac codes). You can also run your code by highlighting it and clicking the Run button. [image] 6.6.3 Run this code Take a few minutes to type out and run each of the following code chunks: # Installing packages install.packages(&quot;tidyverse&quot;) install.packages(&quot;janitor&quot;) install.packages(&quot;skimr&quot;) install.packages(&quot;here&quot;) # Setting up your environment library(tidyverse) library(janitor) library(skimr) library(here) What did you notice happening in the Console? 6.6.4 How to find a package CRAN TaskViews Twitter Google searches 6.6.5 How to learn more about a package Vignettes Blog posts 6.6.6 Function conflicts: between packages, between base R and loaded packages Changing the load order Using :: (provide example) 6.6.7 Commenting in R # and #' 6.7 Using functions to import data 6.7.1 Run this code read_csv(here(&quot;zz_jesse_practice_scripts/data/for_use&quot;, &quot;MA_Public_Schools_2017.csv&quot;)) # change path once finalized in book read_csv(here(&quot;zz_jesse_practice_scripts/data/for_use&quot;, &quot;MA_Public_Schools_2017.csv&quot;)) -&gt; my_data ma_data_init &lt;- read_csv(here(&quot;zz_jesse_practice_scripts/data/for_use&quot;, &quot;MA_Public_Schools_2017.csv&quot;)) What do you notice happening in the Console? 6.7.2 The assignment operator &lt;- and -&gt; 6.7.3 The here package Works in concert with Projects in RStudio 6.7.4 Data types There are different types We can change data types through coercion See Hands on Programming With R and Advanced R for more in-depth information 6.8 Datasets and variables 6.8.1 Run the following code # Exploring and manipulating your data names(ma_data_init) glimpse(ma_dat_init) glimpse(ma_data_init) summary(ma_data_init) glimpse(ma_data_init$Town) summary(ma_data_init$Town) glimpse(ma_data_init$AP_Test Takers) glimpse(ma_data_init$`AP_Test Takers`) summary(ma_data_init$`AP_Test Takers`) What differences do you see between each line of code? What changes in the output to the Console with each line of code that you run? 6.8.2 Tibbles and dataframes, oh my! Difference between the two Other data structures you may encounter (resource: Hands on Programming with R) 6.8.3 The $ operator Many ways to isolate a single variable 6.8.4 Spaces in variable names 6.9 The pipe operator ma_data_init %&gt;% group_by(District Name) %&gt;% count() ma_data_init %&gt;% group_by(`District Name`) %&gt;% count() ma_data_init %&gt;% group_by(`District Name`) %&gt;% count() %&gt;% filter(n &gt; 10) # ma_data_init %&gt;% # group_by(`District Name`) %&gt;% # count() %&gt;% # filter(n &gt; 10) %&gt;% # arrange(desc(n) 6.9.1 Piping practices The magrittr package dplyr and package dependencies “Correct” pipe length (careful - will need to stick to this throughout text) 6.9.2 Closing your parentheses Seeing the + sign in the Console Fixing it by pressing Esc 6.10 Finicky functions (rename this section) ma_data_init %&gt;% group_by(`District Name`) %&gt;% count() %&gt;% filter(n &gt; 10) %&gt;% arrange(desc(n)) ma_data_init %&gt;% group_by(`District Name`) %&gt;% count() %&gt;% filter(n = 10) %&gt;% arrange(desc(n)) ma_data_init %&gt;% group_by(`District Name`) %&gt;% count() %&gt;% filter(n == 10) %&gt;% arrange(desc(n)) ma_data_init %&gt;% rename(district_name = `District Name`, grade = Grade) %&gt;% select(district_name, grade) 6.10.1 When functions need arguments and when they don’t (Is there any way to know for sure?) Help documentation 6.10.2 Functions within functions Saw in import as well 6.10.3 The difference between = and == Assignment vs. equality 6.10.4 Writing your own functions Basic function structure Quosures - out of scope of this chapter 6.11 SECTION ON c() and %in% 6.12 Creating objects In R, everything is an object ma_data %&gt;% clean_names() 01_ma_data &lt;- ma_data_init %&gt;% #&#39; intentional error clean_names() $_ma_data &lt;- ma_data_init %&gt;% #&#39; intentional error clean_names() ma_data_01 &lt;- ma_data_init %&gt;% clean_names() 6.12.1 Object-oriented programming (OOP) How in-depth should we go? May not be relevant 6.12.2 Analysis vs. development Using pre-made functions vs. writing our own functions and packages, exploring pre-made functions – import from 05A: plug in as needed 6.13 Creating Projects Before proceeding, we’re going to take a few steps to set ourselves to make the analysis easier; namely, through the use of Projects, an RStudio-specific organizational tool. To create a project, in RStudio, navigate to “File” and then “New Directory”. Then, click “New Project”. Choose a directory name for the project that helps you to remember that this is a project that involves data science in education; it can be convenient if the name is typed in lower-case-letters-separated-by-dashes, like that. You can also choose the sub-directory. If you are just using this to learn and to test out creating a Project, you may consider placing it in your downloads or another temporary directory so that you remember to remove it later. Even if you do not create a Project, you can always check where your working directory (i.e., where your R is pointing) is by running getwd(). To change it manually, run setwd(desired/file/path/here). 6.13.1 Track Two: Welcome to the Tidyverse The tidyverse is a set of packages for data manipulation, exploration, and visualization using the design philosophy of ‘tidy’ data. Tidy data has a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. The packages contained in the tidyverse provide useful functions that augment base R functionality. You can installing and load the complete tidyverse with: install.packages(&quot;tidyverse&quot;) library(tidyverse) For more information on tidy data, check out Hadley Wickhams’s Tidy Data paper. 6.14 Include at end of chapter! 6.15 Loading Data from Excel, SAV, and Google Sheets 6.15.1 Loading Excel Files We will now do the same with an Excel file. You might be thinking that you can open the file in Excel and then save it as a .csv. This is generally a good idea. At the same time, sometimes you may need to directly read a file from Excel. Note that, when possible, we recommend the use of .csv files. They work well across platforms and software (i.e., even if you need to load the file with some other software, such as Python). The package for loading Excel files, readxl, is not a part of the tidyverse, so we will have to install it first (remember, we only need to do this once), and then load it using library(readxl). Note that the command to install readxl is grayed-out below: The # symbol before install.packages(\"readxl\") indicates that this line should be treated as a comment and not actually run, like the lines of code that are not grayed-out. It is here just as a reminder that the package needs to be installed if it is not already. Once we have installed readxl, we have to load it (just like tidyverse): install.packages(&quot;readxl&quot;) library(readxl) We can then use the function read_excel() in the same way as read_csv(), where “path/to/file.xlsx” is where an Excel file you want to load is located (note that this code is not run here): my_data &lt;- read_excel(&quot;path/to/file.xlsx&quot;) Of course, were this run, you can replace my_data with a name you like. Generally, it’s best to use short and easy-to-type names for data as you will be typing and using it a lot. Note that one easy way to find the path to a file is to use the “Import Dataset” menu. It is in the Environment window of RStudio. Click on that menu bar option, select the option corresponding to the type of file you are trying to load (e.g., “From Excel”), and then click The “Browse” button beside the File/URL field. Once you click on the, RStudio will automatically generate the file path - and the code to read the file, too - for you. You can copy this code or click Import to load the data. 6.15.2 Loading SAV Files The same factors that apply to reading Excel files apply to reading SAV files (from SPSS). NOte that you can also read .csv file directly into SPSS and so because of this and the benefits of using CSVs (they are simple files that work across platforms and software), we recommend using CSVs when possible. First, install the package haven, load it, and the use the function read_sav(): install.packages(&quot;haven&quot;) library(haven) my_data &lt;- read_sav(&quot;path/to/file.xlsx&quot;) 6.15.3 Google Sheets Finally, it can sometimes be useful to load a file directly from Google Sheets, and this can be done using the Google Sheets package. install.packages(&quot;googlesheets&quot;) library(googlesheets) When you run the command below, a link to authenticate with your Google account will open in your browser. my_sheets &lt;- gs_ls() You can then simply use the gs_title() function in conjunction with the gs_read() function: df &lt;- gs_title(&#39;title&#39;) df &lt;- gs_read(df) This section goes into depth on loading various types of data sources. The next chapter covers a very specific .csv import and looks more at data types and the tidy data structure. In this section, we’ll load data. You might be thinking that an Excel file is the first that we would load, but there happens to be a format which you can open and edit in Excel that is even easier to use between Excel and R as well as SPSS and other statistical software, like MPlus, and even other programming languages, like Python. That format is .csv, or a comma-separated-values file. The .csv file is useful because you can open it with Excel and save Excel files as .csv files. Additionally, and as its name indicates, a .csv file is rows of a spreadsheet with the columns separated by commas, so you can view it in a text editor, like TextEdit for Macintosh, as well. Not surprisingly, Google Sheets easily converts .csv files into a Sheet, and also easily saves Sheets as .csv files. However we would be remiss if we didn’t point out that there is a package, googlesheets, which can be used to read a Google Sheet directly into R. For these reasons, we start with - and emphasize - reading .csv files. 6.15.4 Saving a File from the Web You’ll need to copy this URL: https://goo.gl/bUeMhV Here’s what it resolves to (it’s a .csv file): https://raw.githubusercontent.com/data-edu/data-science-in-education/master/data/pisaUSA15/stu-quest.csv This next chunk of code downloads the file to your working directory. Run this to download it so in the next step you can read it into R. As a note: There are ways to read the file directory (from the web) into R. Also, of course, you could do what the next (two) lines of code do manually: Feel free to open the file in your browser and to save it to your computer (you should be able to ‘right’ or ‘control’ click the page to save it as a text file with a .csv extension). student_responses_url &lt;- &quot;https://goo.gl/bUeMhV&quot; student_responses_file_name &lt;- paste0(getwd(), &quot;/data/student-responses-data.csv&quot;) download.file( url = student_responses_url, destfile = student_responses_file_name) It may take a few seconds to download as it’s around 20 MB. The process above involves many core data science ideas and ideas from programming/coding. We will walk through them step-by-step. The character string \"https://goo.gl/wPmujv\" is being saved to an object called student_responses_url. student_responses_url &lt;- &quot;https://goo.gl/bUeMhV&quot; We concatenate your working directory file path to the desired file name for the .csv using a function called paste0. This is stored in another object called student_reponses_file_name. This creates a file name with a file path in your working directory and it saves the file in the folder that you are working in. student_responses_file_name &lt;- paste0(getwd(), &quot;/data/student-responses-data.csv&quot;) The student_responses_url object is passed to the url argument of the function called download.file() along with student_responses_file_name, which is passed to the destfile argument. In short, the download.file() function needs to know - where the file is coming from (which you tell it through the url) argument and - where the file will be saved (which you tell it through the destfile argument). download.file( url = student_responses_url, destfile = student_responses_file_name) Understanding how R is working in these terms can be helpful for troubleshooting and reaching out for help. It also helps you to use functions that you have never used before because you are familiar with how some functions work. Now, in RStudio, you should see the downloaded file in the Files tab. This should be the case if you created a project with RStudio; if not, it should be whatever your working directory is set to. If the file is there, great. If things are not working, consider downloading the file in the manual way and then move it into the directory that the R Project you created it. 6.15.5 Loading a .csv File Okay, we’re ready to go. The easiest way to read a .csv file is with the function read_csv() from the package readr, which is contained within the Tidyverse. Let’s load the tidyverse library: library(tidyverse) # so tidyverse packages can be used for analysis You may have noticed the hash symbol after the code that says library(tidyverse). It reads# so tidyverse packages can be used for analysis`. That is a comment and the code after it (but not before it) is not run (the code before it runs just like normal). Comments are useful for showing why a line of code does what it does. After loading the tidyverse packages, we can now load a file. We are going to call the data student_responses: # readr::write_csv(pisaUSA15::stu_quest, here::here(&quot;data&quot;, &quot;pisaUSA15&quot;, &quot;stu_quest.csv&quot;)) student_responses &lt;- read_csv(&quot;./data/student-responses-data.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_double(), ## CNT = col_character(), ## CYC = col_character(), ## NatCen = col_character(), ## STRATUM = col_character(), ## Option_Read = col_character(), ## Option_Math = col_character(), ## ST011D17TA = col_character(), ## ST011D18TA = col_character(), ## ST011D19TA = col_character(), ## ST124Q01TA = col_logical(), ## IC001Q01TA = col_logical(), ## IC001Q02TA = col_logical(), ## IC001Q03TA = col_logical(), ## IC001Q04TA = col_logical(), ## IC001Q05TA = col_logical(), ## IC001Q06TA = col_logical(), ## IC001Q07TA = col_logical(), ## IC001Q08TA = col_logical(), ## IC001Q09TA = col_logical(), ## IC001Q10TA = col_logical() ## # ... with 420 more columns ## ) ## See spec(...) for full column specifications. Since we loaded the data, we now want to look at it. We can type its name in the function glimpse() to print some information on the dataset (this code is not run here). glimpse(student_responses) Woah, that’s a big data frame (with a lot of variables with confusing names, to boot)! Great job loading a file and printing it! We are now well on our way to carrying out analysis of our data. 6.15.6 Saving Files Using our data frame student_responses, we can save it as a .csv (for example) with the following function. The first argument, student_reponses, is the name of the object that you want to save. The second argument, student-responses.csv, what you want to call the saved dataset. write_csv(student_responses, &quot;student-responses.csv&quot;) That will save a .csv file entitled student-responses.csv in the working directory. If you want to save it to another directory, simply add the file path to the file, i.e. path/to/student-responses.csv. To save a file for SPSS, load the haven package and use write_sav(). There is not a function to save an Excel file, but you can save as a .csv and directly load it in Excel. 6.15.7 Conclusion We will detail the functions used to read every file in a folder (or, to write files to a folder). 6.16 Processing Data Now that we have loaded student_responses into an object, we can process it. This section highlights some common data processing functions. We’re also going to introduce a powerful, unusual operator in R, the pipe. The pipe is this symbol: %&gt;%. It lets you compose functions. It does this by passing the output of one function to the next. A handy shortcut for writing out %&gt;% is Command + Shift + M. Here’s an example. Let’s say that we want to select a few variables from the student_responses dataset and save those variables into a new object, student_mot_vars. Here’s how we would do that using dplyr::select(). student_mot_vars &lt;- # save object student_mot_vars by... student_responses %&gt;% # using dataframe student_responses select(SCIEEFF, JOYSCIE, INTBRSCI, EPIST, INSTSCIE) # and selecting only these five variables Note that we saved the output from the select() function to student_mot_vars but we could also save it back to student_responses, which would simply overwrite the original data frame (the following code is not run here): student_responses &lt;- # save object student_responses by... student_responses %&gt;% # using dataframe student_responses select(student_responses, SCIEEFF, JOYSCIE, INTBRSCI, EPIST, INSTSCIE) # and selecting only these five variables We can also rename the variables at the same time we select them. I put these on separate lines so I could add the comment, but you could do this all in the same line, too. It does not make a difference in terms of how select() will work. student_mot_vars &lt;- # save object student_mot_vars by... student_responses %&gt;% # using dataframe student_responses select(student_efficacy = SCIEEFF, # selecting variable SCIEEFF and renaming to student_efficiency student_joy = JOYSCIE, # selecting variable JOYSCIE and renaming to student_joy student_broad_interest = INTBRSCI, # selecting variable INTBRSCI and renaming to student_broad_interest student_epistemic_beliefs = EPIST, # selecting variable EPIST and renaming to student_epistemic_beliefs student_instrumental_motivation = INSTSCIE # selecting variable INSTSCIE and renaming to student_instrumental_motivation ) [will add more on creating new variables, filtering grouping and summarizing, and joining data sets] 6.17 Communicating / Sharing Results R Markdown is a highly convenient way to communicate and share results. Navigate to “New File” and then “R Markdown”. [add] Then, click “Knit to PDF”, “Knit to HTML”, or “Knit to Word”. 6.17.1 miscellaneous from import clipr is a package to easily copy data into and out of R using the clipboard. [add more] datapasta is another option. [add more] – end of import 6.18 Old text - keep until reincorporated//readdressed To break that down another way, it’s helpful to know that R reads everything on the left of the assignment operator, &lt;-, first, and while it reads from left to right, it starts with the innermost set of parentheses per object. We can recognize functions in R by the use of parentheses. For example, data() is a function that pulls up all of the existing data sets within R. The parentheses within a function can be empty, but sometimes a function requires arguments in order to work. Arguments are the components that go inside the parentheses of a function, like when we coerced our UCBAdmissions data into a tibble. We can see which arguments a function uses by calling up the help documentation. We do this by going to the Console and typing a ? followed by the function’s name, then hitting enter. For example, run ?data in the Console (notice that there is not a space between the question mark and the function’s name). Now try ?mutate - what’s different? mutate is a function from the dplyr package - a package we’ll use extensively throughout this book. But because we haven’t brought the dplyr package into R yet (how does this fit into the mental model? Is this the right place to talk about this? - might be too far out of scope and belong at the end, in the explanations section.) R doesn’t “see” the mutate function as existing. If we want to search all of R’s packages (on our computer? look into this - may require package installation section) we can use ??function_name, in this case ??mutate. This brings up the entire list of packages that mention a mutate function. It’s important to note that there is no requirement that function names be specific within the entire world of R (OK this definitely goes at the end). There’s not a lot stopping every package on CRAN (all 10,000+ of them!) from having their own mutate function. That’s not necessarily a problem, as you likely won’t be running every package on CRAN simultaneously over the course of this book. However we can address two quick ways of dealing with this: The order in which you load your packages matters! If there are conflicting functions between packages, the package loaded most recently will take priority. But what if you need a function from a package that was loaded earlier? It might be tempting to re-load the earlier package, but we don’t recommend this - it’s going to cause you headaches further down the line when you try to re-run your script. Instead, keep in mind option two: Explicitly tell R which package to load a function from - independent of the order in which you loaded the packages - by using a double colon (is this what it’s called?) ::. This brings us full circle back to translating everything we did with the UCBAdmissions data, where we saw the code tibble::as_tibble(UCBAdmissions). The use of the :: is how we tell R to go to the tibble package and pull out the as_tibble function. “But wait!” you say, “we’ve never loaded a package!” Exactly! We can call functions from packages without loading the package into our environment by using the ::. The only requirement is that the package be installed on the machine that we’re using (confirm that this is true). 6.19 May not need Grab your data Kaggle Massachusetts Public Schools Data Comes as a .zip file (explain this?) "],
["c07.html", "7 Walkthrough 1: The Education Dataset Science Pipeline With Online Science Class Data 7.1 Vocabulary 7.2 Introduction 7.3 Load Packages 7.4 Import Data 7.5 View data 7.6 Process data 7.7 Analysis 7.8 Results 7.9 Conclusion", " 7 Walkthrough 1: The Education Dataset Science Pipeline With Online Science Class Data 7.1 Vocabulary 7.2 Introduction 7.2.1 Background In the 2015-2016 and 2016-2017 school years, researchers carried out a study on students’ motivation to learn in online science classes. The online science classes were part of a statewide online course provider designed to supplement(and not replace) students’ enrollment in their local school. For example, students may choose to enroll in an online physics class because one was not offered at their school (or they were not able to take it given their schedule). The study involved a number of different data sources which were explored to understand students’ motivation: A self-report survey for three distinct but related aspects of students’ motivation Log-trace data, such as data output from the learning management system Discussion board data (not used in this walkthrough) Achievement-related (i.e., final grade) data Our purpose for this walkthrough is to begin to understand what explains students’ performance in these online courses. To do so, we will focus on a variable that was available through the learning management system used for the courses, on he amount of time sudents’ spent on the course. We will also explore how different (science) subjects as well as being in a particular class may help to explain student performance. First, these different data sources will be described in terms of how they were provided by the school. 7.2.2 Data Sources 7.2.2.1 Data source #1: Self-report survey about students’ motivation The first data source is a self-report survey. This was data collected before the start of the course via self-report survey. The survey included 10 items, each corresponding to one of three measures, namely, for interest, utility value, and perceived competence: I think this course is an interesting subject. (Interest) What I am learning in this class is relevant to my life. (Utility value) I consider this topic to be one of my best subjects. (Perceived competence) I am not interested in this course. (Interest - reverse coded) I think I will like learning about this topic. (Interest) I think what we are studying in this course is useful for me to know. (Utility value) I don’t feel comfortable when it comes to answering questions in this area. (Perceived competence) I think this subject is interesting. (Interest) I find the content of this course to be personally meaningful. (Utility value) I’ve always wanted to learn more about this subject. (Interest) 7.2.3 Data source #2: Log-trace data Log-trace data is data generated from our interactions with digital technologies, such as archived data from social media postings (see chapter 11 and chapter 12). In education, an increasingly common source of log-trace data is that generated from interactions with learning management systems and other digital tools (Siemens and d Baker 2012). The data for this walk-through is a summary of log-trace data, namely, the number of minutes students spent on the course. Thus, while this data is rich, you can imagine even more complex sources of log-trace data (i.e. timestamps associated with when students started and stopped accessing the course!). 7.2.4 Data source #3: Achievement-related and gradebook data This is a common source of data, namely, one associated with graded assignments students completed. In this walkthrough, we just examine students’ final grade. 7.2.5 Data source #4: Discussion board data Discussion board data is both rich and unstructured, in that it is primarily in the form of written text. We collected discussion board data, too, and highlight this as a potentially very rich data source. 7.2.6 Methods 7.3 Load Packages This analysis uses R packages, which are collections of R code that help users code more efficiently, as you wil recall from Chapter 1. We load these packages with the function library. In particular, the packages we’ll use will help us load Excel files, organize the structure of the data, work with dates in the data, and navigate file directories. library(readxl) library(tidyverse) library(lubridate) library(here) library(dataedu) library(apaTables) library(sjPlot) 7.4 Import Data This code chunk loads the log trace data using the read_csv function. Note that we call read_csv three times, once for each of the three logtrace datasets. We assign each of the datasets a name using &lt;-. # Gradebook and log-trace data for F15 and S16 semesters course_data &lt;- dataedu::course_data # Pre-survey for the F15 and S16 semesters pre_survey &lt;- dataedu::pre_survey # Log-trace data for F15 and S16 semesters - this is for time spent course_minutes &lt;- dataedu::course_minutes 7.5 View data Now that we’ve successfully loaded all three logtrace datasets, we can visually inspect the data by typing the names that we assigned to each dataset. pre_survey ## # A tibble: 1,102 x 12 ## RespondentId opdata_CourseID Q1Maincellgroup… Q1Maincellgroup… ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 489226 OcnA-S216-02 4 4 ## 2 434707 BioA-S116-01 4 3 ## 3 436886 BioA-S116-01 4 3 ## 4 483940 FrScA-S216-02 5 5 ## 5 433131 AnPhA-S116-01 5 5 ## 6 432242 OcnA-S116-01 4 2 ## 7 432987 FrScA-S116-01 NA NA ## 8 432242 OcnA-S116-01 4 3 ## 9 434209 BioA-S116-01 NA NA ## 10 457331 FrScA-S116-01 5 4 ## # … with 1,092 more rows, and 8 more variables: Q1MaincellgroupRow3 &lt;dbl&gt;, ## # Q1MaincellgroupRow4 &lt;dbl&gt;, Q1MaincellgroupRow5 &lt;dbl&gt;, ## # Q1MaincellgroupRow6 &lt;dbl&gt;, Q1MaincellgroupRow7 &lt;dbl&gt;, ## # Q1MaincellgroupRow8 &lt;dbl&gt;, Q1MaincellgroupRow9 &lt;dbl&gt;, ## # Q1MaincellgroupRow10 &lt;dbl&gt; course_data ## # A tibble: 29,711 x 5 ## CourseSectionOrigID Bb_UserPK Points_Possible Points_Earned Gender ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AnPhA-S116-01 60186 5 5 F ## 2 AnPhA-S116-01 60186 30 27.4 F ## 3 AnPhA-S116-01 60186 10 10 F ## 4 AnPhA-S116-01 60186 5 3 F ## 5 AnPhA-S116-01 60186 5 5 F ## 6 AnPhA-S116-01 60186 30 27 F ## 7 AnPhA-S116-01 60186 5 4 F ## 8 AnPhA-S116-01 60186 597 575. F ## 9 AnPhA-S116-01 60186 10 NA M ## 10 AnPhA-S116-01 60186 10 10 F ## # … with 29,701 more rows course_minutes ## # A tibble: 598 x 3 ## Bb_UserPK CourseSectionOrigID TimeSpent ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 88596 FrScA-S216-01 886. ## 2 95957 AnPhA-S216-01 4013. ## 3 72287 PhysA-S116-01 920. ## 4 87489 FrScA-S116-01 1452. ## 5 78002 FrScA-S116-01 1464. ## 6 96871 FrScA-S216-02 1671. ## 7 90996 PhysA-S116-01 1303. ## 8 91175 FrScA-S116-04 1589. ## 9 86267 AnPhA-S216-01 1632. ## 10 86277 AnPhA-S116-01 2528. ## # … with 588 more rows 7.6 Process data Often, survey data needs to be processed in order to be (most) useful. Here, we process the self-report items into three scales, for: interest, self-efficacy, and utility value. We do this by Renaming the question variables to something more managable Reversing the response scales on questions 4 and 7 Categorizing each question into a measure Computing the mean of each measure Let’s take these steps in order: Rename the question columns to something much simpler: pre_survey &lt;- pre_survey %&gt;% # Rename the qustions something easier to work with because R is case sensitive # and working with variable names in mix case is prone to error rename( q1 = Q1MaincellgroupRow1, q2 = Q1MaincellgroupRow2, q3 = Q1MaincellgroupRow3, q4 = Q1MaincellgroupRow4, q5 = Q1MaincellgroupRow5, q6 = Q1MaincellgroupRow6, q7 = Q1MaincellgroupRow7, q8 = Q1MaincellgroupRow8, q9 = Q1MaincellgroupRow9, q10 = Q1MaincellgroupRow10 ) %&gt;% # Convert all question responses to numeric mutate_at(vars(q1:q10), list( ~ as.numeric(.))) Let’s take a moment to discuss the dplyr function mutate_at. mutate_at is a version of mutate, which changes the values in an existing column or creates new columns. It’s useful in education datasets because you’ll often need to transform your data before analyzing it. Try this example, where we create a new total_students column by adding the number of male students and female students: # Dataset of students df &lt;- tibble( male = 5, female = 5 ) df %&gt;% mutate(total_students = male + female) ## # A tibble: 1 x 3 ## male female total_students ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 5 10 mutate_at is a special version of mutate, which conveniently changes the values of multiple columns. In our dataset pre_survey, we let mutate know we want to change the variables q1 through q10. We do this with the argument vars(q1:q10) Next we’ll reverse the scale of the survey responses on questions 4 and 7 so the responses for all questions can be interpreted in the same way. Rather than write a lot of code once to reverse the scales for question 4 then writing it again to reverse the scales on question 7, we’ll build a function that does that job for us. Then we’ll use the same function for question 4 and question 7. This will result in much less code, plus it will make it easier for us to change in the future. # This part of the code is where we write the function: # Function for reversing scales reverse_scale &lt;- function(question) { # Reverses the response scales for consistency # Args: # question: survey question # Returns: a numeric converted response # Note: even though 3 is not transformed, case_when expects a match for all # possible conditions, so it&#39;s best practice to label each possible input # and use TRUE ~ as the final statement returning NA for unexpected inputs x &lt;- case_when( question == 1 ~ 5, question == 2 ~ 4, question == 4 ~ 2, question == 5 ~ 1, question == 3 ~ 3, TRUE ~ NA_real_ ) x } # And here&#39;s where we use that function to reverse the scales # Reverse scale for questions 4 and 7 pre_survey &lt;- pre_survey %&gt;% mutate(q4 = reverse_scale(q4), q7 = reverse_scale(q7)) We’ll accomplish the last two steps in one chunk of code. First we’ll create a column called measure and we’ll fill that column with one of three question categories: int: interest uv: utility value pc: self efficacy After that we’ll find the mean response of each category using mean function. # Add measure variable measure_mean &lt;- pre_survey %&gt;% # Gather questions and responses pivot_longer(cols = q1:q10, names_to = &quot;question&quot;, values_to = &quot;response&quot;) %&gt;% # Here&#39;s where we make the column of question categories mutate( measure = case_when( question %in% c(&quot;q1&quot;, &quot;q4&quot;, &quot;q5&quot;, &quot;q8&quot;, &quot;q10&quot;) ~ &quot;int&quot;, question %in% c(&quot;q2&quot;, &quot;q6&quot;, &quot;q9&quot;) ~ &quot;uv&quot;, question %in% c(&quot;q3&quot;, &quot;q7&quot;) ~ &quot;pc&quot;, TRUE ~ NA_character_ ) ) %&gt;% group_by(measure) %&gt;% # Here&#39;s where we compute the mean of the responses summarise( # Mean response for each measure mean_response = mean(response, na.rm = TRUE), # Percent of each measure that had NAs in the response field percent_NA = mean(is.na(response)) ) measure_mean ## # A tibble: 3 x 3 ## measure mean_response percent_NA ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 int 4.25 0.175 ## 2 pc 3.65 0.175 ## 3 uv 3.77 0.177 We will use a similar process later to calculate these variables’ correlations. 7.6.1 Processing the course data We also can process the course data in order to create new variables which we can use in analyses. Information about the course subject, semester, and section are stored in a single column, CourseSectionOrigID. If we give each of these their own columns, we’ll have more opportunities to analyze them as their own variables. We’ll use a function called separate to do this. This pulls out the subject, semester, and section from the course ID so we can use them later on. # split course section into components course_data &lt;- course_data %&gt;% # Give course subject, semester, and section their own columns separate( col = CourseSectionOrigID, into = c(&quot;subject&quot;, &quot;semester&quot;, &quot;section&quot;), sep = &quot;-&quot;, remove = FALSE ) 7.6.2 Joining the data To join the course data and pre-survey data, we need to create similar keys. In other words, our goal here is to have one variable that matches across both datasets, so that we can merge the datasets on the basis of that variable. For these data, both have variables for the course and the student, though they have different names in each. Our first goal will be to rename two variables in each of our datasets so that they will match. One variable will correspond to the course, and the other will correspond to the student. We are not changing anything in the data itself at this step - instead, we are just cleaning it up so that we can look at the data all in one place. Let’s start with the pre-survey data. We will rename RespondentID and opdata_CourseID to be student_id and course_id, respectively. pre_survey &lt;- pre_survey %&gt;% rename(student_id = RespondentId, course_id = opdata_CourseID) pre_survey ## # A tibble: 1,102 x 12 ## student_id course_id q1 q2 q3 q4 q5 q6 q7 q8 q9 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 489226 OcnA-S21… 4 4 3 4 5 4 4 4 5 ## 2 434707 BioA-S11… 4 3 4 4 4 4 5 4 3 ## 3 436886 BioA-S11… 4 3 3 5 5 5 4 4 3 ## 4 483940 FrScA-S2… 5 5 5 5 5 4 5 5 4 ## 5 433131 AnPhA-S1… 5 5 3 5 5 5 3 5 5 ## 6 432242 OcnA-S11… 4 2 2 4 5 3 4 5 1 ## 7 432987 FrScA-S1… NA NA NA NA NA NA NA NA NA ## 8 432242 OcnA-S11… 4 3 3 4 4 3 3 4 3 ## 9 434209 BioA-S11… NA NA NA NA NA NA NA NA NA ## 10 457331 FrScA-S1… 5 4 5 5 5 5 5 5 5 ## # … with 1,092 more rows, and 1 more variable: q10 &lt;dbl&gt; Looks better now! Let’s proceed to the course data. Our goal is to rename two variables that correspond to the course and the student so that we can match with the other variables we just created for the pre-survey data. course_data &lt;- course_data %&gt;% rename(student_id = Bb_UserPK, course_id = CourseSectionOrigID) Now that we have two variables that are consistent across both datasets - we have called them “course_id” and “student_id” - we can join these using the dplyr function, left_join(). Let’s save our joined data as a new object called “dat.” dat &lt;- left_join(course_data, pre_survey, by = c(&quot;student_id&quot;, &quot;course_id&quot;)) dat ## # A tibble: 29,711 x 18 ## course_id subject semester section student_id Points_Possible Points_Earned ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AnPhA-S1… AnPhA S116 01 60186 5 5 ## 2 AnPhA-S1… AnPhA S116 01 60186 30 27.4 ## 3 AnPhA-S1… AnPhA S116 01 60186 10 10 ## 4 AnPhA-S1… AnPhA S116 01 60186 5 3 ## 5 AnPhA-S1… AnPhA S116 01 60186 5 5 ## 6 AnPhA-S1… AnPhA S116 01 60186 30 27 ## 7 AnPhA-S1… AnPhA S116 01 60186 5 4 ## 8 AnPhA-S1… AnPhA S116 01 60186 597 575. ## 9 AnPhA-S1… AnPhA S116 01 60186 10 NA ## 10 AnPhA-S1… AnPhA S116 01 60186 10 10 ## # … with 29,701 more rows, and 11 more variables: Gender &lt;chr&gt;, q1 &lt;dbl&gt;, ## # q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;, q7 &lt;dbl&gt;, q8 &lt;dbl&gt;, ## # q9 &lt;dbl&gt;, q10 &lt;dbl&gt; left_join() is named on the basis of the order of the two data frames that are being joined - and which data frame is joined to the other. In the above case, note the order of the data frames passed to our “left” join. Let’s focus just on the first two arguments. Left joins retain all of the rows in the “left” data frame, and joins every matching row in the right data frame to it. Note that in the above, after left_join(, we see course_data and then pre_survey. In this case, course_data is the “left” data frame (passed as the first argument), while pre_survey is the “right” data frame, passed as the second argument. So, in the above, knowing how left_join() works, what happened? You can run the code yourself to check. What our aim was - and what should happen - is that all of the rows of course_data are retained in our new data frame, dat, with matching rows of pre_survey joined to it. We note that in this case, one key is that there are not multiple matching rows of pre-survey: in this case, you would end up with more rows in dat than expected. There is a lot packed into one simple function that we just unpacked. Joins are, however, extremely powerful - and common - in many data analysis processing pipelines, in education and in any field. Think of all of the times you have data in more than one data frame, and want them to be in a single data frame! As a result, we think that joins are well worth investing the time to be able to use. With education (and other) data, left_joins() are helpful for carrying out most tasks related to joining datasets. There are, though, functions for other types of joins, those less important than left_join() but still worth mentioning. They are the following (note that for all of these, the “left” data frame is always the first argument, and the “right” data frame is always the second): 7.6.2.1 semi_join() semi_join(): joins and retains all of the matching rows in the “left” and “right” data frame; useful when you are only interested in keeping the rows (or cases/observations) that are able to be joined. semi_join() will not create duplicate rows of the left data frame, even when it finds multiple matches on the right data frame. It will also keep only the columns from the left data frame. For example, the following returns only the rows that are present in both course_data and pre_survey: dat_semi &lt;- semi_join(course_data, pre_survey, by = c(&quot;student_id&quot;, &quot;course_id&quot;)) dat_semi ## # A tibble: 0 x 8 ## # … with 8 variables: course_id &lt;chr&gt;, subject &lt;chr&gt;, semester &lt;chr&gt;, ## # section &lt;chr&gt;, student_id &lt;dbl&gt;, Points_Possible &lt;dbl&gt;, ## # Points_Earned &lt;dbl&gt;, Gender &lt;chr&gt; 7.6.2.2 anti_join() anti_join(): removes all of the rows in the “left” data frame that can be joined with those in the “right” data frame. dat_anti &lt;- anti_join(course_data, pre_survey, by = c(&quot;student_id&quot;, &quot;course_id&quot;)) dat_anti ## # A tibble: 29,711 x 8 ## course_id subject semester section student_id Points_Possible Points_Earned ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AnPhA-S1… AnPhA S116 01 60186 5 5 ## 2 AnPhA-S1… AnPhA S116 01 60186 30 27.4 ## 3 AnPhA-S1… AnPhA S116 01 60186 10 10 ## 4 AnPhA-S1… AnPhA S116 01 60186 5 3 ## 5 AnPhA-S1… AnPhA S116 01 60186 5 5 ## 6 AnPhA-S1… AnPhA S116 01 60186 30 27 ## 7 AnPhA-S1… AnPhA S116 01 60186 5 4 ## 8 AnPhA-S1… AnPhA S116 01 60186 597 575. ## 9 AnPhA-S1… AnPhA S116 01 60186 10 NA ## 10 AnPhA-S1… AnPhA S116 01 60186 10 10 ## # … with 29,701 more rows, and 1 more variable: Gender &lt;chr&gt; 7.6.2.3 right_join() right_join(): perhaps the least helpful of the three, right_join() works the same as left_join(), but by retaining all of the rows in the “right” data frame, and joining matching rows in the “left” data frame (so, the opposite of left_join()). dat_right &lt;- right_join(course_data, pre_survey, by = c(&quot;student_id&quot;, &quot;course_id&quot;)) dat_right ## # A tibble: 1,102 x 18 ## course_id subject semester section student_id Points_Possible Points_Earned ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 OcnA-S21… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 489226 NA NA ## 2 BioA-S11… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 434707 NA NA ## 3 BioA-S11… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 436886 NA NA ## 4 FrScA-S2… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 483940 NA NA ## 5 AnPhA-S1… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 433131 NA NA ## 6 OcnA-S11… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 432242 NA NA ## 7 FrScA-S1… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 432987 NA NA ## 8 OcnA-S11… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 432242 NA NA ## 9 BioA-S11… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 434209 NA NA ## 10 FrScA-S1… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 457331 NA NA ## # … with 1,092 more rows, and 11 more variables: Gender &lt;chr&gt;, q1 &lt;dbl&gt;, ## # q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;, q7 &lt;dbl&gt;, q8 &lt;dbl&gt;, ## # q9 &lt;dbl&gt;, q10 &lt;dbl&gt; If we wanted this to return exacty the same output as left_join() (and so to create a data frame that is identical to the dat data frame above), we could simply switch the order of the two data frames to be the opposite of those used for the left_join() above: dat_right &lt;- semi_join(pre_survey, course_data, by = c(&quot;student_id&quot;, &quot;course_id&quot;)) dat_right ## # A tibble: 0 x 12 ## # … with 12 variables: student_id &lt;dbl&gt;, course_id &lt;chr&gt;, q1 &lt;dbl&gt;, q2 &lt;dbl&gt;, ## # q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;, q7 &lt;dbl&gt;, q8 &lt;dbl&gt;, q9 &lt;dbl&gt;, ## # q10 &lt;dbl&gt; Just one more data frame to merge: course_minutes &lt;- course_minutes %&gt;% rename(student_id = Bb_UserPK, course_id = CourseSectionOrigID) course_minutes &lt;- course_minutes %&gt;% # Change the data type for student_id in course_minutes so we can match to # student_id in dat mutate(student_id = as.integer(student_id)) dat &lt;- dat %&gt;% left_join(course_minutes, by = c(&quot;student_id&quot;, &quot;course_id&quot;)) Note that they’re now combined, even though the course data has many more rows: The pre_survey data has been joined for each student by course combination. We have a pretty large data frame! Let’s take a quick look. dat ## # A tibble: 31,539 x 19 ## course_id subject semester section student_id Points_Possible Points_Earned ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AnPhA-S1… AnPhA S116 01 60186 5 5 ## 2 AnPhA-S1… AnPhA S116 01 60186 30 27.4 ## 3 AnPhA-S1… AnPhA S116 01 60186 10 10 ## 4 AnPhA-S1… AnPhA S116 01 60186 5 3 ## 5 AnPhA-S1… AnPhA S116 01 60186 5 5 ## 6 AnPhA-S1… AnPhA S116 01 60186 30 27 ## 7 AnPhA-S1… AnPhA S116 01 60186 5 4 ## 8 AnPhA-S1… AnPhA S116 01 60186 597 575. ## 9 AnPhA-S1… AnPhA S116 01 60186 10 NA ## 10 AnPhA-S1… AnPhA S116 01 60186 10 10 ## # … with 31,529 more rows, and 12 more variables: Gender &lt;chr&gt;, q1 &lt;dbl&gt;, ## # q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;, q7 &lt;dbl&gt;, q8 &lt;dbl&gt;, ## # q9 &lt;dbl&gt;, q10 &lt;dbl&gt;, TimeSpent &lt;dbl&gt; It looks like we have 31539 observations from 30 variables. There is one last step to take. Were we interested in a fine-grained analysis of how students performed (according to the teacher) on different assignments (see the Gradebook_Item column), we would keep all rows of the data. But, our goal (for now) is more modest: to calculate the percentage of points students earned as a measure of their final grade (noting that the teacher may have assigned a different grade–or weighted their grades in ways not reflected through the points). dat &lt;- dat %&gt;% group_by(student_id, course_id) %&gt;% mutate(Points_Earned = as.integer(Points_Earned)) %&gt;% summarize( total_points_possible = sum(Points_Possible, na.rm = TRUE), total_points_earned = sum(Points_Earned, na.rm = TRUE) ) %&gt;% mutate(percentage_earned = total_points_earned / total_points_possible) %&gt;% ungroup() %&gt;% # note that we join this back to the original data frame to retain all of the variables left_join(dat) 7.6.3 Finding distinct cases at the student-level This last step calculated a new column, for the percentage of points each student earned. That value is the same for the same student (an easy way we would potentially use to check this is View(), i.e., View(dat)). But–because we are not carrying out a finer-grained analysis using the Gradebook_Item–the duplicate rows are not necessary. We only want variables at the student-level (and not at the level of different gradebook items). We can do this using the distinct() function. This function takes the name of the data frame and the name of the variables used to determine what counts as a unique case. Imagine having a bucket of Halloween candy that has 100 pieces of candy. You know that these 100 pieces are really just a bunch of duplicate pieces from a relatively short list of candy brands. distinct takes that bucket of 100 pieces and returns a bucket containing only one of each distinct piece. Another thing to note about distinct() is that it will only return the variable(s) (we note that you can pass more than one variable to distinct()) you used to determine uniqueness, unless you include the argument .keep_all = TRUE. For the sake of making it very easy to view the output, we omit this argument (only for now). Were we to run distinct(dat, Gradebook_Item), what do you think would be returned? distinct(dat, Gradebook_Item) ## # A tibble: 31,539 x 22 ## student_id course_id total_points_po… total_points_ea… percentage_earn… ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 43146 FrScA-S2… 1821 1039 0.571 ## 2 43146 FrScA-S2… 1821 1039 0.571 ## 3 43146 FrScA-S2… 1821 1039 0.571 ## 4 43146 FrScA-S2… 1821 1039 0.571 ## 5 43146 FrScA-S2… 1821 1039 0.571 ## 6 43146 FrScA-S2… 1821 1039 0.571 ## 7 43146 FrScA-S2… 1821 1039 0.571 ## 8 43146 FrScA-S2… 1821 1039 0.571 ## 9 43146 FrScA-S2… 1821 1039 0.571 ## 10 43146 FrScA-S2… 1821 1039 0.571 ## # … with 31,529 more rows, and 17 more variables: subject &lt;chr&gt;, ## # semester &lt;chr&gt;, section &lt;chr&gt;, Points_Possible &lt;dbl&gt;, Points_Earned &lt;dbl&gt;, ## # Gender &lt;chr&gt;, q1 &lt;dbl&gt;, q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;, ## # q7 &lt;dbl&gt;, q8 &lt;dbl&gt;, q9 &lt;dbl&gt;, q10 &lt;dbl&gt;, TimeSpent &lt;dbl&gt; What is every distinct gradebook item is what is returned. You might be wondering (as we were) whether some gradebook items have the same values across courses; we can return the unique combination of courses and gradebook items by simply adding another variable to distinct(): distinct(dat, course_id, Gradebook_Item) ## # A tibble: 26 x 1 ## course_id ## &lt;chr&gt; ## 1 FrScA-S216-02 ## 2 OcnA-S116-01 ## 3 FrScA-S216-01 ## 4 OcnA-S216-01 ## 5 PhysA-S116-01 ## 6 FrScA-S216-03 ## 7 AnPhA-S216-01 ## 8 FrScA-S116-01 ## 9 FrScA-S116-02 ## 10 OcnA-S116-02 ## # … with 16 more rows It looks like a lot of gradebook items were repeated - likely across the different sections of the same course (we would be curious to hear what you find if you investigate this!). Let’s use what we just did, but to find the unique values at the student-level. Thus, instead of exploring unique gradebook items, we will explore unique students (still accounting for the course, as students could enroll in more than one course.) This time, we will add the keep_all = TRUE argument. dat &lt;- distinct(dat, course_id, student_id, .keep_all = TRUE) This is a much smaller data frame - with one row for each sudnet in the course (instead of the 29,701 rows which we would be interested in were we analyzing this data at the level of specific students’ grades for specific gradebook items). Now that our data are ready to go, we can start to ask some questions of the data, 7.7 Analysis In this section, we focus on some initial analyses in the form of visualizations and some models. We note that we expand on these in a [later chapter’(#c13). 7.7.1 The relationship between time spent on course and percentage of points earned One thing we might be wondering is how time spent on course is related to students’ final grade. We note that ggplot2, which we use to create these plots, is discussed further in chapter XXX. dat %&gt;% # aes() tells ggplot2 what variables to map to what feature of a plot # Here we map variables to the x- and y-axis ggplot(aes(x = TimeSpent, y = percentage_earned)) + # Creates a point with x- and y-axis coordinates specified above geom_point() + theme_dataedu() There appears to be some relationship. What if we added a line of best fit - a linear model? dat %&gt;% ggplot(aes(x = TimeSpent, y = percentage_earned)) + geom_point() + # same as above # this adds a line of best fit # method = &quot;lm&quot; tells ggplot2 to fit the line using linear regression geom_smooth(method = &quot;lm&quot;) + theme_dataedu() So, it appeares that the more time students spent on the course, the more points they earned. 7.7.2 Linear model (regression) We can find out exactly what the relationship is using a linear model. We also discuss linear models in walkthrough XXX. Let’s use this technique to model the relationship between the time spent on the course and the percentage of points earned. Here, we predict percentage_earned, or the percentage of the total points that are possible for a student to earn. Here, percentage earned is the dependent, or y-variable, and so we enter it first, after the lm() command, before the tilde (~) symbol. To the right of the tilde is one independent variable, TimeSpent, or the time that students spent on the course. We also pass the data frame, dat. At this point, we’re ready to run the model. Let’s run this line of code and save the results to an object - we chose m_linear, but any name will work, as well as the summary() function on the output. m_linear &lt;- lm(percentage_earned ~ TimeSpent, data = dat) Another way that we can generate table output is with a function from the sjPlot package, tab_model. sjPlot::tab_model(m_linear) percentage earned Predictors Estimates CI p (Intercept) 0.76 0.74 – 0.79 &lt;0.001 TimeSpent -0.00 -0.00 – 0.00 0.801 Observations 148 R2 / R2 adjusted 0.000 / -0.006 This will work well for R Markdown documents (or simply to interpet the model in R). If you want to save the model for use in a Word document, the apaTables package may be helpful. To do so, just pass the name of the regression model, like we did with sjPlot::tab_model(). Then, you can save the output to a Word document, simply by adding a filename argument: apaTables::apa.reg.table(m_linear, filename = &quot;regression-table-output.doc&quot;) You might be wondering what else the apaTables package does; we encourage you to read more about the package here: https://cran.r-project.org/web/packages/apaTables/index.html. The vignette is especially helpful. One function that may be useful for writing manuscripts is the following function for creating correlation tables; the function takes, as an input, a data frame with the variables for which you wish to calculate correlations. Before we proceed to the next code chunk, let’s talk about some functions we’ll be using a lot in this book. filter, group_by, and summarise are functions in the dplyr package that you will see a lot in upcoming chapters. filter removes rows from the dataset that don’t match a criteria. Use it for tasks like only keeping records for students in the fifth grade group_by groups records together so you can perform operations on those groups instead of on the entire dataset. Use it for tasks like getting the mean test score of each school instead of a whole school district summarize and summarise reduce your dataset down to a summary statistic. Use it for tasks like turning a datset of student test scores into a datset of grade levels and their mean test score So let’s use these dplyr functions on our survey analysis. We will create the same measures (based on the survey items) that we used earlier to understand how they relate to one another: survey_responses &lt;- pre_survey %&gt;% # Gather questions and responses pivot_longer(cols = q1:q10, names_to = &quot;question&quot;, values_to = &quot;response&quot;) %&gt;% mutate( # Here&#39;s where we make the column of question categories measure = case_when( question %in% c(&quot;q1&quot;, &quot;q4&quot;, &quot;q5&quot;, &quot;q8&quot;, &quot;q10&quot;) ~ &quot;int&quot;, question %in% c(&quot;q2&quot;, &quot;q6&quot;, &quot;q9&quot;) ~ &quot;uv&quot;, question %in% c(&quot;q3&quot;, &quot;q7&quot;) ~ &quot;pc&quot;, TRUE ~ NA_character_ ) ) %&gt;% group_by(student_id, measure) %&gt;% # Here&#39;s where we compute the mean of the responses summarise( # Mean response for each measure mean_response = mean(response, na.rm = TRUE) ) %&gt;% filter(!is.na(mean_response)) %&gt;% pivot_wider(names_from = measure, values_from = mean_response) survey_responses ## # A tibble: 615 x 4 ## # Groups: student_id [615] ## student_id int pc uv ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 430162 4 3.5 3.67 ## 2 430222 3.8 3.5 3.67 ## 3 431821 3.2 3.5 2.67 ## 4 431864 3.63 3.33 3.39 ## 5 431909 4.3 4 3 ## 6 431956 2.83 3.5 3 ## 7 431999 4.4 3.75 4.17 ## 8 432171 5 5 4.67 ## 9 432210 3.9 3.5 2.6 ## 10 432215 4 4 3 ## # … with 605 more rows Now that we’ve prepared the survey responses, we can use the apa.cor.table() function: survey_responses %&gt;% apa.cor.table() ## ## ## Means, standard deviations, and correlations with confidence intervals ## ## ## Variable M SD 1 2 3 ## 1. student_id 461004.34 28674.77 ## ## 2. int 4.25 0.60 -.15** ## [-.22, -.07] ## ## 3. pc 3.65 0.65 -.16** .61** ## [-.24, -.08] [.56, .66] ## ## 4. uv 3.75 0.74 -.13** .64** .56** ## [-.21, -.05] [.59, .69] [.51, .62] ## ## ## Note. M and SD are used to represent mean and standard deviation, respectively. ## Values in square brackets indicate the 95% confidence interval. ## The confidence interval is a plausible range of population correlations ## that could have caused the sample correlation (Cumming, 2014). ## * indicates p &lt; .05. ** indicates p &lt; .01. ## The time spent variable is on a very large scale (minutes); what if we transform it to represent the number of hours that students spent on the course? Let’s use the mutate() function we used earlier. We’ll end the variable name in _hours, to represent what this variable means. # creating a new variable for the amount of time spent in hours dat &lt;- dat %&gt;% mutate(TimeSpent_hours = TimeSpent / 60) # the same linear model as above, but with the TimeSpent variable in hours m_linear_1 &lt;- lm(percentage_earned ~ TimeSpent_hours, data = dat) # viewing the output of the linear model sjPlot::tab_model(m_linear_1) percentage earned Predictors Estimates CI p (Intercept) 0.76 0.74 – 0.79 &lt;0.001 TimeSpent_hours -0.00 -0.00 – 0.00 0.801 Observations 148 R2 / R2 adjusted 0.000 / -0.006 The scale still does not seem quite right. What if we standardized the variable to have a mean of zero and a standard deviation of one? # this is to standardize the TimeSpent variable to have a mean of zero and a standard deviation of 1 dat &lt;- dat %&gt;% mutate(TimeSpent_std = scale(TimeSpent)) # the same linear model as above, but with the TimeSpent variable standardized m_linear_2 &lt;- lm(percentage_earned ~ TimeSpent_std, data = dat) # viewing the output of the linear model sjPlot::tab_model(m_linear_2) percentage earned Predictors Estimates CI p (Intercept) 0.76 0.75 – 0.78 &lt;0.001 TimeSpent_std -0.00 -0.02 – 0.01 0.801 Observations 148 R2 / R2 adjusted 0.000 / -0.006 That seems to make more sense. However, there is a different interpretation now for the time spent variable: for every one standard deviation increase in the amount of time spent on the course, the percentage of points a student earns increases by .11, or 11 percentage points. 7.8 Results Let’s extend our regression model and consider the following to be the final model in this sequence of models: What other variables may matter? Perhaps there are differences based on the subject of the course. We can add subject as a variable easily, as follows: # a linear model with the subject added # independent variables, such as TimeSpent_std and subject, can simply be separated with a plus symbol: m_linear_3 &lt;- lm(percentage_earned ~ TimeSpent_std + subject, data = dat) We can use sjPlot::tab_model() once again to view the results: sjPlot::tab_model(m_linear_3) percentage earned Predictors Estimates CI p (Intercept) 0.75 0.72 – 0.78 &lt;0.001 TimeSpent_std 0.00 -0.02 – 0.02 0.957 subject [BioA] 0.04 -0.04 – 0.11 0.342 subject [FrScA] 0.00 -0.03 – 0.04 0.872 subject [OcnA] 0.02 -0.03 – 0.07 0.367 subject [PhysA] 0.03 -0.03 – 0.09 0.311 Observations 148 R2 / R2 adjusted 0.017 / -0.018 It looks like subject FrSc - forensic science - and subject Ocn - oceanography - are associated with a higher percentage of points earned, overall. This indicates that students in those two classes earned higher grades than students in other science classes in this dataset. 7.9 Conclusion In this walkthrough, we focused on taking unprocessed, or raw data, and loading, viewing, and then processing it through a series of steps. The result was a data set which we could use to create visualizations and a simple (but powerful!) model, a linear model, also known as a regression model. We found that the time that students spent on the course was positively (and statistically significantly) related to students’ final grades, and that there appeared to be differences by subject. While we focused on using this model in a traditional, explanatory sense, it could also (potentially) be used predictively, in that knowing how long students spent on the course and what subject their course is could be used to estimate what that students’ final grade might be. We focus on predictive uses of models further in chapter 14). Also, In the follow-up to this walkthrough (see chapter 13), we will focus on visualizing and then modeling the data using an advanced methodological technique, multi-level models, using the data we prepared as a part of this data processeing pipeline used in this chapter. write_csv(dat, &quot;data/online-science-motivation/processed/sci_mo_processed.csv&quot;) References "],
["c08.html", "8 Walkthrough 2: Approaching Gradebook Data From a Data Science Perspective 8.1 Introduction 8.2 Data Sources 8.3 Load Packages 8.4 Import Data 8.5 Tidy Data 8.6 Process Data 8.7 Visualize Data 8.8 Model Data", " 8 Walkthrough 2: Approaching Gradebook Data From a Data Science Perspective 8.1 Introduction There are a variety of data sources to explore in the education field. Student assessment scores can be examined for progress towards goals. The text from a teacher’s written classroom observation notes about a particular learner’s in-class behavior or emotional status can be analyzed for trends. We can tap into the exportable data available from common learning software or platforms popular in the K-12 education space. 8.2 Data Sources This walkthrough goes through a series of analyses using the data science framework. The first analysis centers around a ubiquitous K-12 classroom tool: the gradebook. We use an Excel gradebook template, Assessment Types - Points (https://web.mit.edu/jabbott/www/excelgradetracker.html), and simulated student data. On your first read through of this section try using our simulated dataset found in this book’s data/ folder. 8.3 Load Packages As mentioned in the Foundational Skills chapter, begin by loading the libraries that will be used. We will use the {tidyverse} package mentioned in Walkthrough 1. The {readxl} package is used to read and import Excel spreadsheets since these file types are very common in the education field. Make sure you have installed the packages in R on your computer before starting (see Foundational Skills chapter). Load the libraries, as they must be loaded each time we start a new project. # Load libraries library(tidyverse) library(here) library(readxl) library(janitor) library(dataedu) 8.4 Import Data Recall how the Foundational Skills chapter recommended favoring CSV files, or comma-separated values files, when working with datasets in R. This is because CSV files, with the .csv file extension, are common in the digital world. However, data won’t always come in the preferred file format. Fortunately, R can import a variety of data file types. This walkthrough imports an Excel file because these file types, with the .xlsx or .xls extensions, are very likely to be encountered in the K-12 education world. This code uses the read_excel() function of the {readxl} package to find and read the data of the desired file. Note the file path that read_excel() takes to find the simulated dataset file named ExcelGradeBook.xlsx which sits in a folder on your computer if you have downloaded it. The function getwd() will help locate your current working directory. This tells where on the computer R is currently working with files. # See the current working directory getwd() For example, an R user on Linux or Mac might see their working directory as: /home/username/Desktop. A Windows user might see their working directory as: C:\\Users\\Username\\Desktop. From this location go deeper into files to find the desired file. For example, if you downloaded the book repository from Github to your Desktop the path to the Excel file might look like one of these below: /home/username/Desktop/data-science-in-education/data/gradebooks/ExcelGradeBook.xlsx (on Linux &amp; Mac) C:\\Users\\Username\\Desktop\\data-science-in-education\\data\\gradebooks\\ExcelGradeBook.xlsx (on Windows) In the code below ExcelGradeBook &lt;- read_excel(\"path/to/file.xlsx\", sheet = 1, skip = 10) the part written path/to/file.xlsx is just pseudo code to remind you to swap in your own path to the file you want to import. Recall from the Foundational Skills section of this book that directories and file paths are important for finding files on your computer. After locating the sample Excel file run the code below to run the function read_excel() which reads and saves the data from ExcelGradeBook.xlsx to an object also called ExcelGradeBook. Note the two arguments specified in this code sheet = 1 and skip = 10. This Excel file is similar to one you might encounter in real life with superfluous features that we are not interested in. This file has 3 different sheets and the first 10 rows contain things we won’t need. Thus, sheet = 1 tells read_excel() to just read the first sheet in the file and disregard the rest. While skip = 10 tells read_excel() to skip reading the first 10 rows of the sheet and start reading from row 11 which is where the column headers and data actually start inside the Excel file. Remember to replace path/to/file.xlsx your own path to the file you want to import. # Use readxl package to read and import file and assign it a name ExcelGradeBook &lt;- read_excel( here::here(&quot;data&quot;, &quot;gradebooks&quot;, &quot;ExcelGradeBook.xlsx&quot;), sheet = 1, skip = 10 ) The ExcelGradeBook file has been imported into RStudio. Next, assign the data frame to a new name using the code below. Renaming cumbersome filenames can improve the readability of the code and make is easier for the user to call on the dataset later on in the code. # Rename data frame gradebook &lt;- ExcelGradeBook Your environment will now have two versions of the dataset. There is ExcelGradeBook which was the original dataset imported. There is also gradebook which is currently a copy of ExcelGradeBook. As you progress through this section, we will work primarily with the gradebook dataset. Additionally, while working onward in this section of the book, if you make a mistake and mess up the gradebook data frame and are not able to fix it, you can reset the data frame to return to the same state as the original ExcelGradeBook data frame by running gradebook &lt;- ExcelGradeBook again. This will overwrite your messed up gradebook data frame with the originally imported ExcelGradeBook data frame. Afterwards, just continue running code from this point in the text. 8.5 Tidy Data This walkthrough uses an Excel data file because it is one that we are likely to encounter. Moreover, the messy state of this file mirrors what might be encountered in real life. The Excel file contains more than one sheet, has rows we don’t need, and uses column names that have spaces between words. All these things make the data tough to work with. The data is not tidy. We can begin to overcome these challenges before importing the file into RStudio by deleting the unnecessary parts of the Excel file then saving it as a .csv file. However, if you clean the file outside of R, this means if you ever have to clean it up (say, if the dataset is accidentally deleted and you need to redownload it from the original source) you would have to do everything from the beginning. We recommend cleaning the original data in R so that you can recreate all the steps necessary for your analysis. Also, the untidy Excel file provides realistic practice for tidying up the data programmatically using R itself. First, modify the column names of the gradebook data frame to remove any spaces and replace them with an underscore. Using spaces as column names in R can present difficulties later on when working with the data. Second, we want the column names of our data to be easy to use and understand. The original dataset has column names with uppercase letters and spaces. We can use the janitor package to quickly change them to a more useable format. colnames(gradebook) # look at original column names ## [1] &quot;Class&quot; ## [2] &quot;Name&quot; ## [3] &quot;Race&quot; ## [4] &quot;Gender&quot; ## [5] &quot;Age&quot; ## [6] &quot;Repeated Grades&quot; ## [7] &quot;Financial Status&quot; ## [8] &quot;Absent&quot; ## [9] &quot;Late&quot; ## [10] &quot;Make your own categories&quot; ## [11] &quot;Running Average&quot; ## [12] &quot;Letter Grade&quot; ## [13] &quot;Homeworks&quot; ## [14] &quot;Classworks&quot; ## [15] &quot;Formative Assessments&quot; ## [16] &quot;Projects&quot; ## [17] &quot;Summative Assessments&quot; ## [18] &quot;Another Type 2&quot; ## [19] &quot;Classwork 1&quot; ## [20] &quot;Homework 1&quot; ## [21] &quot;Classwork 2&quot; ## [22] &quot;Homework 2&quot; ## [23] &quot;Classwork 3&quot; ## [24] &quot;Classwork 4&quot; ## [25] &quot;Classwork 5&quot; ## [26] &quot;Classwork 6&quot; ## [27] &quot;Homework 3&quot; ## [28] &quot;Formative Assessment 1&quot; ## [29] &quot;Project 1&quot; ## [30] &quot;Classwork 7&quot; ## [31] &quot;Homework 4&quot; ## [32] &quot;Project 2&quot; ## [33] &quot;Classwork 8&quot; ## [34] &quot;Homework 5&quot; ## [35] &quot;Project 3&quot; ## [36] &quot;Homework 6&quot; ## [37] &quot;Classwork 9&quot; ## [38] &quot;Homework 7&quot; ## [39] &quot;Homework 8&quot; ## [40] &quot;Project 4&quot; ## [41] &quot;Project 5&quot; ## [42] &quot;Formative Assessment 2&quot; ## [43] &quot;Project 6&quot; ## [44] &quot;Classwork 10&quot; ## [45] &quot;Homework 9&quot; ## [46] &quot;Classwork 11&quot; ## [47] &quot;Homework 10&quot; ## [48] &quot;Classwork 12&quot; ## [49] &quot;Classwork 13&quot; ## [50] &quot;Project 7&quot; ## [51] &quot;Classwork 14&quot; ## [52] &quot;Classwork 15&quot; ## [53] &quot;Homework 11&quot; ## [54] &quot;Summative Assessment 1&quot; ## [55] &quot;Classwork 16&quot; ## [56] &quot;Homework 12&quot; ## [57] &quot;Classwork 17&quot; ## [58] &quot;Homework 13&quot; ## [59] &quot;Project 8&quot; ## [60] &quot;Project 9&quot; ## [61] &quot;Project 10&quot; ## [62] &quot;Summative Assessment 2&quot; ## [63] &quot;Assessment | Insert new columns before here&quot; gradebook &lt;- gradebook %&gt;% clean_names() colnames(gradebook) # look at cleaned column names ## [1] &quot;class&quot; ## [2] &quot;name&quot; ## [3] &quot;race&quot; ## [4] &quot;gender&quot; ## [5] &quot;age&quot; ## [6] &quot;repeated_grades&quot; ## [7] &quot;financial_status&quot; ## [8] &quot;absent&quot; ## [9] &quot;late&quot; ## [10] &quot;make_your_own_categories&quot; ## [11] &quot;running_average&quot; ## [12] &quot;letter_grade&quot; ## [13] &quot;homeworks&quot; ## [14] &quot;classworks&quot; ## [15] &quot;formative_assessments&quot; ## [16] &quot;projects&quot; ## [17] &quot;summative_assessments&quot; ## [18] &quot;another_type_2&quot; ## [19] &quot;classwork_1&quot; ## [20] &quot;homework_1&quot; ## [21] &quot;classwork_2&quot; ## [22] &quot;homework_2&quot; ## [23] &quot;classwork_3&quot; ## [24] &quot;classwork_4&quot; ## [25] &quot;classwork_5&quot; ## [26] &quot;classwork_6&quot; ## [27] &quot;homework_3&quot; ## [28] &quot;formative_assessment_1&quot; ## [29] &quot;project_1&quot; ## [30] &quot;classwork_7&quot; ## [31] &quot;homework_4&quot; ## [32] &quot;project_2&quot; ## [33] &quot;classwork_8&quot; ## [34] &quot;homework_5&quot; ## [35] &quot;project_3&quot; ## [36] &quot;homework_6&quot; ## [37] &quot;classwork_9&quot; ## [38] &quot;homework_7&quot; ## [39] &quot;homework_8&quot; ## [40] &quot;project_4&quot; ## [41] &quot;project_5&quot; ## [42] &quot;formative_assessment_2&quot; ## [43] &quot;project_6&quot; ## [44] &quot;classwork_10&quot; ## [45] &quot;homework_9&quot; ## [46] &quot;classwork_11&quot; ## [47] &quot;homework_10&quot; ## [48] &quot;classwork_12&quot; ## [49] &quot;classwork_13&quot; ## [50] &quot;project_7&quot; ## [51] &quot;classwork_14&quot; ## [52] &quot;classwork_15&quot; ## [53] &quot;homework_11&quot; ## [54] &quot;summative_assessment_1&quot; ## [55] &quot;classwork_16&quot; ## [56] &quot;homework_12&quot; ## [57] &quot;classwork_17&quot; ## [58] &quot;homework_13&quot; ## [59] &quot;project_8&quot; ## [60] &quot;project_9&quot; ## [61] &quot;project_10&quot; ## [62] &quot;summative_assessment_2&quot; ## [63] &quot;assessment_insert_new_columns_before_here&quot; Review what the gradebook data frame looks like now. It shows 25 students and their individual values in various columns like projects or formative_assessments. view(gradebook) The data frame looks cleaner now but there still are some things we can remove. For example, there are rows without any names in them. Also, there are entire columns that are unused and contain no data (such as gender). These are called missing values and are denoted by NA. Since our simulated classroom only has 25 learners and doesn’t use all the columns for demographic information, we can safely remove these to tidy our dataset up even more. We can remove the extra columns rows that have no data using the janitor package. The handy remove_empty() removes columns, rows, or both that have no information in them. # Removing rows with nothing but missing data gradebook &lt;- gradebook %&gt;% remove_empty(c(&quot;rows&quot;, &quot;cols&quot;)) Now that the empty rows and columns have been removed, notice there are two columns, absent and late, where it seems someone started putting data into but then decided to stop. These two columns didn’t get removed by the last chunk of code because they technically contained some data in those columns. Since the simulated data enterer of this simulated class decided to abandon using the absent and late columns in this gradebook, we can remove it from our data frame as well. In the foundational skills chapter we introduced the select() function, which tells R which columns we want to keep. Let’s do that again here. This time we’ll use negative signs to say we want the dataset without absent and late. # Remove a targeted column because we don&#39;t use absent and late at this school. gradebook &lt;- gradebook %&gt;% select(-absent, -late) At last, the formerly untidy Excel sheet has been turned into a useful data frame. Inspect it once more to see the difference. view(gradebook) 8.6 Process Data R users transform data to facilitate working with the data during later phases of visualization and analysis. A few examples of data transformation include creating new variables, grouping data, and more. This code chunk first creates a new data frame named classwork_df, then selects particular variables from our gradebook dataset using select(), and finally gathers all the homework data under new variables into new columns. As mentioned previously, select() is very powerful. In addition to explicitly writing out the columns you want to keep, you can also use functions from the package {stringr} within select(). The {stringr} package is within {tidyverse}. Here, we’ll use the function contains() to tell R to select columns that contain a certain string (that is, text). Here, it searches for any column with the string “classwork_”. The underscore makes sure the variables from classwork_1 all the way to classwork_17 are included in classwork_df. pivot_longer() transforms the dataset into tidy data. Note that scores are in character format. We use mutate() to transform them to numerical format. # Creates new data frame, selects desired variables from gradebook, and gathers all classwork scores into key/value pairs classwork_df &lt;- gradebook %&gt;% select( name, running_average, letter_grade, homeworks, classworks, formative_assessments, projects, summative_assessments, contains(&quot;classwork_&quot;) ) %&gt;% mutate_at(vars(contains(&quot;classwork_&quot;)), list(~ as.numeric(.))) %&gt;% pivot_longer( cols = contains(&quot;classwork_&quot;), names_to = &quot;classwork_number&quot;, values_to = &quot;score&quot; ) View the new data frame and note which columns were selected for this new data frame. Also, note how all the classwork scores were gathered under new columns classwork_number and score. The contains() function. We will use this classwork_df data frame later. view(classwork_df) 8.7 Visualize Data Visual representations of data are more human friendly than just looking at numbers alone. This next line of code shows a simple summary of the data by each column similar to what we did in Walkthrough 1. # Summary of the data by columns summary(gradebook) But R can do more than just print numbers to a screen. Use the {ggplot} package within {tidyverse} to graph some of the data to help get a better grasp of what the data looks like. This code uses {ggplot} to graph categorical variables into a bar graph. Here we can see the variable Letter_grade is plotted on the x-axis showing the counts of each letter grade on the y-axis. # Bar graph for categorical variable gradebook %&gt;% ggplot(aes(x = letter_grade, fill = running_average &gt; 90)) + geom_bar() + labs(title = &quot;Bar Graph of Student Grades&quot;, x = &quot;Letter Grades&quot;, y = &quot;Count&quot;, fill = &quot;A or Better&quot;) + theme_dataedu() + scale_fill_dataedu() Using {ggplot}, we can create many types of graphs. Using our classwork_df from earlier, we can see the distribution of scores and how they differ from classwork to classwork using boxplots. We are able to do this because we have made the classworks and scores columns into tidy formats. # Scatterplot of continuous variable classwork_df %&gt;% ggplot(aes(x = classwork_number, y = score, fill = classwork_number)) + geom_boxplot() + labs( title = &quot;Distribution of Classwork Scores&quot;, x = &quot;Classwork&quot;, y = &quot;Scores&quot;, color = &quot;90% or Above&quot; ) + theme_dataedu() + scale_fill_dataedu() + theme(legend.position = &quot;none&quot;, # removes legend axis.text.x = element_text(angle = 45, hjust = 1)) # angles the x axis labels 8.8 Model Data 8.8.1 Deciding an Analysis Using this spreadsheet, we can start to form hypotheses about the data. For example, we can ask ourselves, “Can we predict overall grade using formative assessment scores?” For this, we will try to predict a response variable Y (overall grade) as a function of a predictor variable Y (formative assessment scores). The goal is to create a mathematical equation for overall grade as a function of formative assessment scores when only formative assessment scores are known. 8.8.2 Visualize Data to Check Assumptions It’s important to visualize data to see any distributions, trends, or patterns before building a model. We use {ggplot} to understand these variables graphically. 8.8.2.1 Linearity First, we plot X and Y to determine if we can see a linear relationship between the predictor and response. The x-axis shows the formative assessment scores while the y-axis shows the overall grades. The graph suggests a correlation between overall class grade and formative assessment scores. As the formative scores goes up, the overall grade goes up too. # Scatterplot between formative assessment and grades by percent # To determine linear relationship gradebook %&gt;% ggplot(aes(x = formative_assessments, y = running_average)) + geom_point() + labs(title = &quot;Relationship Between Overall Grade and Formative Assessments&quot;, x = &quot;Formative Assessment Score&quot;, y = &quot;Overall Grade in Percentage&quot;) + theme_dataedu() We can layer different types of plots on top of each other in {ggplot}. Here the scatterplot is layered with a line of best fit, that suggests a positive linear relationship. # Scatterplot between formative assessment and grades by percent # To determine linear relationship # With line of best fit gradebook %&gt;% ggplot(aes(x = formative_assessments, y = running_average)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = TRUE) + labs(title = &quot;Relationship Between Overall Grade and Formative Assessments&quot;, x = &quot;Formative Assessment Score&quot;, y = &quot;Overall Grade in Percentage&quot;) + theme_dataedu() 8.8.2.2 Outliers Now we use boxplots to determine if there are any outliers in formative assessment scores or overall grades. For linear regression, we’re hoping to see no outliers in the data. We don’t see any for these two variables, so we can proceed with the model. # Boxplot of formative assessment scores # To determine if there are any outliers gradebook %&gt;% ggplot(aes(x = &quot;&quot;, y = formative_assessments)) + geom_boxplot() + labs(title = &quot;Distribution of Formative Assessment Scores&quot;, x = &quot;Formative Assessment&quot;, y = &quot;Score&quot;) + theme_dataedu() # Boxplot of overall grade scores in percentage # To determine if there are any outliers gradebook %&gt;% ggplot(aes(x = &quot;&quot;, y = running_average)) + geom_boxplot() + labs(title = &quot;Distribution of Overall Grade Scores&quot;, x = &quot;Overall Grade&quot;, y = &quot;Score in Percentage&quot;) + theme_dataedu() 8.8.3 Correlation Analysis We want to know the strength of the relationship between the two variables formative assessment scores and overall grade percentage. The strength is denoted by the “correlation coefficient.” The correlation coefficient goes from -1 to 1. If one variable consistently increases with the increasing value of the other, then they have a positive correlation (towards 1). If one variable consistently decreases with the increasing value of the other, then they have a negative correlation (towards -1). If the correlation coefficient is 0, then there is no relationship between the two variables. Correlation is good for finding relationships but it does not imply that one variable causes the other (correlation does not mean causation). cor(gradebook$formative_assessments, gradebook$running_average) ## [1] 0.6632553 8.8.4 Build Linear Model In the multi-level modeling walkthrough, we introduced the concept of linear models. Let’s use that same technique here. Now that you’ve checked your assumptions and seen a linear relationship, we can build a linear model, that is, a mathematical formula that calculates your running average as a function of your formative assessment score. This is done using the lm() function, where the arguments are: Your predictor (formative_assessments) Your response (running_average) The data (gradebook) linear_mod &lt;- lm(running_average ~ formative_assessments, data = gradebook) summary(linear_mod) ## ## Call: ## lm(formula = running_average ~ formative_assessments, data = gradebook) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.2814 -2.7925 -0.0129 3.3179 8.5353 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.11511 8.54774 5.863 5.64e-06 *** ## formative_assessments 0.42136 0.09914 4.250 0.000302 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.657 on 23 degrees of freedom ## Multiple R-squared: 0.4399, Adjusted R-squared: 0.4156 ## F-statistic: 18.06 on 1 and 23 DF, p-value: 0.0003018 The formula reads as follows: running_average = 50.11511 + 0.42136*formative_assessments "],
["c09.html", "9 Walkthrough 3: Introduction to Aggregate Data 9.1 Introduction 9.2 Data Sources 9.3 Load Packages 9.4 Import Data 9.5 View Data 9.6 Analysis 9.7 Results 9.8 Conclusion 9.9 References", " 9 Walkthrough 3: Introduction to Aggregate Data 9.1 Introduction 9.1.1 Background A common situation encountered when using data for analyzing the education sector, particularly by analysts who are not directly working with schools or districts, is the prevalence of publicly available, aggregate data. Aggregate data refers to numerical or non-numerical information that is (1) collected from multiple sources and/or on multiple measures, variables, or individuals and (2) compiled into data summaries or summary reports, typically for the purposes of public reporting or statistical analysis (Schools, n.d.). Example of publicly available aggregate data include school-level graduation rates, state test proficiency scores by grade and subject, or averaged survey responses. Aggregated datasets are essential both for accountability purposes and for providing useful information about schools and districts to those who are monitoring them. For example, district administrators might aggregate row-level (also known as individual-level or student-level) enrollment reports over time. This allows them to see how many students enroll in each school, in the district overall, and any grade-level variation. Depending on their state, the district administrator might submit these aggregate data to their state education agency for reporting purposes. These datasets might be posted on the state’s department of education website for anybody to download and use. Federal and international education datasets provide additional context for evaluating education systems. In the US, some federal datasets aim to consolidate important metrics from all states. This can be quite useful because each state has its own repository of data and to go through each state to download a particular metric is a significant effort. The federal government also funds assessments and surveys which are disseminated to the public. However, the federal datasets often have more stringent data requirements than the states, so the datasets may be less usable. For education data practitioners, these reports and datasets can be analyzed to answer questions related to their field of interest. However, publicly available, aggregate datasets are large and often suppressed to protect privacy. Sometimes they are already a couple of years old by the time they’re released. Because of their coarseness, they can be difficult to interpret and use. Generally, aggregated data are generally used to surface of broader trends and patterns in education as opposed to diagnosing underlying issues or making causal statements. It is very important that we consider the limitations of aggregate data before analyzing them. Analysis of aggregate data can help us identify patterns that may not have previously been known. When we have gained new insight, we can create research questions, craft hypotheses around our findings, and make recommendations on how to make improvements for the future. 9.1.1.1 Disaggregating Aggregated Data Aggregated data can tell us many things, but in order for us to examine subgroups and their information, we must have data disaggregated by the subgroups we hope to analyze. This data is still aggregated from row-level data but provides data on smaller components than the grand total. Common disaggregations for students include gender, race/ethnicity, socioeconomic status, English learner designation, and whether they are served under the Individuals with Disabilities Education Act (IDEA). 9.1.1.2 Disaggregating Data and Equity Disaggregated data is essential to monitor equity in educational resources and outcomes. If only aggregate data is provided, we are unable to distinguish how different groups of students are doing and what support they need. With disaggregated data, we can identify where solutions are needed to solve disparities in opportunity, resources, and treatment. It is important to define what equity means to your team so you know whether you are meeting your equity goals. 9.2 Data Sources There are many education-related, publicly available aggregate datasets. On the international level, perhaps the most well known is: Programme for International Student Assessment (PISA), which measures 15-year-old school pupils’ scholastic performance on mathematics, science, and reading. On the federal level, examples include: Civil Rights Data Collection (CRDC), which reports many different variables on educational program and services disaggregated by race/ethnicity, sex, limited English proficiency, and disability. These data are school-level. Common Core of Data (CCD), which is the US Department of Education’s primary database on public elementary and secondary education. EdFacts, which includes state assessments and adjusted cohort graduation rates. These data are school- and district-level. Integrated Postsecondary Education Data System (IPEDS), which is the US Department of Education’s primary database on postsecondary education. National Assessment for Educational Progress (NAEP) Data, an assessment of educational progress in the United States. Often called the “nation’s report card.” The NAEP reading and mathematics assessments are administered to a representative sample of fourth- and eighth-grade students in each state every two years. On the state and district level, examples include: California Department of Education, which is the state department of education website. It includes both downloadable CSV files and “Data Quest”, which lets you query the data online. Minneapolis Public Schools, which is a district-level website with datasets beyond those listed in the state website. 9.2.0.1 Selecting Data For the purposes of this walkthrough, we will be looking at a particular school district’s data. This district reports their student demographics in a robust, complete way. Not only do they report the percentage of students in a subgroup, but they also include the number of students in each subgroup. This allows a deep look into their individual school demographics. Their reporting of the composition of their schools provides an excellent opportunity to explore inequities in a system. 9.2.1 Methods In this chapter, we will walk through how running analyses on a publicly available dataset can help education data practitioners understand the landscape of needs and opportunities in the field of education. As opposed to causal analysis, which aims to assess the root cause of an phenomenon or the effects of an intervention, we use analysis on an aggregate dataset to find out whether there is a phenomenon, what it is, and what we’d be trying to solve through interventions. 9.3 Load Packages As usual, we begin our code by calling the libraries we will use. library(tidyverse) library(janitor) library(dataedu) ROpenSci created the {tabulizer} package which provides R bindings to the Tabula java library, which can be used to computationaly extract tables from PDF documents. {RJava} is a required package to load {tabulizer}. Unfortunately, installing {RJava} on Macs can be very tedious. If you find yourself unable to install {tabulizer}, or would like to skip to the data processing, the data pulled from the PDFs is also saved so we can skip the steps requiring {RJava}. library(tabulizer) 9.4 Import Data {tabulizer} pulls the PDF data into lists using extract_tables(). # race data race_pdf &lt;- extract_tables(&quot;https://studentaccounting.mpls.k12.mn.us/uploads/mps_fall2018_racial_ethnic_by_school_by_grade.pdf&quot;) It is important to consistently check what we’re doing with the actual PDF’s to ensure we’re getting the data that we need. We then transform the list to a data frame with map(as_tibble). The slice() in map_df() removes unnecessary rows from the PDF. Now we create readable column names using set_names(). race_df &lt;- race_pdf %&gt;% # turn each page into a tibble map(as_tibble) %&gt;% # remove unnecessary rows map_df( ~ slice(.,-1:-2)) %&gt;% # use descriptive column names set_names( c( &quot;school_group&quot;, &quot;school_name&quot;, &quot;grade&quot;, &quot;na_num&quot;, # native american number of students &quot;na_pct&quot;, # native american percentage of students &quot;aa_num&quot;, # african american number of students &quot;aa_pct&quot;, # african american percentage &quot;as_num&quot;, # asian number of students &quot;as_pct&quot;, # asian percentage &quot;hi_num&quot;, # hispanic number of students &quot;hi_pct&quot;, # hispanic percentage &quot;wh_num&quot;, # white number of students &quot;wh_pct&quot;, # white percentage &quot;pi_pct&quot;, # pacific islander percentage &quot;blank_col&quot;, &quot;tot&quot; # total number of students ) ) For the Race/Ethnicity table, we want the totals for each district school as we won’t be looking at grade-level variation. When analyzing the PDF, we see the totals have “Total” in the School Name. We clean up this dataset by: Removing unnecessary or blank columns using select(). Negative selections means those columns will be removed. Removing all Grand Total rows (otherwise they’ll show up in our data, when we just want district-level data) using filter(). We keep schools that have “Total” in the name but remove any rows that are Grand Total. Then we trim white space from strings. The data in the “percentage” columns are provided with a percentage sign. This means we will have to remove all of them to be able to do math on these columns (for example, adding them together). Also, we want to divide the numbers by 100 so they are true percentages. race_clean &lt;- race_df %&gt;% # remove unnecessary columns select(-school_group,-grade,-pi_pct,-blank_col) %&gt;% # filter to get grade-level numbers filter(str_detect(school_name, &quot;Total&quot;), school_name != &quot;Grand Total&quot;) %&gt;% # clean up school names mutate(school_name = str_replace(school_name, &quot;Total&quot;, &quot;&quot;)) %&gt;% # remove white space mutate_if(is.character, trimws) %&gt;% # turn numbers into percentages mutate_at(vars(contains(&quot;pct&quot;)), list( ~ as.numeric(str_replace(., &quot;%&quot;, &quot;&quot;)) / 100)) We will import the Free Reduced Price Lunch (FRPL) PDF’s now. FRPL stands for Free/Reduced Price Lunch, [often used as a proxy for poverty]((https://nces.ed.gov/blogs/nces/post/free-or-reduced-price-lunch-a-proxy-for-poverty). Students from a household with an income up to 185 percent of the poverty threshold are eligible for free or reduced price lunch. (Sidenote: Definitions are very important in disaggregated data. FRPL is used because it’s ubiquitous and reporting is mandated but there is debate as to whether it actually reflects the level of poverty among students.) frpl_pdf &lt;- extract_tables(&quot;https://studentaccounting.mpls.k12.mn.us/uploads/fall_2018_meal_eligiblity_official.pdf&quot;) Similar to the Race/Ethnicity PDF, there are rows that we don’t need from each page, which we remove using slice(). frpl_df &lt;- frpl_pdf %&gt;% map(as_tibble) %&gt;% map_df( ~ slice(.,-1)) %&gt;% set_names( c( &quot;school_name&quot;, &quot;not_eligible_num&quot;, # number of non-eligible students, &quot;reduce_num&quot;, # number of students receiving reduced price lunch &quot;free_num&quot;, # number of students receiving free lunch &quot;frpl_num&quot;, # total number of students &quot;frpl_pct&quot; # free/reduced price lunch percentage ) ) To clean it up, we remove the rows that are blank. When looking at the PDF, we notice that there are aggregations inserted into the table that are not district-level. For example, they have included ELM K_08, presumably to aggregate FRPL numbers up to the K-8 level. Although this is useful data, we don’t need it for this district-level analysis. There are different ways we can remove these rows but we will just filter them out. frpl_clean &lt;- frpl_df %&gt;% filter( # remove blanks school_name != &quot;&quot;, # filter out the rows in this list school_name %in% c( &quot;ELM K_08&quot;, &quot;Mid Schl&quot;, &quot;High Schl&quot;, &quot;Alt HS&quot;, &quot;Spec Ed Total&quot;, &quot;Cont Alt Total&quot;, &quot;Hospital Sites Total&quot;, &quot;Dist Total&quot; ) ) %&gt;% # turn numbers into percentages mutate(frpl_pct = as.numeric(str_replace(frpl_pct, &quot;%&quot;, &quot;&quot;)) / 100) Because we want to look at race/ethnicity data in conjunction with free/reduced price lunch percentage, we join the two datasets by the name of the school. Use mutate_at() to specify columns to which to apply a function (in this case, as.numeric.) # create full dataset, joined by school name joined_df &lt;- left_join(race_clean, frpl_clean, by = c(&quot;school_name&quot;)) %&gt;% mutate_at(2:17, as.numeric) Did you notice? The total number of students from the Race/Ethnicity table does not match the total number of students from the FRPL table, even though they’re referring to the same districts in the same year. Why? Perhaps the two datasets were created by different people, who used different rules when aggregating the dataset. Perhaps the counts were taken at different times of the year, when students may have moved around in the meantime. We don’t know but it does require us to make strategic decisions about which data we consider the ‘truth’ for our analysis. Now we move on to the fun part of creating new columns based on the merged dataset. We want to calculate, for each race, the number of students in ‘high poverty’ schools. This is defined by NCES as schools that are over 75% FRPL. When a school has over 75% FRPL, we count the number of students for that particular race under the variable [racename]_povnum. The {janitor} package has a handy adorn_totals() function that sums the columns for you. This is important because we want a weighted average of students in each category, so we need the total number of students in each group. We create the weighted average of the percentage of each race by dividing the number of students by race by the total number of students. To get FRPL percentage for all schools, we have to recalculate frpl_pct. To calculate the percentage of students by race who are in high poverty schools, we must divide the number of students in high poverty schools by the total number of students in that race. merged_df &lt;- joined_df %&gt;% # calculate high poverty numbers mutate( hi_povnum = case_when(frpl_pct &gt; .75 ~ hi_num), aa_povnum = case_when(frpl_pct &gt; .75 ~ aa_num), wh_povnum = case_when(frpl_pct &gt; .75 ~ wh_num), as_povnum = case_when(frpl_pct &gt; .75 ~ as_num), na_povnum = case_when(frpl_pct &gt; .75 ~ na_num) ) %&gt;% adorn_totals() %&gt;% # create percentage by demographic mutate( na_pct = na_num / tot, aa_pct = aa_num / tot, as_pct = as_num / tot, hi_pct = hi_num / tot, wh_pct = wh_num / tot, frpl_pct = (free_num + reduce_num) / frpl_num, # create percentage by demographic and poverty hi_povsch = hi_povnum / hi_num[which(school_name == &quot;Total&quot;)], aa_povsch = aa_povnum / aa_num[which(school_name == &quot;Total&quot;)], as_povsch = as_povnum / as_num[which(school_name == &quot;Total&quot;)], wh_povsch = wh_povnum / wh_num[which(school_name == &quot;Total&quot;)], na_povsch = na_povnum / na_num[which(school_name == &quot;Total&quot;)] ) To facilitate the creation of ggplots later on, we put this data in tidy format. tidy_df &lt;- merged_df %&gt;% pivot_longer(cols = -matches(&quot;school_name&quot;), names_to = &quot;category&quot;, values_to = &quot;value&quot;) Running the above code, particularly the download of the PDFs, takes a lot of time. We’ve saved copies of the merged and tidy data in the data folder and can be accessed by running the code below. tidy_df &lt;- read_csv(here::here(&quot;data&quot;, &quot;wt05_agg_data&quot;, &quot;wt05_aggdat_tidy_dat.csv&quot;)) merged_df &lt;- read_csv(here::here(&quot;data&quot;, &quot;wt05_agg_data&quot;, &quot;wt05_aggdat_merged_dat.csv&quot;)) 9.5 View Data 9.5.1 Discovering Distributions What do the racial demographics in this district look like? For this, a barplot can quickly visualize the different proportion of subgroups. tidy_df %&gt;% # filter out Total rows, since we want district-level information filter(school_name == &quot;Total&quot;, str_detect(category, &quot;pct&quot;), category != &quot;frpl_pct&quot;) %&gt;% # reordering x axis so bars appear by descending value ggplot(aes(x = reorder(category, -value), y = value)) + geom_bar(stat = &quot;identity&quot;, aes(fill = category)) + xlab(&quot;Subgroup&quot;) + ylab(&quot;Percentage of Population&quot;) + scale_x_discrete(labels = c(&quot;Black&quot;, &quot;Whifite&quot;, &quot;Hispanic&quot;, &quot;Asian&quot;, &quot;Native Am.&quot;)) + scale_y_continuous(labels = scales::percent) + scale_fill_dataedu() + theme_dataedu() + theme(legend.position = &quot;none&quot;) When we look at these data, the district looks very diverse. Almost 40% of students are Black and around 36% are White. Note: this matches the percentages provided in the original PDF’s. This shows our calculations above were accurate. Hooray! In terms of free/reduced price lunch, we have that calculated under frpl_pct: tidy_df %&gt;% filter(category == &quot;frpl_pct&quot;, school_name == &quot;Total&quot;) %&gt;% knitr::kable() school_name category value Total frpl_pct 0.5685631 56.9% of the students are eligible for FRPL, compared to the US average of 52.1%. This also matches the PDF’s. Great! Now, we dig deeper to see if there is more to the story. 9.5.2 Anayzing Spread Another view of the data can be visualizing the distribution of students with different demographics across schools. Here is a histogram for the percentage of White students within the schools for which we have data. merged_df %&gt;% filter(school_name != &quot;Total&quot;) %&gt;% ggplot(aes(x = wh_pct)) + geom_histogram(breaks = seq(0, 1, by = .1), fill = dataedu_cols(&quot;darkblue&quot;)) + xlab(&quot;White Percentage&quot;) + ylab(&quot;Count&quot;) + scale_x_continuous(labels = scales::percent) + theme(legend.position = &quot;none&quot;) + theme_dataedu() # hist(merged_df$wh_pct)$counts[1:9] # counting number of schools in the first bin (0-10%) 26 of the 74 (35%) of schools have between 0-10% White students. This implies that even though the school district may be diverse, the demographics are not evenly distributed across the schools. More than half of schools enroll fewer than 30% of White students even though White students make up 35% of the district student population. The school race demographics are not representative of the district populations but does that hold for socioeconomic status as well? 9.6 Analysis 9.6.1 Creating Categories High-poverty schools are defined as public schools where more than 75% of the students are eligible for FRPL. According to NCES, 24% of public school students attended high-poverty schools. However, different subgroups were overrepresented and underrepresented within the high poverty schools. Is this the case for this district? tidy_df %&gt;% filter(school_name == &quot;Total&quot;, str_detect(category, &quot;povsch&quot;)) %&gt;% ggplot(aes(x = reorder(category,-value), y = value)) + geom_bar(stat = &quot;identity&quot;, aes(fill = factor(category))) + theme_dataedu() + xlab(&quot;Subgroup&quot;) + ylab(&quot;Percentage in High Poverty Schools&quot;) + scale_x_discrete(labels = c(&quot;Native Am.&quot;, &quot;Black&quot;, &quot;Hispanic&quot;, &quot;Asian&quot;, &quot;White&quot;)) + scale_y_continuous(labels = scales::percent) + theme_dataedu() + scale_fill_dataedu() + theme(legend.position = &quot;none&quot;) 8% of White students attend high poverty schools, compared to 43% of Black students, 39% of Hispanic students, 28% of Asian students, and 45% of Native American students. We can conclude these students are disproportionally attending high poverty schools. 9.6.2 Reveal Relationships Let’s explore what happens when we correlate race and FRPL percentage by school. merged_df %&gt;% filter(school_name != &quot;Total&quot;) %&gt;% ggplot(aes(x = wh_pct, y = frpl_pct)) + geom_point(color = dataedu_cols(&quot;green&quot;)) + theme_dataedu() + xlab(&quot;White Percentage&quot;) + ylab(&quot;FRPL Percentage&quot;) + scale_y_continuous(labels = scales::percent) + scale_x_continuous(labels = scales::percent) + theme(legend.position = &quot;none&quot;) Related to the result above, there is a strong negative correlation between FRPL percentage and the percentage of White students in a school. That is, high poverty schools have a lower percentage of White students and low poverty schools have a higher percentage of White students. 9.7 Results Because of the disaggregated data this district provides, we can go deeper than the average of demographics across the district and see what it looks like on the school level. These views display that (1) there exists a distribution of race/ethnicity within schools that are not representative of the district, (2) that students of color are overrepresented in high poverty schools, and (3) there is a negative relationship between the percentage of White students in a school and the percentage of students eligible for FRPL. 9.8 Conclusion According to the Urban Institute, the disproportionate percentage of students of color attending high poverty schools “is a defining feature of almost all Midwestern and northeastern metropolitan school systems.” Among other issues, high poverty schools tend to lack the educational resources—like highly qualified and experienced teachers, low student-teacher ratios, college prerequisite and advanced placement courses, and extracurricular activities—available in low-poverty schools. This has a huge impact on these students and their futures. In addition, research shows that racial and socioeconomic diversity in schools can provide students with a range of cognitive and social benefits. Therefore, this deep segregation that exists in the district can have adverse effects on students. As an education data practicioner, we can use these data to suggest interventions for what we can do to improve equity in the district. In addition, we can advocate for more datasets such as these, which allow us to dig deep. 9.9 References Loeb, S., Dynarski, S., McFarland, D., Morris, P., Reardon, S., &amp; Reber, S. (2017). Descriptive analysis in education: A guide for researchers. (NCEE 2017–4023). Washington, DC: U.S. Department of Education, Institute of Education Sciences, National Center for Education Evaluation and Regional Assistance. National Forum on Education Statistics. (2016). Forum Guide to Collecting and Using Disaggregated Data on Racial/Ethnic Subgroups. (NFES 2017-017). U.S. Department of Education. Washington, DC: National Center for Education Statistics. References "],
["c10.html", "10 Walkthrough 4: Longitudinal Analysis With Federal Students With Disabilities Data 10.1 Vocabulary 10.2 Introduction 10.3 Load Packages 10.4 Import Data 10.5 Process Data 10.6 Analysis: How Have Child Counts Changed Over Time? 10.7 Results 10.8 Conclusion: Aggregate Data as Context for Student Data", " 10 Walkthrough 4: Longitudinal Analysis With Federal Students With Disabilities Data 10.1 Vocabulary Read in Aggregate data Vector List Tidy format Subset select_at mutate Statistical model Aggregate data Student-level data 10.2 Introduction 10.2.1 Background Data scientists working in education don’t always have access to student level data, so knowing how to work with publicly available datasets is a useful skill. This walkthrough has two goals. First, we’ll be learning some ways to explore data over time. Second, we’ll be learning how to explore a publicly available dataset. Like most public datasets, this one contains aggregate data. This means that someone totaled up the student counts so it doesn’t reveal any private information. You can download the datasets for this walkthrough on the United States Department of Education website (see of Education (2020))1. This walkthrough uses datasets of student with disabilities counts in each state. 10.2.2 Methods In this walkthrough, we’ll learn how to read multiple datasets in using the map() function. Next, we’ll prepare our data for analysis by cleaning the variable names. Finally, we’ll explore this dataset by visualizing student counts and comparing male to female ratios over time. 10.3 Load Packages For this walkthrough, we’ll be using four packages: tidyverse, here, dataedu, and lubridate. You can load those packages by running this code: library(tidyverse) ## ── Attaching packages ────────────── ## ✓ ggplot2 3.2.1 ✓ purrr 0.3.3 ## ✓ tibble 2.1.3 ✓ dplyr 0.8.3 ## ✓ tidyr 1.0.0 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.4.0 ## ── Conflicts ─────────────────────── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(here) ## here() starts at /Users/shortessay/data-science-in-education library(dataedu) library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following object is masked from &#39;package:here&#39;: ## ## here ## The following object is masked from &#39;package:base&#39;: ## ## date 10.4 Import Data In this analysis we’ll be importing and combining six datasets that describe the number of students with disabilities in a given year. Let’s spend some time carefully reviewing how to get the CSVs we’ll need downloaded and stored on your computer. If you want to run the code exactly as written here, you’ll need to store the same datasets in the right location. It’s possible to use this walkthrough on different datasets or to store them in different locations on your computer, but you’ll need to make adjustments to your code based on the datasets you used and where you stored them. We suggest only doing this if you already have some experience using R. What to download In this walkthrough, we’ll be using six separate datasets of child counts, one for each year between 2012 and 2017. If you’re copying and pasting the code in this walkthrough, we recommend downloading the datasets from our GitHub repository for the most reliable results. Here’s a link to each file: https://github.com/data-edu/data-science-in-education/raw/master/data/longitudinal_data/bchildcountandedenvironments2012.csv https://github.com/data-edu/data-science-in-education/raw/master/data/longitudinal_data/bchildcountandedenvironments2013.csv https://github.com/data-edu/data-science-in-education/raw/master/data/longitudinal_data/bchildcountandedenvironments2014.csv https://github.com/data-edu/data-science-in-education/raw/master/data/longitudinal_data/bchildcountandedenvironments2015.csv https://github.com/data-edu/data-science-in-education/raw/master/data/longitudinal_data/bchildcountandedenvironments2016.csv https://github.com/data-edu/data-science-in-education/raw/master/data/longitudinal_data/bchildcountandedenvironments2017-18.csv You can also find these files on the United States Department of Education website: https://www2.ed.gov/programs/osepidea/618-data/state-level-data-files/index.html A note on file paths When you download these files, be sure to store them in a folder in your working directory. To get to the data in this walkthrough, we’ll be using this file path in our working directory: “data/longitudinal_data”. We’ll be using the here package, which convenieniently fills in all the folders in the file path of your working directory all the way up to the folders you specify in the arguments. So when referencing the file path “data/longitudinal_data”, we’ll use code like this: here::here( &quot;data&quot;, &quot;longitudinal_data&quot;, &quot;bchildcountandedenvironments2012.csv&quot; ) ## [1] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2012.csv&quot; You can use a different file path if you like, just take note of where your downloaded files are so you can use the correct file path when writing your code to import the data. 10.4.1 Reading In One Dataset We’ll be learning how to read in more than one dataset using the map() function. Let’s try it first with one dataset, then we’ll scale our solution up to multiple datasets. When you are analyzing multiple datasets that all have the same structure, you can read in each dataset using one code chunk. This code chunk will store each dataset as an element of a list. Before doing that, you should explore one of the datasets to see what you can learn about its structure. Clues from this exploration inform how you read in all the datasets at once later on. For example, we can see that the first dataset has some lines at the top that contain no data: ## # A tibble: 16,234 x 31 ## `Extraction Dat… `6/12/2013` X3 X4 X5 X6 X7 X8 X9 X10 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Updated: 2/12/2014 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 Revised: &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 Year State Name SEA … SEA … Amer… Asia… Blac… Hisp… Nati… Two … ## 5 2012 ALABAMA Corr… All … - - - - - - ## 6 2012 ALABAMA Home All … 1 1 57 12 0 2 ## 7 2012 ALABAMA Home… All … - - - - - - ## 8 2012 ALABAMA Insi… All … - - - - - - ## 9 2012 ALABAMA Insi… All … - - - - - - ## 10 2012 ALABAMA Insi… All … - - - - - - ## # … with 16,224 more rows, and 21 more variables: X11 &lt;chr&gt;, X12 &lt;chr&gt;, ## # X13 &lt;chr&gt;, X14 &lt;chr&gt;, X15 &lt;chr&gt;, X16 &lt;chr&gt;, X17 &lt;chr&gt;, X18 &lt;chr&gt;, ## # X19 &lt;chr&gt;, X20 &lt;chr&gt;, X21 &lt;chr&gt;, X22 &lt;chr&gt;, X23 &lt;chr&gt;, X24 &lt;chr&gt;, ## # X25 &lt;chr&gt;, X26 &lt;chr&gt;, X27 &lt;chr&gt;, X28 &lt;chr&gt;, X29 &lt;chr&gt;, X30 &lt;chr&gt;, X31 &lt;chr&gt; The rows containing “Extraction Date:”, “Updated:” and “Revised:” aren’t actually rows. They’re notes the authors left at the top of the dataset to show when the dataset was changed. read_csv uses the first row as the variable names unless told otherwise, so we need to tell read_csv to skip those lines using the skip argument. If we don’t, read_csv assumes the very first line–the one that says “Extraction Date:”–is the correct row of variable names. That’s why calling read_csv without the skip argument results in column names like X4. When there’s no obvious column name to read in, read_csv names them X[...] and let’s you know in a warning message. Try using skip = 4 in your call to read_csv: ## Parsed with column specification: ## cols( ## .default = col_character(), ## Year = col_double() ## ) ## See spec(...) for full column specifications. ## Warning: 3 parsing failures. ## row col expected actual file ## 16228 Year a double ------------------- &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2012.csv&#39; ## 16229 Year a double - Data not available &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2012.csv&#39; ## 16230 Year a double x Data supressed due to small cell size &#39;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2012.csv&#39; ## # A tibble: 16,230 x 31 ## Year `State Name` `SEA Education … `SEA Disability… `American India… ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2012 ALABAMA Correctional Fa… All Disabilities - ## 2 2012 ALABAMA Home All Disabilities 1 ## 3 2012 ALABAMA Homebound/Hospi… All Disabilities - ## 4 2012 ALABAMA Inside regular … All Disabilities - ## 5 2012 ALABAMA Inside regular … All Disabilities - ## 6 2012 ALABAMA Inside regular … All Disabilities - ## 7 2012 ALABAMA Other Location … All Disabilities 7 ## 8 2012 ALABAMA Other Location … All Disabilities 1 ## 9 2012 ALABAMA Parentally Plac… All Disabilities - ## 10 2012 ALABAMA Residential Fac… All Disabilities 0 ## # … with 16,220 more rows, and 26 more variables: `Asian Age 3-5` &lt;chr&gt;, `Black ## # or African American Age 3-5` &lt;chr&gt;, `Hispanic/Latino Age 3-5` &lt;chr&gt;, ## # `Native Hawaiian or Other Pacific Islander Age 3-5` &lt;chr&gt;, `Two or More ## # Races Age 3-5` &lt;chr&gt;, `White Age 3-5` &lt;chr&gt;, `Female Age 3 to 5` &lt;chr&gt;, ## # `Male Age 3 to 5` &lt;chr&gt;, `LEP Yes Age 3 to 5` &lt;chr&gt;, `LEP No Age 3 to ## # 5` &lt;chr&gt;, `Age 3 to 5` &lt;chr&gt;, `Age 6-11` &lt;chr&gt;, `Age 12-17` &lt;chr&gt;, `Age ## # 18-21` &lt;chr&gt;, `Ages 6-21` &lt;chr&gt;, `LEP Yes Age 6 to 21` &lt;chr&gt;, `LEP No Age 6 ## # to 21` &lt;chr&gt;, `Female Age 6 to 21` &lt;chr&gt;, `Male Age 6 to 21` &lt;chr&gt;, ## # `American Indian or Alaska Native Age 6 to21` &lt;chr&gt;, `Asian Age 6 ## # to21` &lt;chr&gt;, `Black or African American Age 6 to21` &lt;chr&gt;, `Hispanic/Latino ## # Age 6 to21` &lt;chr&gt;, `Native Hawaiian or Other Pacific Islander Age 6 ## # to21` &lt;chr&gt;, `Two or more races Age 6 to21` &lt;chr&gt;, `White Age 6 to21` &lt;chr&gt; The skip argument told read_csv to make the line containing “Year”, “State Name”, and so on as the first line. The result is a dataset that has “Year”, “State Name”, and so on as variable names. 10.4.2 Reading In Many Datasets Will the read_csv and skip = 4 combination work on all our datasets? To find out, we’ll use this strategy: Store a vector of filenames and paths in a list. These paths point to our datasets Pass the list of filenames as arguments to read_csv using purrr::map, including skip = 4, in our read_csv call Examine the new list of datasets to see if the variable names are correct Imagine a widget-making machine that works by acting on raw materials it receives on a conveyer belt. This machine executes one set of instructions on each of the raw materials it receives. You are the operator of the machine and you design instructions to get a widget out of the raw materials. Your plan might look something like this: Raw materials: a list of filenames and their paths Widget-making machine: purrr:map() Widget-making instructions: `read_csv(path, skip = 4) Expected widgets: a list of datasets Let’s create the raw materials first. Our raw materials will be file paths to each of the CSVs we want to read. Use list.files to make a vector of filename paths and name that vector filenames. list.files returns a vector of file names in the folder specified in the path argument. When we set the full.names argument to “TRUE”, we get a full path of these filenames. This will be useful later when we need the file names and their paths to read our data in. # Get filenames from the data folder filenames &lt;- list.files(path = here::here(&quot;data&quot;, &quot;longitudinal_data&quot;), full.names = TRUE) # A list of filenames and paths filenames ## [1] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2012.csv&quot; ## [2] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2013.csv&quot; ## [3] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2014.csv&quot; ## [4] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2015.csv&quot; ## [5] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2016.csv&quot; ## [6] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2017-18.csv&quot; That made a vector of six filenames, one for each year of child count data stored in the data folder. Now pass our raw materials, the vector called filenames, to our widget-making machine called map and give the machine the instructions read_csv(., skip = 4). Name the list of widgets it cranks out all_files: # Pass filenames to map and read_csv all_files &lt;- filenames %&gt;% map(., ~ read_csv(., skip = 4)) It is important to think ahead here. The goal is to combine the datasets in all_files into one dataset using bind_rows. But that will only work if all the datasets in our list have the same number of columns and the same column names. We can check our column names by using map and names: We can use identical to see if the variables from two datasets match. We see that the variable names of the first and second datasets don’t match, but the variables from the second and third do. # Variables of first and second dataset don&#39;t match identical(names(all_files[[1]]), names(all_files[[2]])) ## [1] FALSE # Variables of third and second files match identical(names(all_files[[2]]), names(all_files[[3]])) ## [1] TRUE And we can check the number of columns by using map and ncol: all_files %&gt;% map(ncol) ## [[1]] ## [1] 31 ## ## [[2]] ## [1] 50 ## ## [[3]] ## [1] 50 ## ## [[4]] ## [1] 50 ## ## [[5]] ## [1] 50 ## ## [[6]] ## [1] 50 Congratulations on finding an extremely common problem in education data! You’ve discovered that niehter the number of columns nor the column names match. This is a problem because we won’t be able to combine the datasets into one. When we try, bind_rows returns a dataet with 100 columns instead of the expected 50. bind_rows(all_files) ## # A tibble: 97,386 x 100 ## Year `State Name` `SEA Education … `SEA Disability… `American India… ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2012 ALABAMA Correctional Fa… All Disabilities - ## 2 2012 ALABAMA Home All Disabilities 1 ## 3 2012 ALABAMA Homebound/Hospi… All Disabilities - ## 4 2012 ALABAMA Inside regular … All Disabilities - ## 5 2012 ALABAMA Inside regular … All Disabilities - ## 6 2012 ALABAMA Inside regular … All Disabilities - ## 7 2012 ALABAMA Other Location … All Disabilities 7 ## 8 2012 ALABAMA Other Location … All Disabilities 1 ## 9 2012 ALABAMA Parentally Plac… All Disabilities - ## 10 2012 ALABAMA Residential Fac… All Disabilities 0 ## # … with 97,376 more rows, and 95 more variables: `Asian Age 3-5` &lt;chr&gt;, `Black ## # or African American Age 3-5` &lt;chr&gt;, `Hispanic/Latino Age 3-5` &lt;chr&gt;, ## # `Native Hawaiian or Other Pacific Islander Age 3-5` &lt;chr&gt;, `Two or More ## # Races Age 3-5` &lt;chr&gt;, `White Age 3-5` &lt;chr&gt;, `Female Age 3 to 5` &lt;chr&gt;, ## # `Male Age 3 to 5` &lt;chr&gt;, `LEP Yes Age 3 to 5` &lt;chr&gt;, `LEP No Age 3 to ## # 5` &lt;chr&gt;, `Age 3 to 5` &lt;chr&gt;, `Age 6-11` &lt;chr&gt;, `Age 12-17` &lt;chr&gt;, `Age ## # 18-21` &lt;chr&gt;, `Ages 6-21` &lt;chr&gt;, `LEP Yes Age 6 to 21` &lt;chr&gt;, `LEP No Age 6 ## # to 21` &lt;chr&gt;, `Female Age 6 to 21` &lt;chr&gt;, `Male Age 6 to 21` &lt;chr&gt;, ## # `American Indian or Alaska Native Age 6 to21` &lt;chr&gt;, `Asian Age 6 ## # to21` &lt;chr&gt;, `Black or African American Age 6 to21` &lt;chr&gt;, `Hispanic/Latino ## # Age 6 to21` &lt;chr&gt;, `Native Hawaiian or Other Pacific Islander Age 6 ## # to21` &lt;chr&gt;, `Two or more races Age 6 to21` &lt;chr&gt;, `White Age 6 ## # to21` &lt;chr&gt;, `Age 3` &lt;chr&gt;, `Age 4` &lt;chr&gt;, `Age 5` &lt;chr&gt;, `Age 6` &lt;chr&gt;, ## # `Age 7` &lt;chr&gt;, `Age 8` &lt;chr&gt;, `Age 9` &lt;chr&gt;, `Age 10` &lt;chr&gt;, `Age ## # 11` &lt;chr&gt;, `Age 12` &lt;chr&gt;, `Age 13` &lt;chr&gt;, `Age 14` &lt;chr&gt;, `Age 15` &lt;chr&gt;, ## # `Age 16` &lt;chr&gt;, `Age 17` &lt;chr&gt;, `Age 18` &lt;chr&gt;, `Age 19` &lt;chr&gt;, `Age ## # 20` &lt;chr&gt;, `Age 21` &lt;chr&gt;, `2016` &lt;dbl&gt;, Alabama &lt;chr&gt;, `Correctional ## # Facilities` &lt;chr&gt;, `All Disabilities` &lt;chr&gt;, `-` &lt;chr&gt;, `-_1` &lt;chr&gt;, ## # `-_2` &lt;chr&gt;, `-_3` &lt;chr&gt;, `-_4` &lt;chr&gt;, `-_5` &lt;chr&gt;, `-_6` &lt;chr&gt;, ## # `-_7` &lt;chr&gt;, `-_8` &lt;chr&gt;, `-_9` &lt;chr&gt;, `-_10` &lt;chr&gt;, `-_11` &lt;chr&gt;, ## # `-_12` &lt;chr&gt;, `-_13` &lt;chr&gt;, `-_14` &lt;chr&gt;, `0` &lt;chr&gt;, `0_1` &lt;chr&gt;, ## # `0_2` &lt;chr&gt;, `0_3` &lt;chr&gt;, `0_4` &lt;chr&gt;, `0_5` &lt;chr&gt;, `0_6` &lt;chr&gt;, ## # `0_7` &lt;chr&gt;, `0_8` &lt;chr&gt;, `1` &lt;chr&gt;, `2` &lt;chr&gt;, `4` &lt;chr&gt;, `14` &lt;chr&gt;, ## # `22` &lt;chr&gt;, `30` &lt;chr&gt;, `4_1` &lt;chr&gt;, `0_9` &lt;chr&gt;, `7` &lt;chr&gt;, `70` &lt;chr&gt;, ## # `77` &lt;chr&gt;, `0_10` &lt;chr&gt;, `77_1` &lt;chr&gt;, `1_1` &lt;chr&gt;, `76` &lt;chr&gt;, ## # `0_11` &lt;chr&gt;, `0_12` &lt;chr&gt;, `68` &lt;chr&gt;, `0_13` &lt;chr&gt;, `0_14` &lt;chr&gt;, ## # `0_15` &lt;chr&gt;, `9` &lt;chr&gt; We’ll correct this in the next section by selecting and renaming our variables, but it’s good to notice this problem early in the process so you know to work on it later. 10.5 Process Data Transforming your dataset before visualizing it and fitting models is critical. It’s easier to write code when variable names are concise and informative. Many functions in R, especially those in the ggplot2 package, work best when datsets are in a “tidy” format. It’s easier to do an analysis when you have just the variables you need. Any unused variables can confuse your thought process. Let’s preview the steps we’ll be taking: Fix the variable names in the 2016 data Combine the datasets Pick variables Filter for the desired categories Rename the variables Standardize the state names Transform the column formats from wide to narrow using gather Change the data types of variables Explore NAs In real life, data scientists don’t always know the cleaning steps until they dive into the work. Learning what cleaning steps are needed requires exploration, trial and error, and clarity on the analytic questions you want to answer. After a lot of exploring, we settled on these steps for this analysis. When you do your own, you will find different things to transform. As you do more and more data analysis, your instincts for what to transform will improve. 10.5.1 Fix the variable names in the 2016 data When we print the 2016 dataset, we notice that the variable names are incorrect. Let’s verify that by looking at the 2016 dataset, which is the fifth element of all_files: names(all_files[[5]]) ## [1] &quot;2016&quot; &quot;Alabama&quot; ## [3] &quot;Correctional Facilities&quot; &quot;All Disabilities&quot; ## [5] &quot;-&quot; &quot;-_1&quot; ## [7] &quot;-_2&quot; &quot;-_3&quot; ## [9] &quot;-_4&quot; &quot;-_5&quot; ## [11] &quot;-_6&quot; &quot;-_7&quot; ## [13] &quot;-_8&quot; &quot;-_9&quot; ## [15] &quot;-_10&quot; &quot;-_11&quot; ## [17] &quot;-_12&quot; &quot;-_13&quot; ## [19] &quot;-_14&quot; &quot;0&quot; ## [21] &quot;0_1&quot; &quot;0_2&quot; ## [23] &quot;0_3&quot; &quot;0_4&quot; ## [25] &quot;0_5&quot; &quot;0_6&quot; ## [27] &quot;0_7&quot; &quot;0_8&quot; ## [29] &quot;1&quot; &quot;2&quot; ## [31] &quot;4&quot; &quot;14&quot; ## [33] &quot;22&quot; &quot;30&quot; ## [35] &quot;4_1&quot; &quot;0_9&quot; ## [37] &quot;7&quot; &quot;70&quot; ## [39] &quot;77&quot; &quot;0_10&quot; ## [41] &quot;77_1&quot; &quot;1_1&quot; ## [43] &quot;76&quot; &quot;0_11&quot; ## [45] &quot;0_12&quot; &quot;68&quot; ## [47] &quot;0_13&quot; &quot;0_14&quot; ## [49] &quot;0_15&quot; &quot;9&quot; We want the variable names to be Year and State Name. But first, let’s go back and review how to get at the 2016 dataset from all_files. The order of the list elements was set all the way back when we fed map our list of filenames. If we look at filenames again, we see that the 2016 dataset was stored in the fifth element: filenames ## [1] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2012.csv&quot; ## [2] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2013.csv&quot; ## [3] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2014.csv&quot; ## [4] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2015.csv&quot; ## [5] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2016.csv&quot; ## [6] &quot;/Users/shortessay/data-science-in-education/data/longitudinal_data/bchildcountandedenvironments2017-18.csv&quot; Once we know the 2016 dataset is the fifth element of our list, we can pluck it out by using double brackets: all_files[[5]] ## # A tibble: 16,230 x 50 ## `2016` Alabama `Correctional F… `All Disabiliti… `-` `-_1` `-_2` `-_3` ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2016 Alabama Home All Disabilities 43 30 35 0 ## 2 2016 Alabama Homebound/Hospi… All Disabilities - - - - ## 3 2016 Alabama Inside regular … All Disabilities - - - - ## 4 2016 Alabama Inside regular … All Disabilities - - - - ## 5 2016 Alabama Inside regular … All Disabilities - - - - ## 6 2016 Alabama Parentally Plac… All Disabilities - - - - ## 7 2016 Alabama Residential Fac… All Disabilities 5 3 4 0 ## 8 2016 Alabama Residential Fac… All Disabilities - - - - ## 9 2016 Alabama Separate Class All Disabilities 58 58 98 0 ## 10 2016 Alabama Separate School… All Disabilities 11 20 19 0 ## # … with 16,220 more rows, and 42 more variables: `-_4` &lt;chr&gt;, `-_5` &lt;chr&gt;, ## # `-_6` &lt;chr&gt;, `-_7` &lt;chr&gt;, `-_8` &lt;chr&gt;, `-_9` &lt;chr&gt;, `-_10` &lt;chr&gt;, ## # `-_11` &lt;chr&gt;, `-_12` &lt;chr&gt;, `-_13` &lt;chr&gt;, `-_14` &lt;chr&gt;, `0` &lt;chr&gt;, ## # `0_1` &lt;chr&gt;, `0_2` &lt;chr&gt;, `0_3` &lt;chr&gt;, `0_4` &lt;chr&gt;, `0_5` &lt;chr&gt;, ## # `0_6` &lt;chr&gt;, `0_7` &lt;chr&gt;, `0_8` &lt;chr&gt;, `1` &lt;chr&gt;, `2` &lt;chr&gt;, `4` &lt;chr&gt;, ## # `14` &lt;chr&gt;, `22` &lt;chr&gt;, `30` &lt;chr&gt;, `4_1` &lt;chr&gt;, `0_9` &lt;chr&gt;, `7` &lt;chr&gt;, ## # `70` &lt;chr&gt;, `77` &lt;chr&gt;, `0_10` &lt;chr&gt;, `77_1` &lt;chr&gt;, `1_1` &lt;chr&gt;, ## # `76` &lt;chr&gt;, `0_11` &lt;chr&gt;, `0_12` &lt;chr&gt;, `68` &lt;chr&gt;, `0_13` &lt;chr&gt;, ## # `0_14` &lt;chr&gt;, `0_15` &lt;chr&gt;, `9` &lt;chr&gt; We used skip = 4 when we read in those datasets. That worked for all datasets except the fifth one. In that one, skipping four lines left out the variable name row. To fix it, we’ll read the 2016 dataset again using read_csv and the fifth element of filenames. We’ll assign the newly read dataset to the fifth element of the all_files list: Try printing all_files now. You can confirm we fixed the problem by checking that the variable names are correct. 10.5.2 Pick variables Now that we know all our datasets have the correct variable names, we simplify our datasets by picking the variables we need. This is a good place to think carefully about which variables to pick. This usually requires a fair amount of trial and error, but here is what we found we needed: Our analytic questions are about gender, so let’s pick the gender variable Later, we’ll need to filter our dataset by disability category and program location so we’ll want SEA Education Environment and SEA Disability Category We want to make comparisons by state and reporting year, so we’ll also pick State Name and Year Combining select and contains is a convenient way to pick these variables without writing a lot of code. Knowing that we want variables that contain the acronym “SEA” and variables that contain “male” in their names, we can pass those characters to contains: all_files[[1]] %&gt;% select( Year, contains(&quot;State&quot;, ignore.case = FALSE), contains(&quot;SEA&quot;, ignore.case = FALSE), contains(&quot;male&quot;) ) ## # A tibble: 16,230 x 8 ## Year `State Name` `SEA Education … `SEA Disability… `Female Age 3 t… ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2012 ALABAMA Correctional Fa… All Disabilities - ## 2 2012 ALABAMA Home All Disabilities 63 ## 3 2012 ALABAMA Homebound/Hospi… All Disabilities - ## 4 2012 ALABAMA Inside regular … All Disabilities - ## 5 2012 ALABAMA Inside regular … All Disabilities - ## 6 2012 ALABAMA Inside regular … All Disabilities - ## 7 2012 ALABAMA Other Location … All Disabilities 573 ## 8 2012 ALABAMA Other Location … All Disabilities 81 ## 9 2012 ALABAMA Parentally Plac… All Disabilities - ## 10 2012 ALABAMA Residential Fac… All Disabilities 6 ## # … with 16,220 more rows, and 3 more variables: `Male Age 3 to 5` &lt;chr&gt;, ## # `Female Age 6 to 21` &lt;chr&gt;, `Male Age 6 to 21` &lt;chr&gt; That code chunk verifies that we got the variables we want, so now we will turn the code chunk into a function called pick_vars. We will then use map to feed our list of datasets, all_files, to the function. In this function, we’ll use a special version of select() called select_at(), which conveniently picks variables based on criteria we give it. The argument vars(Year, contains(\"State\", ignore.case = FALSE), contains(\"SEA\", ignore.case = FALSE), contains(\"male\")) tells R we want to keep any column whose name has “State” in upper or lower case letters, has “SEA” in the title, and has “male” in the title. This will result in a newly transformed all_files list that contains six datasets, all with the desired variables. pick_vars &lt;- function(df) { df %&gt;% select_at(vars( Year, contains(&quot;State&quot;, ignore.case = FALSE), contains(&quot;SEA&quot;, ignore.case = FALSE), contains(&quot;male&quot;) )) } all_files &lt;- all_files %&gt;% map(pick_vars) 10.5.3 Combine six datasets into one Now we’ll turn our attention to combining the datasets in our list all_files into one. We’ll use bind_rows, which combines datasets by adding each one to the bottom of the one before it. The first step is to check and see if our datasets have the same number of variables and the same variable names. When we use names on our list of newly changed datasets, we see that each dataset’s variable names are the same: all_files %&gt;% map(names) ## [[1]] ## [1] &quot;Year&quot; &quot;State Name&quot; ## [3] &quot;SEA Education Environment&quot; &quot;SEA Disability Category&quot; ## [5] &quot;Female Age 3 to 5&quot; &quot;Male Age 3 to 5&quot; ## [7] &quot;Female Age 6 to 21&quot; &quot;Male Age 6 to 21&quot; ## ## [[2]] ## [1] &quot;Year&quot; &quot;State Name&quot; ## [3] &quot;SEA Education Environment&quot; &quot;SEA Disability Category&quot; ## [5] &quot;Female Age 3 to 5&quot; &quot;Male Age 3 to 5&quot; ## [7] &quot;Female Age 6 to 21&quot; &quot;Male Age 6 to 21&quot; ## ## [[3]] ## [1] &quot;Year&quot; &quot;State Name&quot; ## [3] &quot;SEA Education Environment&quot; &quot;SEA Disability Category&quot; ## [5] &quot;Female Age 3 to 5&quot; &quot;Male Age 3 to 5&quot; ## [7] &quot;Female Age 6 to 21&quot; &quot;Male Age 6 to 21&quot; ## ## [[4]] ## [1] &quot;Year&quot; &quot;State Name&quot; ## [3] &quot;SEA Education Environment&quot; &quot;SEA Disability Category&quot; ## [5] &quot;Female Age 3 to 5&quot; &quot;Male Age 3 to 5&quot; ## [7] &quot;Female Age 6 to 21&quot; &quot;Male Age 6 to 21&quot; ## ## [[5]] ## [1] &quot;Year&quot; &quot;State Name&quot; ## [3] &quot;SEA Education Environment&quot; &quot;SEA Disability Category&quot; ## [5] &quot;Female Age 3 to 5&quot; &quot;Male Age 3 to 5&quot; ## [7] &quot;Female Age 6 to 21&quot; &quot;Male Age 6 to 21&quot; ## ## [[6]] ## [1] &quot;Year&quot; &quot;State Name&quot; ## [3] &quot;SEA Education Environment&quot; &quot;SEA Disability Category&quot; ## [5] &quot;Female Age 3 to 5&quot; &quot;Male Age 3 to 5&quot; ## [7] &quot;Female Age 6 to 21&quot; &quot;Male Age 6 to 21&quot; That means that we can combine all six datasets into one using bind_rows. We’ll call this newly combined dataset child_counts: child_counts &lt;- all_files %&gt;% bind_rows() Since we know that a) each of our six datasets had eight variables and b) our combined dataset also has eight variables, we can conclude that all our rows combined together correctly. But let’s use str to verify: str(child_counts) ## Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 97387 obs. of 8 variables: ## $ Year : num 2012 2012 2012 2012 2012 ... ## $ State Name : chr &quot;ALABAMA&quot; &quot;ALABAMA&quot; &quot;ALABAMA&quot; &quot;ALABAMA&quot; ... ## $ SEA Education Environment: chr &quot;Correctional Facilities&quot; &quot;Home&quot; &quot;Homebound/Hospital&quot; &quot;Inside regular class 40% through 79% of day&quot; ... ## $ SEA Disability Category : chr &quot;All Disabilities&quot; &quot;All Disabilities&quot; &quot;All Disabilities&quot; &quot;All Disabilities&quot; ... ## $ Female Age 3 to 5 : chr &quot;-&quot; &quot;63&quot; &quot;-&quot; &quot;-&quot; ... ## $ Male Age 3 to 5 : chr &quot;-&quot; &quot;174&quot; &quot;-&quot; &quot;-&quot; ... ## $ Female Age 6 to 21 : chr &quot;4&quot; &quot;-&quot; &quot;104&quot; &quot;1590&quot; ... ## $ Male Age 6 to 21 : chr &quot;121&quot; &quot;-&quot; &quot;130&quot; &quot;3076&quot; ... 10.5.4 Filter for the desired disabilities and age groups We want to explore gender related variables, but our dataset has additional aggregate data for other subgroups. For example, we can use count to explore all the different disability groups in the dataset: child_counts %&gt;% count(`SEA Disability Category`) ## # A tibble: 16 x 2 ## `SEA Disability Category` n ## &lt;chr&gt; &lt;int&gt; ## 1 All Disabilities 6954 ## 2 Autism 6954 ## 3 Deaf-blindness 6954 ## 4 Developmental delay 4636 ## 5 Developmental delay (valid only for children ages 3-9 when defined by … 2318 ## 6 Emotional disturbance 6954 ## 7 Hearing impairment 6954 ## 8 Intellectual disability 6954 ## 9 Multiple disabilities 6954 ## 10 Orthopedic impairment 6954 ## 11 Other health impairment 6954 ## 12 Specific learning disability 6954 ## 13 Speech or language impairment 6954 ## 14 Traumatic brain injury 6954 ## 15 Visual impairment 6954 ## 16 &lt;NA&gt; 31 Since we will be visualizing and modeling gender variables for all students in the dataset, we’ll filter out all subgoups except “All Disabilities” and the age totals: child_counts &lt;- child_counts %&gt;% filter( `SEA Disability Category` == &quot;All Disabilities&quot;, `SEA Education Environment` %in% c(&quot;Total, Age 3-5&quot;, &quot;Total, Age 6-21&quot;) ) 10.5.5 Rename the variables In the next section we’ll prepare the dataset for visualization and modeling by “tidying” it. When we write code to transform datasets, we’ll be typing the column names a lot, so it’s useful to change them to ones with more convenient names. child_counts &lt;- child_counts %&gt;% rename( year = Year, state = &quot;State Name&quot;, age = &quot;SEA Education Environment&quot;, disability = &quot;SEA Disability Category&quot;, f_3_5 = &quot;Female Age 3 to 5&quot;, m_3_5 = &quot;Male Age 3 to 5&quot;, f_6_21 = &quot;Female Age 6 to 21&quot;, m_6_21 = &quot;Male Age 6 to 21&quot; ) 10.5.6 Clean state names You might have noticed that some state names in our dataset are in upper case letters and some are in lower case letters: child_counts %&gt;% count(state) %&gt;% head() ## # A tibble: 6 x 2 ## state n ## &lt;chr&gt; &lt;int&gt; ## 1 Alabama 8 ## 2 ALABAMA 4 ## 3 Alaska 8 ## 4 ALASKA 4 ## 5 American Samoa 8 ## 6 AMERICAN SAMOA 4 If we leave it like this, R will treat state values like “CALIFORNIA” and “California” as two different states. We can use mutate and tolower to transform all the state names to lowercase letters. child_counts &lt;- child_counts %&gt;% mutate(state = tolower(state)) 10.5.7 Tidy the dataset Visualizing and modeling our data will be much easier if our dataset is in a “tidy” format. In his paper Tidy Data (2014), Hadley Wickham defines tidy datasets where Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. A note on the gender variable in this dataset This dataset uses a binary approach to data collection about gender. Students are described as either male or female. The need for an inclusive approach to documenting gender identity is discussed in a paper by (“Reachable: Data Collection Methods for Sexual Orientation and Gender Identity” 2016) of The Williams Institute at UCLA. The gender variables in our dataset are spread across four columns, with each one representing a combination of gender and age range. We can use gather to bring the gender variable into one column. In this transformation, we create two new columns: a gender column and a total column. The total column will contain the number of students in each row’s gender and age category. child_counts &lt;- child_counts %&gt;% gather(gender, total, f_3_5:m_6_21) To make the values of the gender column more intuitive, we’ll use case_when to transform the values to either “f” or “m”: child_counts &lt;- child_counts %&gt;% mutate( gender = case_when( gender == &quot;f_3_5&quot; ~ &quot;f&quot;, gender == &quot;m_3_5&quot; ~ &quot;m&quot;, gender == &quot;f_6_21&quot; ~ &quot;f&quot;, gender == &quot;m_6_21&quot; ~ &quot;m&quot;, TRUE ~ as.character(gender) ) ) 10.5.8 Convert data types The values in the total column represent the number of students from a specific year, state, gender, and age group. We know from the chr under their variable names that R is treating these values like characters instead of numbers. While R does a decent job of treating numbers like numbers when needed, it’s much safer to prepare the dataset by changing these character columns to number columns. We’ll use dplyr::mutate_at to change the count columns. child_counts &lt;- child_counts %&gt;% mutate(total = as.numeric(total)) ## Warning: NAs introduced by coercion child_counts ## # A tibble: 2,928 x 6 ## year state age disability gender total ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2012 alabama Total, Age 3-5 All Disabilities f 2228 ## 2 2012 alabama Total, Age 6-21 All Disabilities f NA ## 3 2012 alaska Total, Age 3-5 All Disabilities f 676 ## 4 2012 alaska Total, Age 6-21 All Disabilities f NA ## 5 2012 american samoa Total, Age 3-5 All Disabilities f 45 ## 6 2012 american samoa Total, Age 6-21 All Disabilities f NA ## 7 2012 arizona Total, Age 3-5 All Disabilities f 4743 ## 8 2012 arizona Total, Age 6-21 All Disabilities f NA ## 9 2012 arkansas Total, Age 3-5 All Disabilities f 4605 ## 10 2012 arkansas Total, Age 6-21 All Disabilities f NA ## # … with 2,918 more rows Converting these count columns from character classes to number classes resulted in two changes. First, the chr under these variable names has now changed to dbl, short for “double-precision”. This lets us know that R recognizes these values as numbers with decimal points. Second, the blank values changed to NA. When R sees a character class value like \"4\", it knows to change it to numeric class 4. But there is no obvious number represented by a character class like \"\", so it changes it to NA: # Convert a character to a number as.numeric(&quot;4&quot;) ## [1] 4 # Convert a blank character to number as.numeric(&quot;&quot;) ## [1] NA Similarly, the variable year needs to be changed from the character format to the date format. Doing so will make sure R treats this variable like a point in time when we plot our dataset. The package lubridate has a handy function called ymd that can help us. We just have to use the truncated argument to let R know we don’t have a month and date to convert. child_counts &lt;- child_counts %&gt;% mutate(year = ymd(year, truncated = 2)) 10.5.9 Explore and address NAs You’ll notice that some rows in the total column contain an NA. When we used gather to create a gender column, R created unique rows for every year, state, age, disability, and gender combination. Since the original dataset had both gender and age range stored in a column like Female Age 3 to 5, R made rows where the total value is NA . For example, there is no student count for the age value “Total, Age 3-5” that also has the gender value for female students who were age 6-21. You can see that more clearly by sorting the dataset by year, state, and gender. In our foundational skills chapter, we introduced a dplyr function called arrange to sort the rows of a dataset by the values in a column. Let’s use arrange here to sort the dataset by the year, state and gender columns. When you pass arrange a variable, it will sort by the order of the values in that variable. If you pass it multiple variables, arrange will sort by the first variable, then by the second, and so on. Let’s see what it does on child_counts when we pass it the year, state, and gender variables: child_counts %&gt;% arrange(year, state, gender) ## # A tibble: 2,928 x 6 ## year state age disability gender total ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2012-01-01 alabama Total, Age 3-5 All Disabilities f 2228 ## 2 2012-01-01 alabama Total, Age 6-21 All Disabilities f NA ## 3 2012-01-01 alabama Total, Age 3-5 All Disabilities f NA ## 4 2012-01-01 alabama Total, Age 6-21 All Disabilities f 23649 ## 5 2012-01-01 alabama Total, Age 3-5 All Disabilities m 5116 ## 6 2012-01-01 alabama Total, Age 6-21 All Disabilities m NA ## 7 2012-01-01 alabama Total, Age 3-5 All Disabilities m NA ## 8 2012-01-01 alabama Total, Age 6-21 All Disabilities m 48712 ## 9 2012-01-01 alaska Total, Age 3-5 All Disabilities f 676 ## 10 2012-01-01 alaska Total, Age 6-21 All Disabilities f NA ## # … with 2,918 more rows We can simplify our dataset by removing these rows, leaving us with one row for each category: females age 3-5 females age 6-21 males age 3-5 males age 6-21 Each of these categories will be associated with a state and reporting year: child_counts &lt;- child_counts %&gt;% filter(!is.na(total)) We can verify we have the categories we want by sorting again: child_counts %&gt;% arrange(year, state, gender) ## # A tibble: 1,390 x 6 ## year state age disability gender total ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2012-01-01 alabama Total, Age 3-5 All Disabilities f 2228 ## 2 2012-01-01 alabama Total, Age 6-21 All Disabilities f 23649 ## 3 2012-01-01 alabama Total, Age 3-5 All Disabilities m 5116 ## 4 2012-01-01 alabama Total, Age 6-21 All Disabilities m 48712 ## 5 2012-01-01 alaska Total, Age 3-5 All Disabilities f 676 ## 6 2012-01-01 alaska Total, Age 6-21 All Disabilities f 5307 ## 7 2012-01-01 alaska Total, Age 3-5 All Disabilities m 1440 ## 8 2012-01-01 alaska Total, Age 6-21 All Disabilities m 10536 ## 9 2012-01-01 american samoa Total, Age 3-5 All Disabilities f 45 ## 10 2012-01-01 american samoa Total, Age 6-21 All Disabilities f 208 ## # … with 1,380 more rows 10.6 Analysis: How Have Child Counts Changed Over Time? In the last section we focused on importing our dataset. In this section, we will turn to exploring it. First, we’ll use visualization to explore the number of students in special education over time. In particular, we’ll compare the count of male and female students. Next, we’ll use what we learn from our visualizations to quantify any differences that we see. 10.6.1 Visualize the Dataset Showing this many states in a plot can be overwhelming, so to start we’ll make a subset of the dataset. We can use a function in the dplyr package called top_n() to help us learn which states have the highest average count of special education students: child_counts %&gt;% group_by(state) %&gt;% summarise(mean_count = mean(total)) %&gt;% top_n(6, mean_count) ## # A tibble: 6 x 2 ## state mean_count ## &lt;chr&gt; &lt;dbl&gt; ## 1 california 180879. ## 2 florida 92447. ## 3 new york 121751. ## 4 pennsylvania 76080. ## 5 texas 115593. ## 6 us, outlying areas, and freely associated states 1671931. These six states have the highest mean count of students in special education over the six years we are examining. For reasons we will see in a later visualization, we are going to exclude outlying areas and freely associated states. That leaves us us with five states: California, Florida, New York, Pennsylvania, and Texas. We can remove all other states but these by using filter(). We’ll call this new dataset high_count: high_count &lt;- child_counts %&gt;% filter(state %in% c(&quot;california&quot;, &quot;florida&quot;, &quot;new york&quot;, &quot;pennsylvania&quot;, &quot;texas&quot;)) Now we can use high_count to do some initial exploration. Our analysis is about comparing counts of male and female students in special education, but visualization is also a great way to explore related curiosities. You may surprise yourself with what you find when visualizing your datasets. You might come up with more interesting hypotheses, find that your initial hypothesis requires more data transformation, or find interesting subsets of the data–we saw a little of that in the surprisingly high mean_count of freely associated states in the state column. Let your curiosity and intuition drive this part of the analysis. It’s one of the activities that makes data analysis a creative process. In that spirit, we’ll start by visualizing specific genders and age groups. Feel free to try these, but also try the other student groups for practice and more exploration. Start by copying and running this code in your console to see what it does: high_count %&gt;% filter(gender == &quot;f&quot;, age == &quot;Total, Age 6-21&quot;) %&gt;% ggplot(aes(x = year, y = total, color = state)) + geom_freqpoly(stat = &quot;identity&quot;, size = 1) + labs(title = &quot;Count of female students in special education over time&quot;, subtitle = &quot;Ages 6-21&quot;) + theme_dataedu() + scale_color_dataedu() That gives us a plot that has the years in the x-axis and a count of female students in the y-axis. Each line takes a different color based on the state it represents. Let’s look at that closer: We used filter to subset our dataset for students who are female and ages 6 to 21. We used aes to connect visual elements of our plot to our data. We connected the x-axis to year, the y-axis to total, and the color of the line to state. It’s worth calling out one more thing, since it’s a technique we’ll be using as we explore further. Note here that, instead of storing our new dataset in a new variable, we filter the dataset then use the pipe operator %&gt;% to feed it to ggplot. Since we’re exploring freely, we don’t need to create a lot of new variables we probably won’t need later. We can also try the same plot, but subsetting for male students instead. We can use the same code we used for the last plot, but filter for the value “m” in the gender field: high_count %&gt;% filter(gender == &quot;m&quot;, age == &quot;Total, Age 6-21&quot;) %&gt;% ggplot(aes(x = year, y = total, color = state)) + geom_freqpoly(stat = &quot;identity&quot;, size = 1) + labs(title = &quot;Count of male students in special education over time&quot;, subtitle = &quot;Ages 6-21&quot;) + theme_dataedu() + scale_color_dataedu() We’ve looked at each gender separately. What do these lines look like if we visualized the total amount of students each year per state? To do that, we’ll need to add both gender values together and both age group values together. We’ll do this using a very common combination of functions: group_by and summarise. high_count %&gt;% group_by(year, state) %&gt;% summarise(n = sum(total)) %&gt;% ggplot(aes(x = year, y = n, color = state)) + geom_freqpoly(stat = &quot;identity&quot;, size = 1) + labs(title = &quot;Total count of students in special education over time&quot;, subtitle = &quot;Ages 6-21&quot;) + theme_dataedu() + scale_color_dataedu() So far we’ve looked at a few ways to count students over time. In each plot, we see that while counts have grown overall for all states, each state has different sized populations. Let’s see if we can summarize that difference by looking at the median student count for each state over the years: high_count %&gt;% group_by(year, state) %&gt;% summarise(n = sum(total)) %&gt;% ggplot(aes(x = state, y = n)) + geom_boxplot() + labs(title = &quot;Median student count&quot;, subtitle = &quot;All ages and genders&quot;) + theme_dataedu() The boxplots show us what we might have expected from our freqpoly plots before it. The highest median student count over time is California and the lowest is Pennsylvania. What have we learned about our data so far? The five states in the US with the highest total student counts (not including outlying areas and freely associated states) do not have similar counts to each other. The student counts for each state also appear to have grown over time. But how can we start comparing the male student count to the female student count? One way is to use a ratio: In mathematics, a ratio is a relationship between two numbers indicating how many times the first number contains the second. Wikpedia We can use the count of male students in each state and divide it by the count of each female student. The result is the number of times male students are in special education more or less than the female students in the same state and year. Our coding strategy will be to Use spread to create separate columns for male and female students. Use mutate to create a new variable called ratio. The values in this column will be the result of dividing the count of male students by the count of female students Note here that we can also accomplish this comparison by dividing the number of female students by the number of male students. In this case, the result would be the number of times female students are in special education more or less than male students. high_count %&gt;% group_by(year, state, gender) %&gt;% summarise(total = sum(total)) %&gt;% # Create new columns for male and female student counts spread(gender, total) %&gt;% # Create a new ratio column mutate(ratio = m / f) %&gt;% ggplot(aes(x = year, y = ratio, color = state)) + geom_freqpoly(stat = &quot;identity&quot;, size = 1) + scale_y_continuous(limits = c(1.5, 2.5)) + labs(title = &quot;Male student to female student ratio over time&quot;, subtitle = &quot;Ages 6-21&quot;) + theme_dataedu() + scale_color_dataedu() By visually inspecting, we can hypothesize that there was no significant change in the male to female ratio between the years 2012 and 2017. But very often we want to understand the underlying properties of our education dataset. We can do this by quantifying the relationship between two variables. In the next section, we’ll explore ways to quantify the relationship between male student counts and female student counts. 10.6.2 Model the Dataset When you visualize your datasets, you are exploring possible relationships between variables. But sometimes visualizations can be misleading because of the way we perceive graphics. In his book Data Visuzliation: A Practical Introduction, Healy (2019) teaches us that Visualizations encode numbers in lines, shapes, and colors. That means that our interpretation of these encodings is partly conditional on how we perceive geometric shapes and relationships generally. What are some ways we can combat these errors of perception and at the same time draw substantive conclusions about our education dataset? When you spot a possible relationship between variables, the relationship between female and male counts for example, you’ll want to quantify that relationship by fitting a statisitcal model. Practically speaking, this means you are selecting a distribution that represents your dataset reasonably well. This distribution will help you quantify and predict relationships between variables. This is an important step in the analytic process, because it acts as a check on what you saw in your exploratory visualizations. In this example, we’ll follow our intuition about the relationship between male and female student counts in our special education dataset. In particular, we’ll test the hypothesis that this ratio has decreased over the years. Fitting a linear regression model that estimates the year as a predictor of the male to female ratio will help us do just that. Do we have enough information for our model? At the start of this section, we chose to exclude outlying areas and freely associated states. This visualization suggests that there are some states that have a child count so high it leaves a gap in the x-axis values. This can be problematic when we try to interpret our model later. Here’s a plot of female students compared to male students. Note that the relationship appears linear, but there is a large gap in the distribution of female student counts somewhere bewteen the values of 250,000 and 1,750,000: child_counts %&gt;% filter(age == &quot;Total, Age 6-21&quot;) %&gt;% spread(gender, total) %&gt;% ggplot(aes(x = f, y = m)) + geom_point(size = 3, alpha = .5) + geom_smooth() + labs( title = &quot;Comparison of female students to male students in special education&quot;, subtitle = &quot;Counts of students in each state, ages 6-21&quot;, x = &quot;Female students&quot;, y = &quot;Male students&quot;, caption = &quot;Data: US Dept of Education&quot; ) + theme_dataedu() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; If you think of each potential point on the linear regression line as a ratio of male to female students, you’ll notice that we don’t know a whole lot about what happens in states where there are between 250,000 and 1,750,000 female students in any given year. To learn more about what’s happening in our dataset, we can filter it for only states that have more than 500,000 female students in any year: child_counts %&gt;% filter(age == &quot;Total, Age 6-21&quot;) %&gt;% spread(gender, total) %&gt;% filter(f &gt; 500000) %&gt;% select(year, state, age, f, m) ## # A tibble: 6 x 5 ## year state age f m ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2012-01-01 us, outlying areas, and freely associ… Total, Age 6… 1933619 3.89e6 ## 2 2013-01-01 us, outlying areas, and freely associ… Total, Age 6… 1937726 3.88e6 ## 3 2014-01-01 us, outlying areas, and freely associ… Total, Age 6… 1965204 3.92e6 ## 4 2015-01-01 us, outlying areas, and freely associ… Total, Age 6… 2007174 3.98e6 ## 5 2016-01-01 us, outlying areas, and freely associ… Total, Age 6… 2014120 3.97e6 ## 6 2017-01-01 us, outlying areas, and freely associ… Total, Age 6… 2051438 4.02e6 This is where we discover that each of the data points in the upper right hand corner of the plot are from the state value “us, us, outlying areas, and freely associated states”. If we remove these outliers, we have a distribution of female students that looks more complete. child_counts %&gt;% filter(age == &quot;Total, Age 6-21&quot;) %&gt;% spread(gender, total) %&gt;% # Filter for female student counts less than 500,000 filter(f &lt;= 500000) %&gt;% ggplot(aes(x = f, y = m)) + geom_point(size = 3, alpha = .5) + labs( title = &quot;Comparison of female students to male students in special education&quot;, subtitle = &quot;Counts of students in each state, ages 6-21.\\nDoes not include outlying areas and freely associated states&quot;, x = &quot;Female students&quot;, y = &quot;Male students&quot;, caption = &quot;Data: US Dept of Education&quot; ) + theme_dataedu() This should allow us to fit a better model for the relationship between male and female student counts, albeit only the ones where the count of female students takes a value between 0 and 500,000. Male to Female Ratio Over Time Earlier we asked the question Do we have enough data points for the count of female students to learn about the ratio of female to male students? Similarly, we should ask the question Do we have enough data points across our year variable to learn about how this ratio has changed over time? To answer that question, let’s start by making a new dataset that excludes any rows where the f variables has a value that is less than or equal to 500000. We’ll convert the year variable to a factor data type–we’ll see how this helps in a bit. We’ll also add a column called ratio that contains the male to female count ratio. model_data &lt;- child_counts %&gt;% filter(age == &quot;Total, Age 6-21&quot;) %&gt;% mutate(year = as.factor(year(year))) %&gt;% spread(gender, total) %&gt;% # Exclude outliers filter(f &lt;= 500000) %&gt;% # Compute male student to female student ratio mutate(ratio = m / f) %&gt;% select(-c(age, disability)) We can see how much data we have per year by using count: model_data %&gt;% count(year) ## # A tibble: 6 x 2 ## year n ## &lt;fct&gt; &lt;int&gt; ## 1 2012 59 ## 2 2013 56 ## 3 2014 56 ## 4 2015 58 ## 5 2016 57 ## 6 2017 55 Let’s visualize the ratio values across all years as an additional check. Note the use of geom_jitter to spread the points horizontally so we can estimate the quantities better: ggplot(data = model_data, aes(x = year, y = ratio)) + geom_jitter(alpha = .5) + labs(title = &quot;Male to female ratio across years (jittered)&quot;) + theme_dataedu() Each year seems to have data points that can be considered when we fit the model. This means that there are enough data points to help us learn how the year variable predicts the ratio variable. We fit the linear regression model by passing the argument ratio ~ year to the function lm. In R, the ~ usually indicates a formula. In this case, the formula is the variable year as a predictor of the variable ratio. The final argument we pass to lm is data = model_data, which tells R to look for the variables ratio and year in the dataset model_data. The results of the model are called a “model object.” We’ll store the model object in ratio_year: ratio_year &lt;- lm(ratio ~ year, data = model_data) Each model object is filled with all sorts of model information. We can look at this information using the fuction summary: summary(ratio_year) ## ## Call: ## lm(formula = ratio ~ year, data = model_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.44025 -0.10138 -0.02810 0.05343 0.75737 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.03356 0.02200 92.418 &lt;2e-16 *** ## year2013 -0.01205 0.03153 -0.382 0.7027 ## year2014 -0.02372 0.03153 -0.752 0.4524 ## year2015 -0.03104 0.03125 -0.993 0.3213 ## year2016 -0.03964 0.03139 -1.263 0.2075 ## year2017 -0.05760 0.03168 -1.818 0.0699 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.169 on 335 degrees of freedom ## Multiple R-squared: 0.01215, Adjusted R-squared: -0.002594 ## F-statistic: 0.8241 on 5 and 335 DF, p-value: 0.5332 Here’s how we can interpret the Estimate column: The estimate of the (Intercept) is 2.03356, which is the estimated value of the ratio variable when the year variable is “2012”. Note that the value year2012 isn’t present in the in the list of rownames. That’s because the (Intercept) row represents year2012. In linear regression models that use factor variables as predictors, the first level of the factor is the intercept. Sometimes this level is called a “dummy variable”. The remaining rows of the model output show how much each year differs from the intercept, 2012. For example, year2013 has an estimate of -0.01205, which suggests that on average the value of ratio is .01205 less than 2.03356. On average, the ratio of year2014 is .02372 less than 2.03356. The t value column tells us the size of difference between the estimated value of the ratio for each year and the estimated value of the ratio of the intercept. Generally speaking, the larger the t value, the larger the chance that any difference between the coefficient of a factor level and the intercept are significant. Though the relationship between year as a preditor of ratio is not linear (recall our previous plot), the linear regression model still gives us useful information. We fit a linear regression model to a factor variable, like year, as a predictor of a continuous variable, likeratio. In doing so, we got the average ratio at every value of year. We can verify this by taking the mean ratio of ever year: model_data %&gt;% group_by(year) %&gt;% summarise(mean_ratio = mean(ratio)) ## # A tibble: 6 x 2 ## year mean_ratio ## &lt;fct&gt; &lt;dbl&gt; ## 1 2012 2.03 ## 2 2013 2.02 ## 3 2014 2.01 ## 4 2015 2.00 ## 5 2016 1.99 ## 6 2017 1.98 This verifies that our intercept, the value of ratio during the year 2012, is 2.033563 and the value of ratio for 2013 is .01205 less than that of 2012 on average. Fitting the model gives us more details about these mean ratio scores– namely the coefficient, t value, and p value. These values help us apply judgement when deciding if differences in ratio values suggest an underlying difference between years or simply differences you can expect from randomness. In this case, the absence of \"*\" in all rows except the Intercept row suggest that any differences occuring between years are within the range you’d expect by chance. If we use summary on our model_data dataset, we can verify the intercept again: model_data %&gt;% filter(year == &quot;2012&quot;) %&gt;% summary() ## year state f m ratio ## 2012:59 Length:59 Min. : 208 Min. : 443 Min. :1.710 ## 2013: 0 Class :character 1st Qu.: 5606 1st Qu.: 11467 1st Qu.:1.927 ## 2014: 0 Mode :character Median : 22350 Median : 44110 Median :1.994 ## 2015: 0 Mean : 32773 Mean : 65934 Mean :2.034 ## 2016: 0 3rd Qu.: 38552 3rd Qu.: 77950 3rd Qu.:2.093 ## 2017: 0 Max. :198595 Max. :414466 Max. :2.692 The mean value of the ratio column when the year column is 2012 is 2.034, just like in the model output’s intercept row. Lastly, we may want to communicate to a larger audience that there were roughly twice amount of male students in this datset than female students and that this did not change significantly between the years 2012 and 2017. When you are not communicating to an audience of other data scientists, it’s helpful to illustrate your point without the technical details of the model output. Think of yourself as an interpreter: since you can speak the language of model outputs and the language of data visualization, your challenge is to take what you learned from the model output and tell that story in a way that is meaningful to your non-data scientist audience. There are many ways to do this, but we’ll choose boxplots to show our audience that there was roughly twice as many male students in special education than female students between 2012 and 2017. For our purposes, let’s verify this by looking at the median male to female ratio for each year: model_data %&gt;% group_by(year) %&gt;% summarise(median_ratio = median(ratio)) ## # A tibble: 6 x 2 ## year median_ratio ## &lt;fct&gt; &lt;dbl&gt; ## 1 2012 1.99 ## 2 2013 1.99 ## 3 2014 1.98 ## 4 2015 1.98 ## 5 2016 1.97 ## 6 2017 1.96 Now let’s visualize this using our boxplots: model_data %&gt;% gather(gender, students, c(f, m)) %&gt;% ggplot(aes(x = year, y = students, color = gender)) + geom_boxplot() + scale_y_continuous(labels = scales::comma) + labs( title = &quot;Median male and female student counts in special education&quot;, subtitle = &quot;Ages 6-21. Does not include outlying areas and freely associated states&quot;, x = &quot;&quot;, y = &quot;&quot;, caption = &quot;Data: US Dept of Education&quot; ) + theme_dataedu() + scale_color_dataedu() Once we learned from our model that male to female ratios did not change in any meaningful way from 2012 to 2017 and that the median ratio across states was about 2 male students to every female student, we can present these two ideas using this plot. When discussing the plot, it helps to have your model output in your notes so you can reference specific coefficient estimates when needed. 10.7 Results We learned that each state has a different count of students with disabilities–so different that we need to use statistics like ratios or visualizations to compare across states. Even when we narrow our focus to the five states with the highest counts of students with disabilities, we see that there are differences in these counts. When we look at these five states over time, we see that despite the differences in total count each year, all five states have increased their student counts. We also learned that though the male to female ratios for students with disabilities appears to have gone down slightly over time, our model suggests that these decreases do not represent a big difference. The comparison of student counts across each state is tricky because there is a lot of variation in total enrollment across all fifty states. While we explored student counts across each state and verified that there is variation in the counts, a good next step would be to combine these data with total enrollment data. This would allow us to compare counts of students with disabilities as a percentage of total enrollment. Comparing proportions like this is a common way to compare subgroups of a population across states when each state’s population varies in size. 10.8 Conclusion: Aggregate Data as Context for Student Data Education data science is about using data science tools to learn about and improve the lives of our students. So why choose a publicly available aggregate dataset instead of a student-level dataset? We chose to use an aggregate dataset because it reflects an analysis that an education data scientist would typically do. Using student-level data requires that the data scientist is either an employee of the school agency or that works under a memorandum of understanding (MOU) that allows her to access this data. Without either of these conditions, the education data scientist learns about the student experience by working on publicly available datasets, almost all of which are aggregated student-level datasets. Before we discuss the benefits of aggregate data, let’s take some time to understand the differences between aggregate and student-level data. Publicly available data–like the US Federal Government special education student count we used in this walkthrough–is a summary of student-level data. That means that student-level data is totaled up to protect the identities of students before making them publicly available. We can use R to demonstrate this concept. Here are rows in a student-level dataset: # student-level data tibble( student = letters[1:10], school = rep(letters[11:15], 2), test_score = sample(0:100, 10, replace = TRUE) ) ## # A tibble: 10 x 3 ## student school test_score ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 a k 22 ## 2 b l 16 ## 3 c m 14 ## 4 d n 35 ## 5 e o 12 ## 6 f k 11 ## 7 g l 60 ## 8 h m 3 ## 9 i n 98 ## 10 j o 18 Aggregate data totals up a variable–the variable test_score in this case–to “hide” the student-level information. The rows of the resulting dataset represent a group. The group in our example is the school variable: tibble( student = letters[1:10], school = rep(letters[11:15], 2), test_score = sample(0:100, 10, replace = TRUE) ) %&gt;% # Aggregate by school group_by(school) %&gt;% summarise(mean_score = mean(test_score)) ## # A tibble: 5 x 2 ## school mean_score ## &lt;chr&gt; &lt;dbl&gt; ## 1 k 74 ## 2 l 22 ## 3 m 78.5 ## 4 n 36 ## 5 o 91.5 Notice here that this dataset no longer identifies individual students. Student-level data for analysis of local populations. Aggregate data for base rate and context. Longitudinal analysis is typically done with student-level data because educators are interested in what happens to students over time. So if you cannot access student-level data, how do we use aggregate data to offer value to the analytic conversation? Aggregate data is valuable because it allows us to learn from populations that are larger or different from the local student-level population. Think of it as an opportunity to learn from totaled up student data from other states or the whole country. In the book Thinking Fast and Slow, Kahneman (2011) discusses the importance of learning from larger populations, a context he refers to as the base rate. The base rate fallacy is the tendency to only focus on conclusions we can draw from immediately available information. It’s the difference between computing how often a student at one school is identified for special education services (student-level data) and how often students are identified for special educations services nationally (base rate data). We can use aggregate data to combat the baserate fallacy by putting what we learn from local student data in the context of surrounding populations. For example, consider an analysis of student-level data in a school district over time. Student-level data allows us to ask questions about our local population: One such question is: Are the rates of special education identification for male students different from other gender identitites in our district? This style of question looks inward at your own educational system. Taking a cue from Daniel Kahneman, we should also ask what this pattern looks like in other states or in the country. Aggregate data allows us to ask questions about a larger population: One such question is Are the rates of special education identification for male students different from other gender identities in the United States? This style of question looks for answers outside your own educational system. The combination of the two lines of inquiry are powerful way to generate new knowledge about the student experience. So education data scientists should not despair in situations where they cannot access student-level data. Aggregate data is a powerful way to learn from state level or national level data when an MOU for student-level data is not possible. In situations where student-level data is available, including aggregate data is an excellent way to combat the base rate fallacy. References "],
["c11.html", "11 Walkthrough 5: Text Analysis With Social Media Data 11.1 Vocabulary 11.2 Introduction 11.3 Load Packages 11.4 Import Data 11.5 View Data 11.6 Process Data 11.7 Analysis: Counting Words 11.8 Analysis: Sentiment analysis 11.9 Conclusion", " 11 Walkthrough 5: Text Analysis With Social Media Data 11.1 Vocabulary tokenize token RDS files anti_join set.seed slice sample_n() stop words 11.2 Introduction The ability to work with many kinds of datasets is one of the great features of doing data science with programming. So far we’ve analyzed data in CSV files, but that’s not the only way data is stored. If we can learn some basic techniques for analyzing text, we increase the number of places we can find information to learn about the student experience. 11.2.1 Background When we think about data science in education, our minds tends to go data in spreadsheets. But what can we learn about the student experience from text data? Take a moment to mentally review all the moments in your work day that you generated or consumed text data. In education, we’re surrounded by it. We do our lessons in word processor documents, our students submit assignments online, and the school community expresses themselves on public social media platforms. The text we generate can be an authentic reflection of reality in schools, so how might we learn from it? Even the most basic text analysis techniques will expand your data science toolkit. For example, you can use text analysis to count the number of key words that appear in open ended survey responses. You can analyze word patterns in student responses or message board posts. Analyzing a collection of text is different from analyzing large numerical datasets because words don’t have agreed upon values the way numbers do. The number 2 will always be more than 1 and less than 3. The word “fantastic”, on the other hand, has multiple ambiguous levels of degree depending on interpretation and context. Using text analysis can help to broadly estimate what is happening in the text. When paired with observations, interviews, and close review of the text, this approach can help education staff learn from text data. In this chapter, we’ll learn how to count the frequency of words in a dataset and associate those words with common feelings like positivity or joy. We’ll show these techniques using a dataset of tweets. We encourage you to complete the walkthrough, then reflect on how the skills learned can be applied to other texts, like word processing documents or websites. 11.2.2 Data Source It’s useful to learn these techniques from text datasets that are available for download. Take a moment to do an online search for “download tweet dataset” and note the abundance of tweet datasets available. Since there’s so much, it’s useful to narrow the tweets to a only those that help you answer your analytic questions. Hashtags are text within a tweet that act as a way to categorize content. Here’s an example: RT (???): I’m trying to recreate some Stata code in R, anyone have a good resource for what certain functions in Stata are doing? #RStats #Stata Twitter recognizes any words that start with a “#” as a hashtag. The hashtags “#RStats” and “#Stata” make this tweet conveniently searchable. If Twitter uses search for “#RStats”, Twitter returns all the Tweets containing that hashtag. In this example, we’ll be analyzing a dataset of tweets that have the hashtag #tidytuesday (https://twitter.com/hashtag/tidytuesday). This hashtag returns tweets about the weekly TidyTuesday ritual, where folks learning R create and tweet data visualizations they made while learning to use tidyverse R packages. You can view the TidyTuesday tweets dataset here: http://bit.ly/tidytuesday-dataset 11.2.3 Methods In this walkthrough, we’ll be learning how to count words in a text dataset. We’ll also use a technique called sentiment analysis to count and visualize the appearance of words that have a positive association. Lastly, we’ll learn how to get more context by selecting random rows of tweets for closer reading. 11.3 Load Packages For this analysis, we’ll be using the tidyverse, here, and dataedu packages. Just a reminder, if you haven’t already installed the dataedu package, you can do so by typing this code: devtools::install_github(&quot;data-edu/dataedu&quot;) library(tidyverse) library(here) library(dataedu) library(tidytext) 11.4 Import Data Let’s start by getting the data into our programming environment so we can start analyzing it. We’ve conveniently included the raw dataset of TidyTuesday tweets in the dataedu package. You can see the dataset by typing tt_tweets. Let’s start by assigning the name raw_tweets to this dataset: raw_tweets &lt;- dataedu::tt_tweets 11.4.1 Using an API It’s worth taking a short detour to talk about another way you can get a dataset like this. A more advanced but common way to import data from a social media website is to use something called an application programming interface (API). A full discussion and walkthrough of using an API is outside the scope of this book, but it’s worth introducing the idea so you have a sense of the possibilities. Once you learn how to do it, using an API offers some advantages over downloading the dataset. Think of an API as a special door a home builder made for a house that has a lot of cool stuff in it. The home builder doesn’t want everyone to be able to walk right in and use a bunch of stuff in the house. But they also don’t want to make it too hard because, after all, sharing is caring! So imagine the home builder made a door just for folks who know how to use doors. In order to get through this door, users need to know where to find it along the outside of the house. Once they’re there, they have to know the code to open. And once they’re through the door, they have to know how to use the stuff inside. An API for social media platforms like Twitter and Facebook are the same way. You can download datasets of social media information, like tweets, using some code and authentication credentials organized by the website. There are some advantages to using an API to import data at the start of your education dataset analysis. Every time you run the code in your analysis, you’ll be using the API to contact the social media platform and download a fresh dataset. Now your analysis is not just a one-off product. By using an API to import new data every time you run your code, you create an analysis that can be run again and again on future datasets. 11.5 View Data Let’s return to our raw_tweets dataset. Run glimpse(raw_tweets) and notice the number of variables in this dataset. It’s good practice to use functions like glimpse or str to look at the data type of each variable. For this walkthrough, we won’t need all 90 variables so let’s clean the dataset and keep only the ones we want. 11.6 Process Data In this section we’ll select the columns we need for our analysis and we’ll transform the dataset so each row represents a word. After that, our dataset will be ready for exploring. First, let’s use select to pick the two columns we’ll need: status_id and text. status_id will help us associate interesting words with a particular tweet and text will give us the text from that tweet. We’ll also change status_id to the character data type since it’s meant to label tweets and doesn’t actually represent a numerical value. tweets &lt;- raw_tweets %&gt;% #filter for English tweets filter(lang == &quot;en&quot;) %&gt;% select(status_id, text) %&gt;% # Convert the ID field to the character data type mutate(status_id = as.character(status_id)) Now the dataset has a column to identify each tweet and a column that shows the text that users tweeted. But each row has the entire tweet in the text variable, which makes it hard to analyze. If we kept our dataset like this, we’d need to use functions on each row to do something like count the number of times the word “good” appears. We can count words more efficiently if each row represented a single word. Splitting sentences in a row into single words in a row is called “tokenizing.” In their book Text Mining With R, Silge and Robinson (2017) describe tokens this way: A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. This one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix. The tidytext package has a convenient function called unnest_tokens that tokenizes vectors of words. Let’s install the tidytext package so we can use it: install.packages(&quot;tidytext&quot;) Let’s use unnest_tokens to take our dataset of tweets and transform it into a dataset of words. tokens &lt;- tweets %&gt;% unnest_tokens(output = word, input = text) tokens ## # A tibble: 131,233 x 2 ## status_id word ## &lt;chr&gt; &lt;chr&gt; ## 1 1163154266065735680 first ## 2 1163154266065735680 tidytuesday ## 3 1163154266065735680 submission ## 4 1163154266065735680 roman ## 5 1163154266065735680 emperors ## 6 1163154266065735680 and ## 7 1163154266065735680 their ## 8 1163154266065735680 rise ## 9 1163154266065735680 to ## 10 1163154266065735680 power ## # … with 131,223 more rows We use output = word to tell unnest_tokens that we want our column of tokens to be called word. We use input = text to tell unnest_tokens to tokenize the tweets in the text column of our tweets dataset. The result is a new dataset where each row has a single word in the word column and a unique ID in the status_id column that tells us which tweet the word appears in. Notice that our tokens dataset has many more rows than our tweets dataset. This tells us a lot about how unnest_tokens works. In the tweets dataset, each row has an entire tweet and its unique ID. Since that unique ID is assigned to the entire tweet, each unique ID only appears once in the dataset. When we used unnest_tokens put each word on its own row, we broke each tweet into many words. This created additional rows in the dataset. And since each word in a single tweet shares the same ID for that tweet, an ID now appears multiple times in our new dataset. We’re almost ready to start analyzing the dataset! There’s one more step we’ll take–removing common words that don’t help us learn about what people are tweeting about. Words like “the” or “a” are in a category of words called “stop words”. Stop words serve a function in verbal communication, but don’t tell us much on their own. As a result, they clutter our dataset of useful words and make it harder to manage the volume of words we want to analyze. The tidytext package includes a dataset called stop_words that we’ll use to remove rows containing stop words. We’ll use anti_join on our tokens dataset and the stop_words dataset to keep only rows that have words not appearing in the stop_words dataset. data(stop_words) tokens &lt;- tokens %&gt;% anti_join(stop_words, by = &quot;word&quot;) Why does this work? Let’s look closer. inner_join matches the observations in one dataset to another by a specified common variable. Any rows that don’t have a match get dropped from the resulting dataset. anti_join does the same thing as inner_join except it drops matching rows and keeps the rows that don’t match. This is convenient for our analysis because we want to remove rows from tokens that contain words in the stop_words dataset. When we call anti_join, we’re left with rows that don’t match words in the stop_words dataset. These remaining words are the ones we’ll be analyzing. One final note before we start counting words: Remember when we first tokenized our dataset and we passed unnest_tokens the argument output = word? We conveniently chose word as our column name because it matches the column name word in the stop_words dataset. This makes our call to anti_join simpler because anti_join knows to look for the column named word in each dataset. 11.7 Analysis: Counting Words Now it’s time to start exploring our newly cleaned dataset of tweets. Computing the frequency of each word and seeing which words showed up the most often is a good start. We can pipe tokens to the count function to do this: tokens %&gt;% count(word, sort = TRUE) ## # A tibble: 15,335 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 t.co 5432 ## 2 https 5406 ## 3 tidytuesday 4316 ## 4 rstats 1748 ## 5 data 1105 ## 6 code 988 ## 7 week 868 ## 8 r4ds 675 ## 9 dataviz 607 ## 10 time 494 ## # … with 15,325 more rows We pass count the argument sort = TRUE to sort the n variable from the highest value to the lowest value. This makes it easy to see the most frequenly occuring words at the top. Not surprisingly, “tidytuesday” was the third most frequent word in this dataset. We may want to explore further by showing the frequency of words as a percent of the whole dataset. Calculating percentages like this is useful in a lot of education scenarios because it helps us make comparisons across different sized groups. For example, you may want to calculate what percentage of students in each classroom receive special education services. In our tweets dataset, we’ll be calculating the count of words as a percentage of all tweets. We can do that by using mutate to add a column called percent. percent will divide n by sum(n), which is the total number of words. Finally, will multiply the result by 100. tokens %&gt;% count(word, sort = TRUE) %&gt;% # n as a percent of total words mutate(percent = n / sum(n) * 100) ## # A tibble: 15,335 x 3 ## word n percent ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 t.co 5432 7.39 ## 2 https 5406 7.36 ## 3 tidytuesday 4316 5.87 ## 4 rstats 1748 2.38 ## 5 data 1105 1.50 ## 6 code 988 1.34 ## 7 week 868 1.18 ## 8 r4ds 675 0.919 ## 9 dataviz 607 0.826 ## 10 time 494 0.672 ## # … with 15,325 more rows Even at 4316 appearances in our dataset, “tidytuesday” represents only about 6 percent of the total words in our dataset. This makes sense when you consider our dataset contains 15335 unique words. 11.8 Analysis: Sentiment analysis Now that we have a sense of the most frequently appearing words, it’s time to explore some questions in our tweets dataset. Let’s imagine that we’re education consultants trying to learn about the community surrounding the TidyTuesday data visualization ritual. We know from the first part of our analysis that the token “dataviz” appeared frequently relative to other words, so maybe we can explore that further. A good start would be to see how the appearance of that token in a tweet is associated with other positive words. We’ll need to use a technique called sentiment analysis to get at the “positivity” of words in these tweets. Sentiment analysis tries to evaluate words for their emotional association. If we analyze words by the emotions they convey, we can start to explore patterns in large text datasets like our tokens data. The functions we’ll be using for our sentiment analysis are in a package called textdata. Let’s start by installing that. install.packages(&quot;tidytext&quot;) Earlier we used anti_join to remove stop words in our dataset. We’re going to do something similar here to reduce our tokens dataset to only words that have a positive association. We’ll use a dataset called the NRC Word-Emotion Association Lexicon to help us identify words with a positive association. This dataset was published in a work called Crowdsourcing a Word-Emotion Association Lexicon (Mohammad and Turney 2013) To explore this dataset more, we’ll use a tidytext function called get_sentiments to view some words and their associated sentiment. If this is your first time using the NRC Word-Emotion Association Lexicon in the tidytext package, you’ll be prompted to download the NRC lexicon. Respond “yes” to the prompt and the NRC lexicon will download. Note that you’ll only have to do this the first time you use the NRC lexicon. get_sentiments(&quot;nrc&quot;) ## # A tibble: 13,901 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 abacus trust ## 2 abandon fear ## 3 abandon negative ## 4 abandon sadness ## 5 abandoned anger ## 6 abandoned fear ## 7 abandoned negative ## 8 abandoned sadness ## 9 abandonment anger ## 10 abandonment fear ## # … with 13,891 more rows This returns a dataset with two columns. The first is word and contains a list of words. The second is the sentiment column, which contains an emotion associated with each word. This dataset is similar to the stop_words dataset. Note that this dataset also uses the column name word, which will again make it easy for us to match this dataset to our tokens dataset. 11.8.1 Tweets associated with positive Let’s begin working on reducing our tokens dataset down to only words that the NRC dataset associates with positivity. We’ll start by creating a new dataset, nrc_pos, which contains the NRC words that have the positive sentiment. Then we’ll match that new dataset to tokens using the word column that is common to both datasets. Finally, we’ll use count to total up the appearances of each positive word. # Only positive in the NRC dataset nrc_pos &lt;- get_sentiments(&quot;nrc&quot;) %&gt;% filter(sentiment == &quot;positive&quot;) # Match to tokens pos_tokens_count &lt;- tokens %&gt;% inner_join(nrc_pos, by = &quot;word&quot;) %&gt;% # Total appearance of positive words count(word, sort = TRUE) pos_tokens_count ## # A tibble: 644 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 fun 173 ## 2 top 162 ## 3 learn 131 ## 4 found 128 ## 5 love 113 ## 6 community 110 ## 7 learning 97 ## 8 happy 95 ## 9 share 90 ## 10 inspired 85 ## # … with 634 more rows We can visualize this words nicely by using ggplot to show the positive words in a bar chart. There are 644 words total, which is hard to convey in a compact chart. We’ll solve that problem by filtering our dataset to only words that appear 75 times or more. pos_tokens_count %&gt;% filter(n &gt;= 75) %&gt;% ggplot(., aes(x = reorder(word, -n), y = n)) + geom_bar(stat = &quot;identity&quot;) + labs( title = &quot;Count of words associated with positivity&quot;, subtitle = &quot;Tweets with the hashtag #tidytuesday&quot;, caption = &quot;Data: Twitter and NRC&quot;, x = &quot;&quot;, y = &quot;Count&quot; ) + theme_dataedu() Note the use of reorder when mapping the word variable to the x aesthetic. Using reorder here sorts our x axis in descending order by the variable n. Sorting the bars from highest frequency to lowest makes it easier for the reader to identify and compare the most and least common words in the visualization. 11.8.2 Dataviz and other positive words Earlier in the analysis we learned that “dataviz” was among the most frequently occuring words in this dataset. We can continue our exploration of TidyTuesday tweets by seeing how many tweets with “dataviz” also had at least one positive word from the NRC dataset. Looking at this might give us some clues about how people in the TidyTuesday learning community view dataviz as a tool. There are a few steps to this part of the analysis, so let’s review our strategy. We’ll need to use the status_id field in the tweets dataset to filter the tweets that have the word dataviz in them. Then we need to use the status_id field in this new bunch of dataviz tweets to identify the tweets that include at least one positive word. How do we know which status_id values contain the word “dataviz” and which ones contain a positive word? Recall that our tokens dataset only has one word per row, which makes it easy to use functions like filter and inner_join to make two new datasets: one of status_id values that have “dataviz” in the word column and one of status_id values that have a positive word in the word column. We’ll explore the combinations of “dataviz” and any positive words in our tweets dataset using these three ingredients: our tweets dataset, a vector of status_ids for tweets that have “dataviz” in them, and a vector of status_ids for tweets that have positive words in them. Now that we have our strategy, let’s write some code and see how it works. First, we’ll make a vector of status_ids for tweets that have “dataviz” in them. This will be used later to identify tweets that contain “dataviz” in the text. We’ll use filter on our tokens dataset to keep only the rows that have “dataviz” in the word column. Let’s name that new dataset dv_tokens. dv_tokens &lt;- tokens %&gt;% filter(word == &quot;dataviz&quot;) dv_tokens ## # A tibble: 607 x 2 ## status_id word ## &lt;chr&gt; &lt;chr&gt; ## 1 1116518351147360257 dataviz ## 2 1098025772554612738 dataviz ## 3 1161454327296339968 dataviz ## 4 1110711892086001665 dataviz ## 5 1151926405162291200 dataviz ## 6 1095854400004853765 dataviz ## 7 1157111441419395074 dataviz ## 8 1154958378764046336 dataviz ## 9 1105642831413239808 dataviz ## 10 1108196618464047105 dataviz ## # … with 597 more rows The result is a dataset that has status_ids in one column and the word “dataviz” in the other column. We can use $ to extract a vector of status_ids for tweets that have “dataviz” in the text. This vector has 607 values, so we’ll use head to view just the first ten. # Extract status_id head(dv_tokens$status_id) ## [1] &quot;1116518351147360257&quot; &quot;1098025772554612738&quot; &quot;1161454327296339968&quot; ## [4] &quot;1110711892086001665&quot; &quot;1151926405162291200&quot; &quot;1095854400004853765&quot; Now let’s do this again, but this time we’ll we’ll make a vector of status_ids for tweets that have positive words in them. This will be used later to identify tweets that contain a positive word in the text. We’ll use filter on our tokens dataset to keep only the rows that have any of the positive words in the in the word column. If you’ve been running all the code up to this point in the walkthrough, you’ll notice that you already have a dataset of positive words called nrc_pos, which can be turned into a vector of positive words by typing nrc_pos$word. We can use the %in% operator in our call to filter to find only words that are in this vector of positive words. Let’s name this new dataset pos_tokens. pos_tokens &lt;- tokens %&gt;% filter(word %in% nrc_pos$word) pos_tokens ## # A tibble: 4,925 x 2 ## status_id word ## &lt;chr&gt; &lt;chr&gt; ## 1 1163154266065735680 throne ## 2 1001412196247666688 honey ## 3 1001412196247666688 production ## 4 1001412196247666688 increase ## 5 1001412196247666688 production ## 6 1161638973808287746 found ## 7 991073965899644928 community ## 8 991073965899644928 community ## 9 991073965899644928 trend ## 10 991073965899644928 white ## # … with 4,915 more rows The result is a dataset that has status_ids in one column and a positive word from tokens in the other column. We’ll again use $ to extract a vector of status_ids for these tweets. # Extract status_id head(pos_tokens$status_id) ## [1] &quot;1163154266065735680&quot; &quot;1001412196247666688&quot; &quot;1001412196247666688&quot; ## [4] &quot;1001412196247666688&quot; &quot;1001412196247666688&quot; &quot;1161638973808287746&quot; That’s a lot of status_ids, many of which are duplicates. Let’s try and make the vector of status_ids a little shorter. We can use distinct to get a dataframe of status_ids, where each status_id only appears once: pos_tokens &lt;- pos_tokens %&gt;% distinct(status_id) Note that distinct drops all variables except for status_id. For good measure, let’s use distinct on our dv_tokens dataframe too: dv_tokens &lt;- dv_tokens %&gt;% distinct(status_id) Now we have a dataframe of status_id for tweets containing “dataviz” and another for tweets containing a positive word. Let’s use these to transform our tweets dataset. First we’ll filter tweets for rows that have the “dataviz” status_id. Then we’ll create a new column called positive that will tell us if the status_id is from our vector of positive word status_ids. We’ll name this filtered dataset dv_pos. dv_pos &lt;- tweets %&gt;% # Only tweets that have the dataviz status_id filter(status_id %in% dv_tokens$status_id) %&gt;% # Is the status_id from our vector of positive word? mutate(positive = if_else(status_id %in% pos_tokens$status_id, 1, 0)) Let’s take a moment to dissect how we use if_else to create our positive column. We gave if_else three arguments: status_id %in% pos_tokens$status_id: a logical statement 1: the value of positive if the logical statement is true 0: the value of positive if the logical statement is false So our new positive column will take the value 1 if the status_id was in our pos_tokens dataset and the value 0 if the status_id was not in our pos_tokens dataset. Practically speaking, positive is 1 if the tweet has a positive word and 0 if it does not have a positive word. And finally, let’s see what percent of tweets that had “dataviz” in them also had at least one positive word: dv_pos %&gt;% count(positive) %&gt;% mutate(perc = n / sum(n)) ## # A tibble: 2 x 3 ## positive n perc ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 272 0.450 ## 2 1 333 0.550 About 55 percent of tweets that have “dataviz” in them also had at least one positive word and about 45 percent of them did not have at least one positive word. It’s worth noting here that this finding doesn’t necessarily mean users didn’t have anything good to say about 45 percent of the “dataviz” tweets. We can’t know precisely why some tweets had positive words and some didn’t, we just know that more dataviz tweets had positive words than not. To put this in perspective, we might have a different impression if 5 percent or 95 percent of the tweets had positive words. Since the point of exploratory data analysis is to explore and develop questions, let’s continue to do that. In this last section we’ll review a random selection of tweets for context. 11.8.3 Close read of randomly selected tweets Let’s review where we are so far as we work to learn more about the TidyTuesday learning community through tweets. So far we’ve counted frequently used words and estimated the number of tweets with positive associations. This dataset is large, so we need to zoom out and find ways to summarize the data. But it’s also useful to explore by zooming in and reading some of the tweets. Reading tweets helps us to build intuition and context about how users talk about TidyTuesday in general. Even though this doesn’t lead to quantitative findings, it helps us to learn more about the content we’re studying and analyzing. Instead of reading all 4418 tweets, let’s write some code to randomly select tweets to review. First, let’s make a dataset of tweets that had positive words from the NRC dataset. Remember earlier when we made a dataset of tweets that had “dataviz” and a column that had a value of 1 for containing positive words and 0 for not containing positive words? Let’s resuse that technique, but instead of applying to a dataset of tweets containing “dataviz”, let’s use it on our dataset of all tweets. pos_tweets &lt;- tweets %&gt;% mutate(positive = if_else(status_id %in% pos_tokens$status_id, 1, 0)) %&gt;% filter(positive == 1) Again, we’re using if_else to make a new column called positive that takes its value based on whether status_id %in% pos_tokens$status_id is true or not. We can use slice to help us pick the rows. When we pass slice a row number, it returns that row from the dataset. For example, we can select the 1st and 3rd row of our tweets datset this way: tweets %&gt;% slice(1, 3) ## # A tibble: 2 x 2 ## status_id text ## &lt;chr&gt; &lt;chr&gt; ## 1 1163154266065735… &quot;First #TidyTuesday submission! Roman emperors and their ri… ## 2 1001412196247666… &quot;My #tidytuesday submission for week 8. Honey production da… Randomly selecting rows from a dataset is great technique to have in your toolkit. Random selection helps us avoid some of the biases we all have when we pick rows to review ourselves. Here’s one way to do that using base R: sample(x = 1:10, size = 5) ## [1] 7 4 2 3 5 Passing sample() a vector of numbers and the size of the sample you want returns a random selection from the vector. Try changing the value of x and size to see how this works. dplyr has a version of this called sample_n() that we can use to randomly select rows in our tweets dataset. If we want to specifiy random row numbers for slice to pick for us, we can use sample(1:nrow(.), 10) like this: set.seed(369) pos_tweets %&gt;% sample_n(., size = 10) ## # A tibble: 10 x 3 ## status_id text positive ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 100038669024092… &quot;A few more maps, this time at the county level, f… 1 ## 2 109913022775261… &quot;Unpivotr::behead() &amp;amp; tidyxl can recognize ind… 1 ## 3 106138333884440… &quot;First time working with maps in #ggplot2\\nNothing… 1 ## 4 106757191930893… &quot;I explored data on bridges in Maryland for this w… 1 ## 5 106525318257251… &quot;I was trying to do some practice last night in R … 1 ## 6 108770602257839… &quot;For my first #tidytuesday, I look at the distribu… 1 ## 7 108888393956397… &quot;In this week&#39;s #tidytuesday screencast, I analyze… 1 ## 8 105291401296128… &quot;#TidyTuesday week 2. Took a look at the relations… 1 ## 9 989114304619302… &quot;@dylanjm_ds And let&#39;s not forget that the primary… 1 ## 10 996466149641482… &quot;Is there a way to combine specific values from a … 1 That returned ten randomly selected tweets that we can now read through and discuss. Let’s look a little closer at how we did that. We used sample_n(), which returns randomly selected rows from our tweets dataset. We also specified that size = 10, which means we want sample_n() to give us 10 randomly selected rows. A few lines before that, we used set.seed(369). This helps us ensure that, while sample_n() theoretically plucks 10 random numbers, we want our readers to run this code and get the same result we did. Using set.seed(369) at the top of your code makes sample_n() pick the same ten rows every time. Try changing 369 to another number and notice how sample_n() picks a different set of ten numbers, but repeatedly picks those numbers until you change the argument in set.seed(). 11.9 Conclusion The purpose of this walkthrough is to share code with you so you can practice some basic text analysis techniques. Now it’s time to make your learning more meaningful by adapting this code to text-based files you regularly see at work. Trying reading in some of these and doing a similar analysis: News articles Procedure manuals Open ended responses in surveys There are also advanced text analysis techniques to explore. Consider trying topic modeling (https://www.tidytextmining.com/topicmodeling.html) or finding correlations between terms (https://www.tidytextmining.com/ngrams.html, both from Julia Silge’s and David Robinson’s book. References "],
["c12.html", "12 Walkthrough 6: Exploring Relationships Using Social Network Analysis Models and Methods 12.1 Accessing data 12.2 Preparing the data for the analysis 12.3 Extracting mentions 12.4 Putting the edgelist together 12.5 Plotting the network 12.6 Selection and influence models 12.7 Appendix A 12.8 Appendix B", " 12 Walkthrough 6: Exploring Relationships Using Social Network Analysis Models and Methods In the past, if a teacher wanted advice about how to plan a unit or to design a lesson, they would likely turn to a trusted peer in their building or district (Spillane, Kim, and Frank 2012). In the present, though, they are as likely to turn to someone in the professional learning network (Trust, Krutka, and Carpenter 2016). There are a few reasons to be interested in social media. For example, if you work in a school district, you may be interested in who is interacting with the content you share. If you are a researcher, you may wish to investigate what teachers, administrators, and others do through state-based hashtags (e.g., Rosenberg et al. (2016)). Social media-based data can also be interesting because it provides new contexts for learning to take place, such as learning through informal communities. In this chapter, we focus on a source of data that could be used to understand how one new community functions. That community, #tidytuesday is one sparked by the work of one of the Data Science in Education Using R co-authors, Jesse Mostipak, who created the #r4ds community from which #tidytuesday was created. #tidytuesday is a weekly data visualization challenge. A great place to see examples from past #tidytuesday challenges is an interactive Shiny application (https://github.com/nsgrantham/tidytuesdayrocks). In this walkthrough, we focus on a) accessing data on #tidytuesday from Twitter and b) trying to understand the nature of the interactions that take place through #tidytuesday. We note that while we focused on #tidytuesday because we think it exemplifies the new kinds of learning that a data science toolkit allows an analyst to try to understand (through new data sources), we also chose this because it is straightforward to access data from Twitter, and we think you may find other opportunities to analyze data from Twitter in other cases. 12.1 Accessing data In this chapter, we access data using the rtweet package (Kearney 2016). Through rtweet, it is easy to access data from Twitter as long as one has a Twitter account. We will load the tidyverse and rtweet packages to get started. Here is an example of searching the most recent 1,000 tweets which include the hashtag #rstats. When you run this code, you will be prompted to authenticate your access via Twitter. library(tidyverse) library(rtweet) library(dataedu) library(randomNames) library(tidygraph) library(ggraph) rstats_tweets &lt;- search_tweets(&quot;#rstats&quot;) The search term can be easily changed to other hashtags - or other terms. To search for #tidytuesday tweets, we can simply replace #rstats with #tidytuesday. tidytuesday_tweets &lt;- search_tweets(&quot;#tidytuesday&quot;) A key point–and limitation–for how Twitter allows access to their data for the seven most recent days. There are a number of ways to access older data, which we discuss at the end of this chapter, though we focus on one way here: having access to the URLs to (or the status IDs for) tweets. We used this technique, which we describe in this chapter’s Appendix A, along with other strategies for collecting historical data from Twitter. The data that we processed is available in the dataedu R package as the tt-tweets dataset. tt_tweets ## # A tibble: 4,418 x 90 ## user_id status_id created_at screen_name text source ## &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 115921… 11631542… 2019-08-18 18:22:42 MKumarYYC &quot;Fir… Twitt… ## 2 107332… 11632475… 2019-08-19 00:33:11 cizzart &quot;El … Twitt… ## 3 107332… 11450435… 2019-06-29 18:57:17 cizzart &quot;Pro… Twitt… ## 4 107332… 11168648… 2019-04-13 00:45:15 cizzart &quot;#Ar… Twitt… ## 5 107332… 11228824… 2019-04-29 15:17:02 cizzart &quot;Pes… Twitt… ## 6 107332… 11176387… 2019-04-15 04:00:17 cizzart &quot;Dat… Twitt… ## 7 107332… 11245531… 2019-05-04 05:55:32 cizzart &quot;El … Twitt… ## 8 107332… 11407021… 2019-06-17 19:25:50 cizzart &quot;#da… Twitt… ## 9 107332… 11325299… 2019-05-26 06:12:46 cizzart &quot;El … Twitt… ## 10 107332… 11233585… 2019-04-30 22:48:43 cizzart &quot;Vis… Twitt… ## # … with 4,408 more rows, and 84 more variables: display_text_width &lt;dbl&gt;, ## # reply_to_status_id &lt;chr&gt;, reply_to_user_id &lt;chr&gt;, ## # reply_to_screen_name &lt;chr&gt;, is_quote &lt;lgl&gt;, is_retweet &lt;lgl&gt;, ## # favorite_count &lt;int&gt;, retweet_count &lt;int&gt;, quote_count &lt;int&gt;, ## # reply_count &lt;int&gt;, hashtags &lt;list&gt;, symbols &lt;list&gt;, urls_url &lt;list&gt;, ## # urls_t.co &lt;list&gt;, urls_expanded_url &lt;list&gt;, media_url &lt;list&gt;, ## # media_t.co &lt;list&gt;, media_expanded_url &lt;list&gt;, media_type &lt;list&gt;, ## # ext_media_url &lt;list&gt;, ext_media_t.co &lt;list&gt;, ext_media_expanded_url &lt;list&gt;, ## # ext_media_type &lt;chr&gt;, mentions_user_id &lt;list&gt;, mentions_screen_name &lt;list&gt;, ## # lang &lt;chr&gt;, quoted_status_id &lt;chr&gt;, quoted_text &lt;chr&gt;, ## # quoted_created_at &lt;dttm&gt;, quoted_source &lt;chr&gt;, quoted_favorite_count &lt;int&gt;, ## # quoted_retweet_count &lt;int&gt;, quoted_user_id &lt;chr&gt;, quoted_screen_name &lt;chr&gt;, ## # quoted_name &lt;chr&gt;, quoted_followers_count &lt;int&gt;, ## # quoted_friends_count &lt;int&gt;, quoted_statuses_count &lt;int&gt;, ## # quoted_location &lt;chr&gt;, quoted_description &lt;chr&gt;, quoted_verified &lt;lgl&gt;, ## # retweet_status_id &lt;chr&gt;, retweet_text &lt;chr&gt;, retweet_created_at &lt;dttm&gt;, ## # retweet_source &lt;chr&gt;, retweet_favorite_count &lt;int&gt;, ## # retweet_retweet_count &lt;int&gt;, retweet_user_id &lt;chr&gt;, ## # retweet_screen_name &lt;chr&gt;, retweet_name &lt;chr&gt;, ## # retweet_followers_count &lt;int&gt;, retweet_friends_count &lt;int&gt;, ## # retweet_statuses_count &lt;int&gt;, retweet_location &lt;chr&gt;, ## # retweet_description &lt;chr&gt;, retweet_verified &lt;lgl&gt;, place_url &lt;chr&gt;, ## # place_name &lt;chr&gt;, place_full_name &lt;chr&gt;, place_type &lt;chr&gt;, country &lt;chr&gt;, ## # country_code &lt;chr&gt;, geo_coords &lt;list&gt;, coords_coords &lt;list&gt;, ## # bbox_coords &lt;list&gt;, status_url &lt;chr&gt;, name &lt;chr&gt;, location &lt;chr&gt;, ## # description &lt;chr&gt;, url &lt;chr&gt;, protected &lt;lgl&gt;, followers_count &lt;int&gt;, ## # friends_count &lt;int&gt;, listed_count &lt;int&gt;, statuses_count &lt;int&gt;, ## # favourites_count &lt;int&gt;, account_created_at &lt;dttm&gt;, verified &lt;lgl&gt;, ## # profile_url &lt;chr&gt;, profile_expanded_url &lt;chr&gt;, account_lang &lt;lgl&gt;, ## # profile_banner_url &lt;chr&gt;, profile_background_url &lt;chr&gt;, ## # profile_image_url &lt;chr&gt; 12.2 Preparing the data for the analysis Network data, in general, and network data from Twitter, particularly, requires some processing before it can be used in subsequent analyses. In particular, we are going to create an edgelist, a data structure that is especially helpful for understanding the nature of relationships. An edgelist looks like the following, where the sender denotes who is initiating the interaction or relationship, and the receiver is who is the recipient of it: ## # A tibble: 12 x 2 ## sender receiver ## &lt;chr&gt; &lt;chr&gt; ## 1 Carpinteyro, Cassandra Hughes, Colton ## 2 Hutter, Elizaveta Rodriguez, Louis ## 3 Hutter, Elizaveta Bennett, Kayla ## 4 Johnson, Hannah Rodriguez, Louis ## 5 Johnson, Hannah Hughes, Colton ## 6 Johnson, Hannah Binns, Tre ## 7 Martin, Derek Bennett, Kayla ## 8 Martin, Derek Toberman, Thomas ## 9 Martin, Derek Binns, Tre ## 10 Ranchhod, Stephen Salaz, Jalisa ## 11 Loftis, Rachel Bennett, Kayla ## 12 Loftis, Rachel Salaz, Jalisa In this edgelist, the sender could indicate, for example, someone who nominates someone else (the receiver) as someone they go to for help. The sender could also indicate someone who interacted with the receiver, such as by recognizing one of their tweets with a favorite (or a mention). In the following steps, we will work to create an edgelist from the data from #tidytuesday on Twitter. 12.3 Extracting mentions Let’s extract the mentions. There is a lot going on in the code below; let’s break it down line-by-line, starting with the mutate(): mutate(all_mentions = str_extract_all(text, regex)): this line uses a regex, or regular expression, to identify all of the usernames in the tweet (note: the regex comes from from this page) mutate(has_mention = ifelse(!is.na(all_mentions), TRUE, FALSE)): this line simply determines there are any mentions at all (or not); it is true if there is one or more mention unnest(all_mentions) this line uses a tidyr function, unnest() to move every mention to its own line, while keeping all of the other information the same (see more about unnest() here: https://tidyr.tidyverse.org/reference/unnest.html) regex &lt;- &quot;@([A-Za-z]+[A-Za-z0-9_]+)(?![A-Za-z0-9_]*\\\\.)&quot; tt_tweets &lt;- tt_tweets %&gt;% mutate(all_mentions = str_extract_all(text, regex)) %&gt;% mutate(has_mention = if_else(!is.na(all_mentions), TRUE, FALSE)) %&gt;% unnest(all_mentions) Let’s put these into their own data frame, called mentions. mentions &lt;- tt_tweets %&gt;% filter(has_mention) %&gt;% mutate(all_mentions = str_trim(all_mentions)) %&gt;% select(sender = screen_name, all_mentions) 12.4 Putting the edgelist together An edgelist is a common social network analysis data structure that has columns for the “sender” and “receiver” of interactions, or relations. For example, someone “sends” the mention to someone who is mentioned, who can be considered to “receive” it. This will require one last processing step. Let’s look at our data as it is now. mentions ## # A tibble: 2,447 x 2 ## sender all_mentions ## &lt;chr&gt; &lt;chr&gt; ## 1 cizzart @eldestapeweb ## 2 cizzart @INDECArgentina ## 3 cizzart @ENACOMArgentina ## 4 cizzart @tribunalelecmns ## 5 cizzart @CamaraElectoral ## 6 cizzart @INDECArgentina ## 7 cizzart @tribunalelecmns ## 8 cizzart @CamaraElectoral ## 9 cizzart @AgroMnes ## 10 cizzart @AgroindustriaAR ## # … with 2,437 more rows What needs to happen to these to make them easier to work with in an edgelist? One step is to remove the “@” symbol from the columns we created and to save the results to a new tibble, edgelist. edgelist &lt;- mentions %&gt;% mutate(all_mentions= str_sub(all_mentions, start = 2)) %&gt;% select(sender, receiver = all_mentions) 12.5 Plotting the network Now that we have our edgelist, it is straightforward to plot the network. We’ll use the {tidygraph} and {ggraph} packages to visualize the data. We’ll use the as_tbl_graph() function, which (by default) identified the first column as the “sender” and the second as the “receiver.” Let’s look at the object it creates, too. g &lt;- as_tbl_graph(edgelist) g ## # A tbl_graph: 1340 nodes and 2447 edges ## # ## # A directed multigraph with 130 components ## # ## # Node Data: 1,340 x 1 (active) ## name ## &lt;chr&gt; ## 1 cizzart ## 2 dgwinfred ## 3 davidmasp ## 4 datawookie ## 5 jvaghela4 ## 6 FournierJohanie ## # … with 1,334 more rows ## # ## # Edge Data: 2,447 x 2 ## from to ## &lt;int&gt; &lt;int&gt; ## 1 1 619 ## 2 1 620 ## 3 1 621 ## # … with 2,444 more rows Next, we’ll use the ggraph() function. Run the code below, and then uncomment, one at a time, the next two lines (the two beginning geom_(), running the code after uncommenting each line). g %&gt;% ggraph() + geom_node_point() + # geom_node_text(aes(label = name)) + # geom_edge_link() + theme_graph() ## Using `stress` as default layout Finally, lets size the points based on a measure of centrality, typically a measure of how (potentially) influence an individual may be, based on the interactions observed. g %&gt;% mutate(centrality = centrality_authority()) %&gt;% ggraph() + geom_node_point(aes(size = centrality, color = centrality)) + scale_color_continuous(guide = &#39;legend&#39;) + geom_node_text(aes(label = name)) + geom_edge_link() + theme_graph() ## Using `stress` as default layout There is much more you can do with ggraph (and tidygraph); check out the ggraph tutorial here: https://ggraph.data-imaginist.com/ 12.6 Selection and influence models Behind these visualizations, though, there are also statistical models and methods that can help to understand what is going on with respect to particular relationships in a network in additional ways. One way to consider these models and methods is in terms of two processes at play in our relationships (cite). These two processes are commonly (though not exclusively) the focus of statistical analyses of networks. In addition to not being exclusive, they do not interact independently: they affect each other reciprocally (Xu, Frank, &amp; Penuel, 2018). They are: Selection: the processes regarding who chooses to have a relationship with whom Infuence: the processes regarding how who we have relationships with affects our behavior While these are complex, they can be studied with the type of data collected from asking people about their relationships (and possibly asking them about or studying their behavior–or measuring some outcome). Happily, the use of these methods has expanded along with R: many of the best tools for studying social networks are in the form of long-standing R packages. Additionally, while there are many potential nuances to studying selection and influence, these are models that can fundamentally be carried out with regression, or the linear model (or extensions of it). We describe these in Appendix B for this chapter, as they do not use the tidytuesday dataset and are likely to be of interest to readers only after having mastered preparing and visualizing network data. 12.7 Appendix A 12.7.1 Accessing historical data using status URLs Because the creator of the interactive web application for exploring #tidytuesday content, #tidytuesday.rocks, searched for (and archived) #tidytuesday tweets on a regular basis, a large data set from more than one year of weekly #tidytuesday challenges is available through the GitHub repository for the Shiny application. These Tweets (saved in the data directory) can be read with the following function raw_tidytuesday_tweets &lt;- read_delim( &quot;https://raw.githubusercontent.com/nsgrantham/tidytuesdayrocks/master/data/tweets.tsv&quot;, &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE ) ## Parsed with column specification: ## cols( ## status_url = col_character(), ## screen_name = col_character(), ## created_at = col_datetime(format = &quot;&quot;), ## favorite_count = col_double(), ## retweet_count = col_double(), ## dataset_id = col_character() ## ) Then the URL for the tweet (the status_url column) can be passed to a different rtweet function than the one we used, lookup_statuses(). Before we do this, there is one additional step to take. Because most of the Tweets are from more than seven days ago, Twitter requires an additional authentication step. In short, you need to use keys and tokens for the Twitter API, or application programming interface. The rtweet vignette on accessing keys and tokens (https://rtweet.info/articles/auth.html) explains the process. The end result will be that you will create a token using rtweet that you will use along with your rtweet function (in this case, lookup_statuses()): token &lt;- create_token( consumer_key = &lt; add - your - key - here &gt; , consumer_secret = &lt; add - your - secret - here &gt; ) # here, we pass the status_url variable from raw_tidytuesday_tweets as the statuses to lookup in the lookup_statuses() function, as well as our token tidytuesday_tweets &lt;- lookup_statuses(raw_tidytuesday_tweets$status_url, token = token) The end result will be a tibble, like that above for #rstats, for #tidytuesday tweets. 12.7.2 Accessing historical data when you do not have access to status URLs In the above case, we had access to the URLs for tweets because they were saved for the #tidytuesday.rocks Shiny. But, in many cases, historical data will not be available. There are two strategies that may be helpful. First is TAGS. TAGS is based in, believe it or not, Google Sheets, and it works great for collecting Twitter data over time - even a long period of time The only catch is that you need to setup and start to use a TAGS sheet in advance of the period for which you want to collect data. For example, you can start a TAGS archiver in August of one year, with the intention to collect data over the coming academic year; or, you can start a TAGS archiver before an academic conference for which you want to collect Tweets. A second option is the Premium API through Twitter. This is an expensive option, but is one that can be done through rtweet, and can also access historical data, even if you haven not started a TAGS sheet and do not otherwise have access to the status URLs. 12.8 Appendix B As noted above, there is much more to understanding interactions, and network analysis, beyond creating edgelists and visualizing network data (through the use of an edgelist). Two processes that are particularly important (and able to be studied with network data using R) are for influence and selection. 12.8.1 An example of influence First, let’s look at an example of influence. To do so, let’s create three different data frames. Here is what they should, at the end of the process, contain: A data frame indicating who the nominator and nominee for the relation (i.e., if Stefanie says that José is her friend, then Stefanie is the nominator and José the nominee) - as well as an optional variable indicating the weight, or strength, of their relation. This data frame and its type can be considered the basis for many types of social network analysis and is a common structure for network data: it is an edgelist. Data frames indicating the values of some behavior - an outcome - at two different time points. In this example, we create some example data that can be used to explore questions about how influence works. Let’s take a look at the merged data. What this data now contains is the first data frame, data1, with each nominees’ outcome at time 1 (yvar1). Note that we will find each nominators’ outcome at time 2 later on. data1 &lt;- data.frame( nominator = c(2, 1, 3, 1, 2, 6, 3, 5, 6, 4, 3, 4), nominee = c(1, 2, 2, 3, 3, 3, 4, 4, 4, 5, 6, 6), relate = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) ) data2 &lt;- data.frame(nominee = c(1, 2, 3, 4, 5, 6), yvar1 = c(2.4, 2.6, 1.1, -0.5, -3, -1)) data3 &lt;- data.frame(nominator = c(1, 2, 3, 4, 5, 6), yvar2 = c(2, 2, 1, -0.5, -2, -0.5)) 12.8.2 Joining the data Next, we’ll join the data into one data frame. Note that while this (and data wrangling, in general) is sometimes tedius, especially with large (and messy) sources of network data, it is a key step for being able to carry out network analysis - often, even for creating visualiations that are informative. Next, we’ll join the data into one data frame. Note that while this is sometimes tedius and time-consuming, especially with large sources of network data, it is a key step for being able to carry out network analysis - often, even for creating visualiations that are informative. data &lt;- left_join(data1, data2, by = &quot;nominee&quot;) data &lt;- data %&gt;% mutate(nominee = as.character(nominee)) # this makes merging later easier # calculate indegree in tempdata and merge with data tempdata &lt;- data.frame(table(data$nominee)) tempdata &lt;- tempdata %&gt;% rename(&quot;nominee&quot; = &quot;Var1&quot;, # rename the column &quot;Var1&quot; to &quot;nominee&quot; &quot;indegree&quot; = &quot;Freq&quot;) %&gt;% # rename the column &quot;Freq&quot; to &quot;indegree&quot; mutate(nominee = as.character(nominee)) # makes nominee a character data type, instead of a factor, which can cause problems data &lt;- left_join(data, tempdata, by = &quot;nominee&quot;) 12.8.2.1 Calculating an exposure term This is the key step that makes this model - a regression, or linear, model - one that is special. It is creating an exposure term. The idea is that the exposure term “captures” how your interactions with someone, over some period of time (between the first and second time points) impact some outcome. This model accounts for an individual’s initial report of the outcome, i.e., their time 1 prior value, so it is a model for change in some outcome. # Calculating exposure data &lt;- data %&gt;% mutate(exposure = relate * yvar1) # Calculating mean exposure mean_exposure &lt;- data %&gt;% group_by(nominator) %&gt;% summarize(exposure_mean = mean(exposure)) What this data frame - mean_exposure - contains is the mean of the outcome (in this case, yvar1) for all of the individuals the nominator had a relation with. As we need a final data set with mean_exposure,degree, yvar1, and yvar2 added, we’ll process the data a bit more. data2 &lt;- data2 %&gt;% rename(&quot;nominator&quot; = &quot;nominee&quot;) # rename nominee as nominator to merge these final_data &lt;- left_join(mean_exposure, data2, by = &quot;nominator&quot;) final_data &lt;- left_join(final_data, data3, by = &quot;nominator&quot;) # data3 already has nominator, so no need to change 12.8.2.2 Regression (linear model) Calculating the exposure term is the most distinctive and important step in carrying out influence models. Now, we can simply use a linear model to find out how much relations - as captured by the influence term - affect some outcome. model1 &lt;- lm(yvar2 ~ yvar1 + exposure_mean, data = final_data) summary(model1) ## ## Call: ## lm(formula = yvar2 ~ yvar1 + exposure_mean, data = final_data) ## ## Residuals: ## 1 2 3 4 5 6 ## 0.02946 -0.09319 0.09429 -0.02730 -0.02548 0.02222 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.11614 0.03445 3.371 0.0434 * ## yvar1 0.67598 0.02406 28.092 9.9e-05 *** ## exposure_mean 0.12542 0.03615 3.470 0.0403 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.08232 on 3 degrees of freedom ## Multiple R-squared: 0.9984, Adjusted R-squared: 0.9974 ## F-statistic: 945.3 on 2 and 3 DF, p-value: 6.306e-05 So, the influence model is used to study a key process for social network analysis, but it is one that is useful, because you can quantify, given what you measure and how you measure it, the network effect, something that is sometimes not considered, especially in education (Frank, 2009). It’s also fundamentally a regression. That’s really it, as the majority of the work goes into calculating the exposure term. 12.8.3 An example of selection Selection models are also commonly used - and are commonly of interest not only to researchers but also to administrators and teachers (and even to youth and students). Here, we briefly describe a few possible approaches for using a selection model. At its core, the selection model is a regression - albeit, one that is a generalization of one, namely, a logistic regression (sometimes termed a generalized linear model, because it is basically a regression but is one with an outcome that consists just of 0’s and 1’s). Thus, the most straight-away way to use a selection model is to use a logistic regression where all of the relations (note the relate variable in data1 above) are indicated with a 1. But, here is the important and challenging step: all of the possible relations (i.e., all of the relations that are possible between all of the individuals in a network) are indicated with a 0 in an edgelist. Note that, again, an edgelist is the preferred data structure for carrying out this analysis. This step involves some data wrangling, especially the idea of widening or lengthening a data frame. Once all of the relations are indicated with a 1 or a 0, then a simple linear regression can be used. Imagine that we are interested in whether individuals from the same group are more or less likely to interact than those from different groups; same could be created in the data frame based upon knowing which group both nominator and nominee are from: m_selection &lt;- glm(relate ~ 1 + same, data = edgelist1) While this is a straightforward way to carry out a selection model, there are some limitations to it. Namely, it does not account for individuals who send more (or less) nominations overall–and not considering this may mean other effects, like the one associated with being from the same group, are not accurate. A few extensions of the linear model - including those that can use data for which relationships are indicated with weights, not just 1’s and 0’s, have been developed. One type of model extends the logistic regression. It can be used for data that is not only 1’s and 0’s but also data that is normally distributed . It is the amen package available here. A particularly common one is an Exponential Random Graph Model, or an ERGM. An R package that makes estimating these easy is available here. That R package, ergm, is part of a powerful and often-used collection of packages, including those for working with network data (data that can begin with an edgelist, but may need additional processing that is challenging to do with edgelist data), statnet. A link to the statnet packages is here. Trust, T., Krutka, D. G., &amp; Carpenter, J. P. (2016). “Together we are better”: Professional learning networks for teachers. Computers &amp; education, 102, 15-34. References "],
["c13.html", "13 Walkthrough 7: The Role (and Usefulness) of Multi-Level Models 13.1 But what about different courses? 13.2 Toward multi-level models 13.3 Other groups", " 13 Walkthrough 7: The Role (and Usefulness) of Multi-Level Models This walkthrough accompanies the previous chapter, which focused on preparing the data and beginning to viusualize and model the data. Here, we focus on an extension of the models we ran, one focused on how to address the fact that students in our dataset shared classes. As for the earlier walkthrough using the same data, the purpose for this walkthrough is to explore students’ performance in these online courses, focusing on the time spent in the course (made available throygh the learning management system) and the effects of being in a particular class. First, let’s load the tidyverse and read the data we processed in walkthrough 1. library(tidyverse) library(dummies) library(sjPlot) library(lme4) library(performance) dat &lt;- dataedu::sci_mo_processed 13.1 But what about different courses? Are there course-specific differences in how much time students spend on the course as well as in how time spent is related to the percentage of points students earned? There are a number of ways to approach this question. Let’s use our linear model. Specifically, we can dummy-code the groups. Dummy coding means transforming a variable with multiple categories into multiple, new variables, where each variable indicates the presence and absence of only one of the categories. 13.1.1 The role of dummy codes We can see how dummy coding works through using the {dummies} package, though, as we will see, you often do not need to manually dummy code variables like this. Let’s consider the iris data that comes built into R, but, since we are fans of the {tidyverse}, we will first change it into a tibble. iris &lt;- as_tibble(iris) iris ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows As we can see above, the Species variable is a factor. If we consider how we could include a variable such as this in a linear model, things may become a little confusing. Species seems to be made up of, well, words, such as “setosa.” The common way to approach this is through dummy coding, where you create new variables for each of the possible values of Species (such as “setosa”). Then, these new variables have a value of 1 when the row is associated with that level (i.e., the first row in the data frame above would have a 1 for a column named setosa). Let’s return to {dummies}. How many possible values are there for Species? We can check with the levels function. levels(iris$Species) ## [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; When we run the dummy() function on the Species variable, we can see that it returns three variables, one for each of the three levels of Species - “setosa”, “versicolor”, and “virginica”. dummies::dummy(iris$Species) %&gt;% head() ## Warning in model.matrix.default(~x - 1, model.frame(~x - 1), contrasts = FALSE): ## non-list contrasts argument ignored ## {\\n args = commandArgs(TRUE)\\n out = do.call(rmarkdown::render, c(args[1], readRDS(args[2]), list(run_pandoc = FALSE, encoding = &quot;UTF-8&quot;)))\\n out_expected = xfun::with_ext(args[1], &quot;.md&quot;)\\n if (out != out_expected) {\\n file.rename(out, out_expected)\\n attributes(out_expected) = attributes(out)\\n out = out_expected\\n }\\n if (file.exists(args[3])) {\\n res = readRDS(args[3])\\n res[[args[1]]] = out\\n saveRDS(res, args[3])\\n }\\n else saveRDS(setNames(list(out), args[1]), args[3])\\n}setosa ## [1,] 1 ## [2,] 1 ## [3,] 1 ## [4,] 1 ## [5,] 1 ## [6,] 1 ## {\\n args = commandArgs(TRUE)\\n out = do.call(rmarkdown::render, c(args[1], readRDS(args[2]), list(run_pandoc = FALSE, encoding = &quot;UTF-8&quot;)))\\n out_expected = xfun::with_ext(args[1], &quot;.md&quot;)\\n if (out != out_expected) {\\n file.rename(out, out_expected)\\n attributes(out_expected) = attributes(out)\\n out = out_expected\\n }\\n if (file.exists(args[3])) {\\n res = readRDS(args[3])\\n res[[args[1]]] = out\\n saveRDS(res, args[3])\\n }\\n else saveRDS(setNames(list(out), args[1]), args[3])\\n}versicolor ## [1,] 0 ## [2,] 0 ## [3,] 0 ## [4,] 0 ## [5,] 0 ## [6,] 0 ## {\\n args = commandArgs(TRUE)\\n out = do.call(rmarkdown::render, c(args[1], readRDS(args[2]), list(run_pandoc = FALSE, encoding = &quot;UTF-8&quot;)))\\n out_expected = xfun::with_ext(args[1], &quot;.md&quot;)\\n if (out != out_expected) {\\n file.rename(out, out_expected)\\n attributes(out_expected) = attributes(out)\\n out = out_expected\\n }\\n if (file.exists(args[3])) {\\n res = readRDS(args[3])\\n res[[args[1]]] = out\\n saveRDS(res, args[3])\\n }\\n else saveRDS(setNames(list(out), args[1]), args[3])\\n}virginica ## [1,] 0 ## [2,] 0 ## [3,] 0 ## [4,] 0 ## [5,] 0 ## [6,] 0 We can confirm that every row associated with a specific species has a 1 in the column it corresponds to. We can do this by binding together the dummy codes and the iris data and then counting, for each of the three species, how many of the rows for each dummy code were coded with a “1”. For example, when the Species is “setosa”, the variable Speciessetosa always equals 1 - as is the case for the other species (for their respective variables). bind_cols() is a useful tidyverse function for binding together data frames by column. species_dummy_coded &lt;- dummies::dummy(iris$Species) ## Warning in model.matrix.default(~x - 1, model.frame(~x - 1), contrasts = FALSE): ## non-list contrasts argument ignored species_dummy_coded &lt;- as_tibble(species_dummy_coded) iris_with_dummy_codes &lt;- bind_cols(iris, species_dummy_coded) Let’s look at the results. iris_with_dummy_codes %&gt;% count(Species, Speciessetosa, Speciesversicolor, Speciesvirginica) Okay, this covers how dummy codes work: but, how do they work when used in a model, like with the linear model we have been using (through lm())? In the context of using lm() (and many other functions in R) is that the number of levels to be created is always the number of different possible values minus one, because each group will be modeled in comparison to the group without a column, or what is commonly called the reference group. Why can every group not simply have their own dummy-coded column? The reason has to do with how the dummy codes are used. The purpose of the dummy code is to show how different the dependent variable is for all of the observations that are in one group (i.e., all of the flowers that are setosa specimens). In order to represent how different those flowers are, they have to be compared to something else - and the intercept of the model usually represents this “something else.” However, if every level of a factor (such as Species) is dummy-coded, then there would be no cases available to estimate an intercept - in short, the dummy code would not be compared to anything else. For this reason, one group is typically selected as the reference group, to which every other group is compared. 13.1.2 Using dummy codes This may be clearer with an example. Let’s return to our online science class data and consider the effect (for a student) of being in a specific class in the data set. First, let’s determine how many classes there are. We can use the count() function. dat %&gt;% count(course_id) %&gt;% nrow() ## [1] 26 There are 26 distinct courses. We will save this output to m_linear_courses, where the dc stands for dummy code. We will keep the variables we used in our last set of models - TimeSpent and subject - as independent variables. m_linear_dc &lt;- lm(percentage_earned ~ TimeSpent_std + course_id, data = dat) The output will be a bit, well, long, because each group will have its own intercept. Here it is: sjPlot::tab_model(m_linear_dc) percentage earned Predictors Estimates CI p (Intercept) 0.80 0.75 – 0.84 &lt;0.001 TimeSpent_std 0.01 -0.01 – 0.02 0.428 course_id [AnPhA-S116-02] -0.02 -0.10 – 0.05 0.525 course_id [AnPhA-S216-01] -0.14 -0.21 – -0.07 &lt;0.001 course_id [AnPhA-S216-02] -0.04 -0.13 – 0.05 0.393 course_id [AnPhA-T116-01] -0.03 -0.21 – 0.16 0.771 course_id [BioA-S116-01] -0.02 -0.10 – 0.07 0.717 course_id [BioA-S216-01] 0.09 -0.10 – 0.27 0.348 course_id [FrScA-S116-01] -0.05 -0.11 – 0.01 0.081 course_id [FrScA-S116-02] 0.10 -0.09 – 0.28 0.306 course_id [FrScA-S116-03] -0.06 -0.14 – 0.03 0.195 course_id [FrScA-S216-01] -0.03 -0.09 – 0.03 0.306 course_id [FrScA-S216-02] -0.07 -0.16 – 0.01 0.102 course_id [FrScA-S216-03] 0.08 -0.11 – 0.26 0.411 course_id [FrScA-S216-04] -0.03 -0.16 – 0.10 0.657 course_id [FrScA-T116-01] 0.00 -0.18 – 0.19 0.987 course_id [OcnA-S116-01] -0.00 -0.09 – 0.08 0.961 course_id [OcnA-S116-02] 0.03 -0.15 – 0.21 0.740 course_id [OcnA-S216-01] -0.05 -0.12 – 0.02 0.193 course_id [OcnA-S216-02] 0.05 -0.08 – 0.19 0.445 course_id [PhysA-S116-01] -0.01 -0.08 – 0.06 0.742 course_id [PhysA-S216-01] -0.03 -0.21 – 0.16 0.756 Observations 148 R2 / R2 adjusted 0.184 / 0.048 Wow! That is a lot of effects. In addition to the time spent and subject variables, the model estimated the difference, accounting for the effects of being a student in a specific class. Let’s count how many classes there are. If we count the number of classes, we see that there are 25 - and not 26! One has been automatically selected as the reference group, and every other class’s coefficient represents how different each class is from it. The intercept’s value of 0.74 represents the percentage of points that students in the reference group class, which is automatically the first level of the course_id variable when it is converted to a factor, “course_idAnPhA-S116-01” (which represents an anatomy and physiology course from semester S1 (for the fall) of 2016; this is the first section (01)). We can easily choose another class to serve as a reference group. Imagine, for example, that we want “course_idPhysA-S116-01” (the first section of the physics class offered during this semester and year) to be the reference group. The fct_relevel() function (which is a part of the {tidyverse} suite of packages) makes it easy to do this. This function allows us to re-order the levels within a factor, so that the “first” level will change. We’ll also use mutate again here, which we introduced in the previous chapter. dat &lt;- dat %&gt;% mutate(course_id = fct_relevel(course_id, &quot;PhysA-S116-01&quot;)) We can now see that that group is no longer listed as an independent variable, or a predictor: every coefficient in this model is now in reference to it. # Here we run a linear model again, predicting percentage earned in the course # The predictor variables are the (standardized) amount of time spent and the subject of the course (course_id) m_linear_dc_1 &lt;- lm(percentage_earned ~ TimeSpent_std + course_id, data = dat) sjPlot::tab_model(m_linear_dc_1) percentage earned Predictors Estimates CI p (Intercept) 0.78 0.73 – 0.83 &lt;0.001 TimeSpent_std 0.01 -0.01 – 0.02 0.428 course_id [AnPhA-S116-01] 0.01 -0.06 – 0.08 0.742 course_id [AnPhA-S116-02] -0.01 -0.09 – 0.06 0.747 course_id [AnPhA-S216-01] -0.13 -0.21 – -0.06 0.001 course_id [AnPhA-S216-02] -0.03 -0.12 – 0.06 0.541 course_id [AnPhA-T116-01] -0.02 -0.20 – 0.17 0.864 course_id [BioA-S116-01] -0.00 -0.09 – 0.08 0.922 course_id [BioA-S216-01] 0.10 -0.09 – 0.28 0.293 course_id [FrScA-S116-01] -0.04 -0.11 – 0.02 0.187 course_id [FrScA-S116-02] 0.11 -0.08 – 0.29 0.253 course_id [FrScA-S116-03] -0.04 -0.13 – 0.04 0.315 course_id [FrScA-S216-01] -0.02 -0.08 – 0.04 0.545 course_id [FrScA-S216-02] -0.06 -0.15 – 0.03 0.177 course_id [FrScA-S216-03] 0.09 -0.10 – 0.27 0.347 course_id [FrScA-S216-04] -0.02 -0.15 – 0.12 0.780 course_id [FrScA-T116-01] 0.01 -0.17 – 0.20 0.892 course_id [OcnA-S116-01] 0.01 -0.08 – 0.10 0.839 course_id [OcnA-S116-02] 0.04 -0.14 – 0.23 0.652 course_id [OcnA-S216-01] -0.03 -0.11 – 0.04 0.344 course_id [OcnA-S216-02] 0.06 -0.07 – 0.20 0.356 course_id [PhysA-S216-01] -0.02 -0.20 – 0.17 0.848 Observations 148 R2 / R2 adjusted 0.184 / 0.048 Using dummy codes is very common - they are used in nearly every case in which you are using a model (such as a linear model, through lm()) and you have variables that are factors. A benefit of using lm() (and many other functions) in R for modeling, such as the lme4::lmer() function we discuss later, is that if you have variables which are not factors, but simply character strings, they will be automatically changed to factors when used in a model. This means, for instance, that if you have a variable for the subject matter of courses labeled “mathematics”, “science”, “english language” (typed like that!), “social studies”, and “art”, and you include this variable in an lm() model, then the function will automatically dummy-code these for you. The only essential step that is not taken for you is choosing which is the reference group. We note that there are cases in which not having a reference group that the other, dummy-coded groups are compared to is desired. In such cases, no intercept is estimated. This can be done by passing a -1 as the first value after the tilde, as follows: # specifying the same linear model as the previous example, but using a &quot;-1&quot; to indicate that there should not be a reference group m_linear_dc_2 &lt;- lm(percentage_earned ~ -1 + TimeSpent_std + course_id, data = dat) sjPlot::tab_model(m_linear_dc_2) percentage earned Predictors Estimates CI p TimeSpent_std 0.01 -0.01 – 0.02 0.428 course_id [PhysA-S116-01] 0.78 0.73 – 0.83 &lt;0.001 course_id [AnPhA-S116-01] 0.80 0.75 – 0.84 &lt;0.001 course_id [AnPhA-S116-02] 0.77 0.71 – 0.83 &lt;0.001 course_id [AnPhA-S216-01] 0.65 0.59 – 0.71 &lt;0.001 course_id [AnPhA-S216-02] 0.75 0.67 – 0.83 &lt;0.001 course_id [AnPhA-T116-01] 0.77 0.59 – 0.95 &lt;0.001 course_id [BioA-S116-01] 0.78 0.71 – 0.85 &lt;0.001 course_id [BioA-S216-01] 0.88 0.70 – 1.06 &lt;0.001 course_id [FrScA-S116-01] 0.74 0.70 – 0.78 &lt;0.001 course_id [FrScA-S116-02] 0.89 0.71 – 1.07 &lt;0.001 course_id [FrScA-S116-03] 0.74 0.67 – 0.81 &lt;0.001 course_id [FrScA-S216-01] 0.77 0.73 – 0.80 &lt;0.001 course_id [FrScA-S216-02] 0.72 0.65 – 0.80 &lt;0.001 course_id [FrScA-S216-03] 0.87 0.69 – 1.05 &lt;0.001 course_id [FrScA-S216-04] 0.76 0.64 – 0.89 &lt;0.001 course_id [FrScA-T116-01] 0.80 0.62 – 0.98 &lt;0.001 course_id [OcnA-S116-01] 0.79 0.72 – 0.87 &lt;0.001 course_id [OcnA-S116-02] 0.83 0.65 – 1.00 &lt;0.001 course_id [OcnA-S216-01] 0.75 0.70 – 0.80 &lt;0.001 course_id [OcnA-S216-02] 0.85 0.72 – 0.97 &lt;0.001 course_id [PhysA-S216-01] 0.77 0.59 – 0.94 &lt;0.001 Observations 148 R2 / R2 adjusted 0.988 / 0.986 This does not work in many cases, and it is much more common to dummy-code factors, and so we emphasized that in this walkthrough. However, we want you to be aware that it is possible (though uncommon) to estimate a model without an intercept. 13.2 Toward multi-level models Dummy-coding is a very helpful strategy. It is particularly useful with a small number of groups (i.e., for estimating the effects of being in one of the five subjects in the online science data set, as in this walkthrough; we note that in addition to these five subjects, we also have multiple sections, or classes, for each subject). With effects such as being a student in a particular class, though, the output seems to be less useful: it is hard to interpret the 25 different effects (and to compare them to the intercept). Additionally, analysts often have the goal not of determining the effect of being in a specific class, per se, but rather of accounting for the fact that students share a class. This is important because linear models (i.e., the model we estimated using lm()) have an assumption that the data points are - apart from sharing levels of the variables that are used in the model - independent, or not correlated. This is what is meant by the “assumption of independence” or of “independently and identically distributed” (i.i.d.) residuals (Field, Miles, &amp; Field, 2012). Multi-level models are a way to deal with the difficulty of interpreting the estimated effects for each of many groups, like classes, and to address the assumption of independence. Multi-level models do this by (still) estimating the effect of being a student in each group, but with a key distinction from linear models: Instead of determining how different the observations in a group are from those in the reference group, the multi-level model “regularizes” (sometimes the term “shrinks” is used) the difference based on how systematically different the groups are. The reason why “shrinkage” is occasionally used is that the group-level estimates (i.e., for classes) that are obtained through multi-level modeling can never be larger than those from a linear model (regression). As described earlier, when there are groups included in the model, a regression effectively estimates the effect for each group independent of all of the others. Through regularization, groups that are comprised of individuals who are consistently different (higher or lower) than individuals on average are not regularized very much - their estimated difference may be close to the estimate from a multi-level model - whereas groups with only a few individuals, or with a lot of variability within individuals, would be regularized a lot. In the former case, the multi-level model considers there to be strong evidence for a group effect, whereas in the latter, the model recognizes that there is less certainty about a group (class) effect for that particular group, in part because that group is small. Multi-level models are very common in educational research for cases such as this: accounting for the way in which students take the same classes, or even go to the same school (see Raudenbush &amp; Bryk, 2002). The way that a multi-level model does this “regularizing” is by considering the groups (and not the data points, in this case) to be samples from a larger population of classes. By considering the effects of groups to be samples from a larger population, the model is able to use information not only particular to each group (as the models created using lm()), but also information across all of the data. Using multi-level models means that the assumption of independence can be addressed; their use also means that individual coefficients for classes do not need to be included (or interpreted, thankfully!), though they are still included in and accounted for in the model. As we describe, the way that information about the groups is reported is usually in the form of the intraclass correlation coefficient (ICC), which explains the proportion of variation in the dependent variable that the groups explain. Smaller ICCs (such as ICCs with values of 0.05, representing 5% of the variation in the dependent variable) mean that the groups are not very important; larger ICCs, such as ICCs with values of 0.10 or larger (values as high as 0.50 are not uncommon!). ICCs that are larger would indicate that groups are important and that they have to do with a lot of the differences observed in the dependent variable (and that not including them may potentially ignore the assumption of independence in a case in which it may be important to recognize it - and lead to bias in the results). That was a lot of technical information about multi-level models; thank you for sticking with us through it! We wanted to include this as multi-level models are common: consider how often the data you collect involves students nested (or grouped) in classes, or classes nested in schools (or even schools nested in districts - you get the picture!). Educational data is complex, and so it is not surprising that multi-level models may be encountered in educational data science analyses, reports, and articles. Fortunately, for all of the complicated details, multi-level models are very easy to use in R. Their requires a new package; one of the most common for estimating these types of models is lme4. We use it very similarly to the lm() function, but we pass it an additional argument about what the groups, in the data are. Such a model is often referred to as a “varying intercepts” multi-level model, because what is different between the groups is the effect of being a student in a class: the intercepts between groups vary. # install.packages(&quot;lme4&quot;) # note that this only needs to be run one time, after which the package will be installed on your local computer m_course &lt;- lme4::lmer(percentage_earned ~ TimeSpent + (1|course_id), data = dat) ## Warning: Some predictor variables are on very different scales: consider ## rescaling sjPlot::tab_model(m_course) percentage earned Predictors Estimates CI p (Intercept) 0.76 0.74 – 0.79 &lt;0.001 TimeSpent 0.00 -0.00 – 0.00 1.000 Random Effects σ2 0.01 τ00 course_id 0.00 ICC 0.08 N course_id 21 Observations 148 Marginal R2 / Conditional R2 0.000 / 0.076 In a way, what is going on above is straightforward (and similar to what we have seen with lm()), but, it is also different and potentially confusing. Parentheses are not commonly used with lm(); there is a term ((1|course_id)) in parentheses. Also, the bar symbol - | - is not commonly used with lm(). As different as these (the parentheses and bar) are, they are used for a relatively straightforward purpose: to model the group (in this case, courses) in the data. With lmer(), these group terms are specified in parentheses - specifically, to the right of the bar. That is what the |course_id part means - it is telling lmer that courses are groups in the data. The left side of the bar tells lmer that what we want to be specified are varying intercepts for each group (1 is used to denote the intercept). That is basically it! That is basically it, but there is potentially more to the story: in addition to the 1, variables which can be specified to have a different effect for each group can also be specified. These variables are referred to not as varying intercepts, but as varying slopes. We will not cover these in this walkthrough, but want you to be aware of them (we recommend the book by West, Welch, and Galecki [2014] provide an excellent walkthrough on how to specify varying slopes using lmer). To say just a bit more, there is a connection between multi-level models and Bayesian methods (Gelman and Hill (2006)); one way to think about the “regularizing” going on is that estimates for each group (class) are made taking account of the data across all of the groups (classes). The data for all of the classes can be interpreted as a prior for the group estimates. There is another part of the above code to mention. The sjPlot::tab_model() function comparably as it does for lm() models, providing output for the model, including some fit statistics as well as coefficients and their standard errors and estimates. There are two things to note about lmer() output: p-values are not automatically provided, due to debates in the wider field about how to calculate the degrees of freedom for coefficients (run ?lme4::pvalues to see a discussion of the issue as well as solutions; we have found the lmerTest to be helpful as an easy solution, though we note that some of the recommendations available through ?lme4::pvalues may be preferable, as the technique lmerTest implements has some known issues) There As we mentioned earlier, a common way to understand how much variability is at the group level is to calculate the intra-class correlation. This value is the proportion of the variability in the outcome (the y-variable) that is accounted for solely by the groups identified in the model. There is a useful function in the performance package for doing this. # install.packages(&quot;performance&quot;) performance::icc(m_course) ## # Intraclass Correlation Coefficient ## ## Adjusted ICC: 0.076 ## Conditional ICC: 0.076 This shows that nearly 17% of the variability in the percentage of points students earned can be explained simply by knowing what class they are in. There is much more to do with multi-level models. We briefly discuss a common extension to the model we just used, adding additional levels. The data that we are using is all from one school, and so we cannot estimate a “two-level” model. Imagine, however, that instead of 26 classes, we had data from students from 230 classes, and that these classes were from 15 schools. We could estimate a two-level, varying intercepts (where there are now two groups with effects) model very similar to the model we estimated above, but simply with another group added for the school. The model will account for the way in which the classes are nested within the schools automatically (Bates, Maechler, Bolker, &amp; Walker, 2015). # this model would specify a group effect for both the course and school m_course_school &lt;- lme4::lmer(percentage_earned ~ TimeSpent + (1|course_ID) + (1|school_id), data = dat) Were we to estimate this model (and then use the icc() function), we would see two ICC values representing the proportion of the variation in the dependent variable explained by each of the two groups we added - the course and the school. A common question those using lme4 have is whether it is necessary to explicitly nest the courses within schools; as long as the courses are unique labelled, this is not necessary to do. You can add further still levels to the model, as the {lme4} package was designed for complex multi-level models (and even those with not nested, but crossed random effects; a topic beyond the scope of this walkthrough, but which is described in West, Welch, &amp; Galecki, 2015). 13.3 Other groups In this example (and in many examples in educational research), the groups are classes. But, multi-level models can be used for other cases in which data is associated with a common group. For example, if students respond to repeated measures (such as quizzes) over time, then the multiple quiz responses for each sudent could be considered to be “grouped” within students. In such a case, instead of specifying the model with the course as the “grouping factor”, students could be. Moreover, multi-level models can include multiple groups (as noted above), even if the groups are of very different kinds (i.e., if students from multiple classes responded to multiple quizzes). There is much more that can be done with multi-level models; we have additional recommendations in the additional resources chapter. Finally, as noted earlier, multi-level models have similarities to the Bayesian methods which are becoming more common among some R users - and educational data scientists. There are also references to recommended books on Bayesian methods in the additional resources chapter. References "],
["c14.html", "14 Walkthrough 8: Predicting Students’ Final Grades Using Machine Learning Methods 14.1 Background 14.2 Information about the dataset 14.3 Selecting an analysis 14.4 Information on random forests 14.5 Analysis 14.6 Examining predictive accuracy on the test data set", " 14 Walkthrough 8: Predicting Students’ Final Grades Using Machine Learning Methods 14.1 Background One area of interest is the delivery of online instruction, which is becoming more prevalent: in 2007, over 3.9 million U.S. students were enrolled one or more online courses (Allen and Seaman 2008). With the dawn of online learning comes an abundance of new educational tools to facilitate that learning. Indeed, online learning interfaces are used to facilitate the submission of assignments and quizzes in courses in which students and instructor meet face-to-face, but these interfaces are also used in fully online courses to deliver all instruction and assessment. In a face-to-face classroom, an educator might count on behavioral cues to help them effectively deliver instruction. However, one constraint of online education is that educators do not have access as readily to the behavioral cues that can be essential to effective face-to-face instruction. For example, in a face-to-face classroom, cues such as a student missing class repeatedly or many students seeming distracted during a lecture can trigger a shift in the delivery of instruction. While technology is rapidly developing, many educators find themselves looking for ways to understand and support students online in the same way that face-to-face instructors would. Educational technology affords unique opportunities to support student success online because it provides new methods of collecting and storing data. Indeed, online learning management systems often automatically track several types of student interactions with the system and feed that data back to the course instructor. For example, an instructor might be able to quickly see how many students logged into their course on a certain day, or they might see how long students engaged with a posted video before pausing it or logging out. The collection of this data is met with mixed reactions from educators. Some are concerned that data collection in this manner is intrusive, but others see a new opportunity to support students in online contexts in new ways. As long as data are collected and utilized responsibly, data collection can support student success. One meaningful perspective from which to consider students’ engagement with online courses is related to their motivation to achieve. More specifically, it is important to consider how and why students are engaging with the course. Considering the psychological mechanisms behind achievement is valuable because doing so may help to identify meaningful points of intervention for educators and for researchers and administrators in online and face-to-face courses interested in the intersection between behavioral trace measures and students’ motivational and emotional experiences in such courses. In this walkthrough, we examine the educational experiences of students in online science courses at a virtual middle school in order to characterize their motivation to achieve and their tangible engagement with the course in terms of behavioral trace measures. To do so, we use a robust data set, which includes self-reported motivation as well as behavioral trace data collected from a learning management system (LMS) to identify predictors of final course grade. Our work examines the idea of educational success in terms of student interactions with an online science course. We explore the following four questions: Is motivation more predictive of course grades as compared to other online indicators of engagement? Which types of motivation are most predictive of achievement? Which types of trace measures are most predictive of achievement? How does a random forest compare to a simple linear model (regression)? 14.2 Information about the dataset This dataset came from 499 students enrolled in online middle school science courses in 2015-2016. The data were originally collected for use as a part of a research study, though the findings have not been published anywhere yet. The setting of this study was a public provider of individual online courses in a Midwestern state. In particular, the context was two semesters (Fall and Spring) of offerings of five online science courses (Anatomy &amp; Physiology, Forensic Science, Oceanography, Physics, and Biology), with a total of 36 classes. Specific information in the dataset included: a pre-course survey students completed about their self-reported motivation in science — in particular, their perceived competence, utility value, and interest the time students spent on the course (obtained from the learning management system (LMS), Blackboard students’ final course grades students’ involvement in discussion forums For discussion board responses, we were interested in calculating the number of posts per student and understanding the emotional tone of the discussion board posts. We used the Linguistic Inquiry and Word Count (LIWC; Pennebaker, Boyd, Jordan, &amp; Blackburn, 2015) tool to calculate the number of posts per student and to categorize the emotional tone (positive or negative) and topics of those posts. Those linguistic categorization was conducted after the data was gathered from the discussion posts, but is not replicated here to protect the privacy of the students’ posts. Instead, we present the already-categorized discussion board data, in its ready-to-use format. Thus, in the dataset used in this walkthrough, we will see pre-created variables for the mean levels of students’ cognitive processing, positive emotions, negative emotions, and social-related discourse. At the beginning of the semester, students were asked to complete the pre-course survey about their perceived competence, utility value, and interest. At the end of the semester, the time students spent on the course, their final course grades, and the contents of the discussion forums were collected. In this walkthrough, we used the R package caret to carry out the analyses. 14.3 Selecting an analysis 14.3.1 Defining our research question When you begin a new project, there are often many approaches to analyzing data and answering questions you might have about it. Some projects have a clearly defined scope and question to answer. This type of project is characterized by 1) a defined number of variables (data inputs) and 2) specific directional hypotheses. For example, if we are studying the effect of drinking coffee after dinner on ability to quickly fall asleep, we might have a very specific directional hypothesis: we expect that drinking coffee after dinner would decrease the ability to fall asleep quickly. In this case, we might collect data by having some people drink coffee and having other people drink nothing or an herbal tea before bed. We could monitor how quickly people from each group fall asleep. Since we collected data from two clearly defined groups, we can then do a statistical analysis that compares the amount of time it takes to fall asleep for each group. One option would be a test called a t-test, which we could use to see if there is a significant difference in the average amount of minutes to fall asleep for the group. This approach works very well in controlled experimental situations, especially when we can change only one thing at a time (in our coffee example, the only thing we changed was the coffee-drinking behavior of our participants - all other life conditions were held equal for both groups). Rarely are educational data projects as clear-cut and simple. For this walkthrough, we have many sources of data - survey data, learning management system data, discussion forum data, and academic achievement data as measured by final course grades. Luckily, having too much data is what we call a “good problem.” In our coffee example above, we had one really specific idea that we wanted to investigate - does coffee affect time taken to fall asleep? In this walkthrough we have many ideas we are curious to explore: the relationships among motivation, engagement in the course (discussion boards, time spent online in the course site), and academic achievement. If we wanted to tackle a simpler problem, we could choose just one of these relationships. For example, we could measure whether students with high motivation earn higher grades than students with low motivation. However, we are being a bit more ambitious than that here - we are interested in understanding the complex relationships among the different types of motivation. Rather than simply exploring whether A affects B, we are interested in the nuances: we suspect that many factors affect B, and we would like to see which of those factors has most relative importance. To explore this idea, we will use a machine learning approach. 14.3.2 Predictive analytics and machine learning A buzzword in education software spheres these days is “predictive analytics.” Administrators and educators alike are interested in applying the methods long utilized by marketers and other business professionals to try to determine what a person will want, need, or do next. “Predictive analytics” is a blanket term that can be used to describe any statistical approach that yields a prediction. We could ask a predictive model: “What is the likelihood that my cat will sit on my keyboard today?” and, given enough past information about your cat’s computer-sitting behavior, the model could give you a probability of that computer-sitting happening today. Under the hood, some predictive models are not very complex. If we have an outcome with two possibilities, a logistic regression model could be fit to the data in order to help us answer the cat-keyboard question. In this chapter, we’ll compare a machine learning model to another type of regression: multiple regression. We want to make sure to fit the simplest model as possible to our data. After all, the effectiveness in predicting the outcome is really the most important thing: not the fanciness of the model. Data collection is an essential first step in any type of machine learning or predictive analytics. It is important to note here that machine learning only works effectively when (1) a person selects variables to include in the model that are anticipated to be related to the outcome and (2) a person correctly interprets the model’s findings. There is an adage that goes, “garbage in, garbage out.” This holds true here: if we do not feel confident that the data we collected are accurate, no matter what model we build, we will not be able to be confident in our conclusions. To collect good data, we must first clarify what it is that we want to know (i.e., what question are we really asking?) and what information we would need in order to effectively answer that question. Sometimes, people approach analysis from the opposite direction - they might look at the data they have and ask what questions could be answered based on that data. That approach is okay - as long as you are willing to acknowledge that sometimes the pre-existing dataset may not contain all the information you need, and you might need to go out and find additional information to add to your dataset to truly answer your question. When people talk about “machine learning,” you might get the image in your head of a desktop computer learning how to spell. You might picture your favorite social media site showing you advertisements that are just a little too accurate. At its core, what machine learning really is is the process of “showing” your statistical model only some of the data at once, and training the model to predict accurately on that training dataset (this is the “learning” part of machine learning). Then, the model as developed on the training data is shown new data - data you had all along, but hid from your computer initially - and you see how well the model that you developed on the training data performs on this new testing data. Eventually, you might use the model on entirely new data. 14.4 Information on random forests For our analyses, we used Random Forest modeling (Breiman 2001). Random forest is an extension of decision tree modeling, whereby a collection of decision trees are simultaneously “grown” and are evaluated based on out-of-sample predictive accuracy (Breiman 2001). Random forest is random in two main ways: first, each tree is only allowed to “see” and split on a limited number of predictors instead of all the predictors in the parameter space; second, a random subsample of the data is used to grow each individual tree, such that no individual case is weighted too heavily in the final prediction. One thing about random forest that makes it quite different from other types of analysis we might do is that here, we are giving the computer a large amount of information and asking it to find connections that might not be immediately visible to the naked human eye. This is great for a couple of reasons. First, while humans are immensely creative and clever, we are not immune to biases. If we are exploring a dataset, we usually come in with some predetermined notions about what we think is true, and we might (consciously or unconsciously) seek evidence that supports the hypothesis we privately hold. By setting the computer loose on some data, we can learn that there are connections between areas that we did not expect. We must also be ready for our hypotheses to not be supported! Random forest is particularly well-suited to the research questions explored here because we do not have specific directional hypotheses. Machine learning researchers talk about this as “exploring the parameter space” - we want to see what connections exist, and we acknowledge that we might not be able to accurately predict all the possible connections. Indeed, we expect - and hope - that we will find surprising connections. Whereas some machine learning approaches (e.g., boosted trees) would utilize an iterative model-building approach, random forest estimates all the decision trees at once. In this way, each tree is independent of every other tree. Thus, the random forest algorithm provides a robust regression approach that is distinct from other modeling approaches. The final random forest model aggregates the findings across all the separate trees in the forest in order to offer a collection of “most important” variables as well as a percent variance explained for the final model. 500 trees were grown as part of our random forest. We partitioned the data before conducting the main analysis so that neither the training nor the testing data set would be disproportionately representative of high-achieving or low-achieving students. The training data set consisted of 80% of the original data (n = 400 cases), whereas the testing data set consisted of 20% of the original data (n = 99 cases). We built our random forest model on the training data set, and then evaluated the model on the testing data set. Three variables were tried at each node. Note that the random forest algorithm does not accept cases with missing data, and so we deleted cases listwise if data were missing. This decision eliminated 51 cases from our original data set, to bring us to our final sample size of 499 unique students. If you have a very small dataset with a lot of missing data, the random forest approach may not be well suited for your goals – you might consider a linear regression instead. A random forest is well suited to the research questions that we had here because it allows for nonlinear modeling. We hypothesized complex relationships between students’ motivation, their engagement with the online courses, and their achievement. For this reason, a traditional regressive or structural equation model would have been insufficient to model the parameter space we were interesting in modeling. Our random forest model had one outcome and eleven predictors. One term you will hear used in machine learning is “tuning parameter.” People often think of tuning parameters as knobs or dials on a radio: they are features of the model that can be adjusted to get the clearest signal. A common tuning parameter for machine learning models is the number of variables considered at each split (Kuhn and others 2008); we considered three variables at each split for this analysis. The outcome was the final course grade that the student earned. The predictor variables included motivation variables (interest value, utility value, and science perceived competence) and trace variables (the amount of time spent in the course, the course name, the number of discussion board posts over the course of the semester, the mean level of cognitive processing evident in discussion board posts, the positive emotions evident in discussion board posts, the negative emotions evident in discussion board posts, and the social-related discourse evident in their discussion board posts). We used this random forest model to address all three of our research questions. To interpret our findings, we will three main factors: (1) predictive accuracy of the random forest model, (2) variable importance, and (3) variance explained by the final random forest model. In this walkthrough, we will the R package caret to carry out the analyses. 14.5 Analysis First, we will load the data. Our data is stored in the dataedu package that is part of this book. Within that package, the data is stored as an .rda file. #loading the data from the .rda file and storing it as an object named &#39;data&#39; data &lt;- dataedu::sci_mo_data It’s a good practice to take a look at the data and make sure it looks the way you expect it to look. R is pretty smart, but sometimes we run into issues like column headers being read as datapoints. By using the glimpse function from the dplyr package, we can quickly skim our data and see whether we have all the right variables and datapoints. Remember that the dplyr package loads automatically when we load the tidyverse library, so there is no need to call the dplyr package separately. Now, we’ll glimpse the data. glimpse(sci_mo_data) ## Observations: 550 ## Variables: 20 ## $ pre_int &lt;dbl&gt; 5.0, 4.2, 4.6, 5.0, 4.0, 4.0, 2.4, 4.2, 4.0, 4.2, 4… ## $ pre_uv &lt;dbl&gt; 5.000000, 5.000000, 4.333333, 4.333333, 3.666667, 4… ## $ pre_percomp &lt;dbl&gt; 4.0, 4.5, 4.5, 4.0, 3.5, 4.0, NA, 2.5, 4.5, 3.0, 3.… ## $ time_spent &lt;dbl&gt; 2167.9669, 1550.4668, 850.7329, 3067.4676, 1800.217… ## $ course_ID &lt;chr&gt; &quot;AnPhA-S116-01&quot;, &quot;FrScA-S116-01&quot;, &quot;FrScA-S216-01&quot;, … ## $ final_grade &lt;dbl&gt; 93.753600, 95.890411, 88.356164, 91.337600, 88.7202… ## $ subject &lt;chr&gt; &quot;AnPhA&quot;, &quot;FrScA&quot;, &quot;FrScA&quot;, &quot;AnPhA&quot;, &quot;OcnA&quot;, &quot;BioA&quot;,… ## $ enrollment_reason &lt;chr&gt; &quot;Learning Preference of the Student&quot;, &quot;Course Unava… ## $ semester &lt;chr&gt; &quot;S116&quot;, &quot;S116&quot;, &quot;S216&quot;, &quot;S216&quot;, &quot;S116&quot;, &quot;S116&quot;, &quot;S2… ## $ enrollment_status &lt;chr&gt; &quot;Approved/Enrolled&quot;, &quot;Approved/Enrolled&quot;, &quot;Approved… ## $ cogproc &lt;dbl&gt; 10.127500, 14.702162, 15.465128, 11.697391, 13.9536… ## $ social &lt;dbl&gt; 7.550435, 5.903571, 5.729250, 6.001304, 7.981111, 1… ## $ posemo &lt;dbl&gt; 3.905882, 3.278378, 3.544242, 1.150000, 5.439643, 7… ## $ negemo &lt;dbl&gt; 1.5700000, 0.8097500, 1.0472973, 0.7665714, 0.60095… ## $ n &lt;dbl&gt; 25, 40, 42, 26, 14, 6, 19, 39, 21, 15, 30, 46, 17, … ## $ section &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01… ## $ post_int &lt;dbl&gt; NA, 2.25, NA, NA, 5.00, NA, NA, NA, NA, NA, NA, 5.0… ## $ post_uv &lt;dbl&gt; NA, 4.333333, NA, NA, 3.666667, NA, NA, NA, NA, NA,… ## $ post_percomp &lt;dbl&gt; NA, 2.0, NA, NA, 4.5, NA, NA, NA, NA, NA, NA, 4.0, … ## $ WC &lt;dbl&gt; 74.24000, 64.30769, 96.28571, 87.77551, 67.53846, 6… Scanning the data we glimpsed, we see that we have 662 observations and 111 variables. Many of these variables - everything below “WC” except the variable “n” - are related to the text content of the discussion board posts. Our analysis here is not focused on the specifics of the discusion board posts, so we will select just a few variables from the LIWC analysis. If you’re interested in learning more about analyzing text, the text analysis walkthrough in this volume will be a good place to start. As is the case with many datasets you’ll work with in education contexts, there is lots of great information in this dataset - but we won’t need all of it. Even if your dataset has many variables, for most analyses you will find that you are only interested in some of them. There are statistical reasons not to include twenty or more variables in a data analysis, and the quick explanation of the reason why is that at a certain point, adding more variables will appear to make your analysis more accurate, but will in fact obscure the truth from you. It’s generally a good practice to select a few variables you are interested in and go from there. As we discussed above, the way to do this is to start with the research questions you are trying to answer. Since we are interested in data from one specific semester, we’ll need to narrow down the data to make sure that we only include datapoints relevant to that semester. Thus, we will filter the data to include only the data from one that semester, and then select variables of interest. For each step, we will save over the previous version of the “data” object so that our working environment doesn’t get cluttered with each new version of the dataset. Keep in mind that the original data will stay intact, and that any changes we make to it within R will not overwrite that original data (unless we tell R to specifically save out a new file with exactly the same name as the original file). Changes we make within our working environment are all totally reversible. Below, we will filter to remove all the datapoints from the spring 2017 semester (indicated with a value of “S217” for the “semester” variable). We use the “!” to indicate that we want to keep all datapoints EXCEPT the datapoints that have a value of “S217” for the semester variable. Then, we will select only the variables we are interested in: motivation, time spent in the course, grade in the course, subject, enrollment information, positive and negative emotions, cognitive processing, and the number of discussion board posts. #filtering the data to only include 2016 data. data &lt;- data %&gt;% filter(semester != &quot;S217&quot;) ## filter: no rows removed #selecting only the variables we are interested in: data &lt;- data %&gt;% select( pre_int, pre_uv, pre_percomp, time_spent, final_grade, subject, enrollment_reason, semester, enrollment_status, cogproc, social, posemo, negemo, n ) ## select: dropped 6 variables (course_ID, section, post_int, post_uv, post_percomp, …) 14.5.1 Use of caret Here, we remove observations with missing data (per our note above about random forests requiring complete cases). #checking how many rows are in our dataset nrow(data) ## [1] 550 #we see that we have 550 rows from spring 2017 #calling the na.omit function to eliminate ANY rows that have ANY missing data data &lt;- na.omit(data) #checking whether our na.omit call worked as expected nrow(data) ## [1] 494 #after running the code above, we see that we now have 499 rows - this is as we expected First, machine learning methods often involve using a large number of variables. Oftentimes, some of these variables will not be suitable to use: they may be highly correlated with other variables, for instance, or may have very little - or no - variability. Indeed, for the data set used in this study, one variable has the same (character string) value for all of the observations. We can detect this variable and any others using the following function: #we run the nearZeroVar function to determine if there are variables with NO variability nearZeroVar(data, saveMetrics = TRUE) ## freqRatio percentUnique zeroVar nzv ## pre_int 1.500000 2.8340081 FALSE FALSE ## pre_uv 1.474359 2.4291498 FALSE FALSE ## pre_percomp 1.225225 1.4170040 FALSE FALSE ## time_spent 1.250000 65.5870445 FALSE FALSE ## final_grade 1.250000 63.1578947 FALSE FALSE ## subject 1.360544 1.0121457 FALSE FALSE ## enrollment_reason 3.229885 1.0121457 FALSE FALSE ## semester 1.344660 0.6072874 FALSE FALSE ## enrollment_status 0.000000 0.2024291 TRUE TRUE ## cogproc 1.000000 61.1336032 FALSE FALSE ## social 1.000000 62.1457490 FALSE FALSE ## posemo 2.200000 63.7651822 FALSE FALSE ## negemo 4.200000 61.3360324 FALSE FALSE ## n 1.240000 9.7165992 FALSE FALSE After conducting our zero variance check, we want to scan the “zeroVar” column to see if any of our variables failed this check. If we see any “TRUE” values for “zeroVar,” that means we should look more closely at that variable. In the nearZeroVar function we just ran, we see a result in the ZeroVar column of “TRUE” for the enrollment_status variable. If we look at enrollment_status, we will see that it is “Approved/Enrolled” for all of the students. When we use variables with no variability in certian models, it may cause some problems, and so we remove it first. #Taking the dataset and re-saving it as the same dataset, but without the enrollment status variable data &lt;- data %&gt;% select(-enrollment_status) ## select: dropped one variable (enrollment_status) Note that many times you may wish to pre-process the variables, such as by centering or scaling them. Often the data will come to you in a format that is not ready for immediate analysis, as we have discussed elsewhere in the book. For our current dataset, we could work on pre-processing with code like you will see below. We set this next code chunk up to not run here (if you are viewing the book online), as we will do this analysis with the variables’ original values. #example pre-processing step: manipulating the dataset &#39;data&#39; so that if a variable is numeric, its format will now be scale data &lt;- data %&gt;% mutate_if(is.numeric, scale) As another pre-processing step, we want to make sure our text data is in a format that we can easily evaluate. To facilitate that, we want to make character string variables into factors. #converting the text (character) variables in our dataset into factors data &lt;- data %&gt;% mutate_if(is.character, as.factor) ## mutate_if: converted &#39;subject&#39; from character to factor (0 new NA) ## converted &#39;enrollment_reason&#39; from character to factor (0 new NA) ## converted &#39;semester&#39; from character to factor (0 new NA) Now, we will prepare the train and test datasets, using the caret function for creating data partitions. Here, the p argument specifies what proportion of the data we want to be in the training partition. Note that this function splits the data based upon the outcome, so that the training and test data sets will both have comparable values for the outcome. This means that since our outcome is final grade, we are making sure that we don’t have either a training or testing dataset that has too many good grades - or too many bad grades. Note the times = 1 argument; this function can be used to create multiple train and test sets, something we will describe in more detail later. Before we create our training and testing datasets, we want to initiate a process called “setting the seed.” This means that we are ensuring that if we run this same code again, we will get the same results in terms of the data partition. The seed can be any number that you like - some people choose their birthday or another meaningful number. The only constraint is that wehn you open the same code file again to run in the future, you do not change the number you selected for your seed. This ensures your code is reproducible. In fact, it ensures that anyone who runs the same code file on any computer, anywhere, will get the same result. With that background information, try running the code chunk below. #First, we set a seed to ensure the reproducibility of our data partition. set.seed(62020) #we create a new object called trainIndex that will take 80 percent of the data trainIndex &lt;- createDataPartition(data$final_grade, p = .8, list = FALSE, times = 1) #We add a new variable to our dataset, temporarily: #this will let us select our rows according to their row number #we populate the rows with the numbers 1:499, in order data &lt;- data %&gt;% mutate(temp_id = 1:494) ## mutate: new variable &#39;temp_id&#39; with 494 unique values and 0% NA #we filter our dataset so that we get only the #rows indicated by our &quot;trainIndex&quot; vector data_train &lt;- data %&gt;% filter(temp_id %in% trainIndex) ## filter: removed 96 rows (19%), 398 rows remaining #we filter our dataset in a different way so that we get only the rows NOT in our &quot;trainIndex&quot; vector #adding the ! before the temp_id variable achieves the opposite of what we did in the line of code above data_test &lt;- data %&gt;% filter(!temp_id %in% trainIndex) ## filter: removed 398 rows (81%), 96 rows remaining #We delete the temp_id variable from (1) the original data, (2) the portion of the original data we marked as training, and (3) the portion of the original data we marked as testing, as we no longer need that variable data &lt;- data %&gt;% select(-temp_id) ## select: dropped one variable (temp_id) data_train &lt;- data_train %&gt;% select(-temp_id) ## select: dropped one variable (temp_id) data_test &lt;- data_test %&gt;% select(-temp_id) ## select: dropped one variable (temp_id) Finally, we will estimate the models. Here, we will use the train function, passing all of the variables in the data frame (except for the outcome, or dependent variable, final_grade) as predictors. The predictor variables include three indicators of motivation: interest in the course (pre_int), perceived utility value of the course (pre_uv), and perceived competence for the subject matter (pre_percomp). There are a few predictor variables that help differentiate between the different courses in the dataset: subject matter of the course (subject), reason the student enrolled in the course (enrollment_reason), and semester in which the course took place (semester). We have a predictor variable that indicates the amount of time each student spent engaging with the online learning platform of the course (time_spent). We also have a number of variables associated with the discussion board posts from the course. Specifically, the variables include the average level of cognitive processing in the discussion board posts (cogproc), the average level of social (rather than academic) content in the discussion board posts (social), the positive and negative emotions evident in the discussion board posts (posemo and negemo), and finally, the number of discussion board posts in total (n). We are using all those variables discussed in this paragraph to predict the outcome of the final grade in the course (final_grade). Note that you can read more about the specific random forest implementation chosen here. To specify that we want to predict the outcome using every variable except the outcome itself, we use the formulation (outcome ~ .,). R interprets this code as: predict the outcome using all the variables except outcome itself. The outcome always comes before the ~, and the . that we see after the ~ means that we want to use all the rest of the variables. An alternative specification of this model would be to write (outcome ~ predictor1, predictor2). Anything that follows the ~ and precedes the comma is treated as predictors of the outcome. Here, we set the seed again, to ensure that our analysis is reproducible. This step of setting the seed is especially important due to the “random” elements of random forest, because it’s likely that the findings would change (just slightly) if the seed were not set. As we get into random forest modeling, you might notice that the code takes a bit longer to run. This is normal - just think of the number of decision trees that are “growing!” #setting a seed for reproducibility set.seed(62020) #we run the model here rf_fit &lt;- train(final_grade ~ ., data = data_train, method = &quot;ranger&quot;) #here, we get a summary of the model we just built rf_fit ## Random Forest ## ## 398 samples ## 12 predictor ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 398, 398, 398, 398, 398, 398, ... ## Resampling results across tuning parameters: ## ## mtry splitrule RMSE Rsquared MAE ## 2 variance 15.40824 0.6062405 11.338374 ## 2 extratrees 17.51403 0.5443134 12.555711 ## 10 variance 13.46816 0.6535352 9.806955 ## 10 extratrees 13.58057 0.6767111 9.952872 ## 19 variance 13.55542 0.6443721 9.615483 ## 19 extratrees 13.07647 0.6814068 9.541441 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 19, splitrule = extratrees ## and min.node.size = 5. We have some results! First, we see that we have 400 samples, or 400 observations, the number in the train data set. No pre-processing steps were specified in the model fitting, but note that the output of preProcess can be passed to train() to center, scale, and transform the data in many other ways. Next, in our example, a resampling technique has been used. This resampling is not for validating the model (per se), but is rather for selecting the tuning parameters - the options that need to be specified as a part of the modeling. These parameters can be manually provided, or can be estimated via strategies such as the bootstrap resample (or k-folds cross validation). As we interpret these findings, we are looking to minimize the error (RMSE) and maximize the variance explained (rsquared). It appears that the model with the value of the mtry tuning parameter equal to 19 seemed to explain the data best, the splitrule being “extratrees”, and min.node.size held constant at a value of 5. We know this model fits best because the RMSE is the lowest of the options (13.13) and the Rsquared is the highest of the options (.582). The value of resampling here is that it allows for higher accuracy of the model (James et al. 2013). Without resampling (bootstrapping or cross-validation), the variance would be higher and the predictive accuracy of the model would be lower. Let’s see if we end up with slightly different values if we change the resampling technique to cross-validation, instead of bootstrap resampling. We set a seed again here, for reproducibility. set.seed(62020) train_control &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10) rf_fit1 &lt;- train(final_grade ~ ., data = data_train, method = &quot;ranger&quot;, trControl = train_control) rf_fit1 ## Random Forest ## ## 398 samples ## 12 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 358, 359, 358, 358, 358, 358, ... ## Resampling results across tuning parameters: ## ## mtry splitrule RMSE Rsquared MAE ## 2 variance 14.84149 0.6245157 10.914358 ## 2 extratrees 16.87348 0.5851049 12.238703 ## 10 variance 12.59610 0.6771022 9.180824 ## 10 extratrees 12.84618 0.6956843 9.452986 ## 19 variance 12.52421 0.6749867 8.947893 ## 19 extratrees 12.41260 0.6945521 9.067094 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 19, splitrule = extratrees ## and min.node.size = 5. When we look at this output, we are looking to see which values of the various tuning parameters were selected. We see at the bottom of the output above that the value of mtry was 19, the split rule was “extratrees,” and the minimum node size is 5. We let this model explore which value of mtry was best and to explore whether extra trees or variance was a better split rule, but we forced all iterations of the model to a minimum node size of five (so that minimum node size value in the output shouldn’t be a surprise to us). When we look at the bottom row of the output, it shows the final values selected for the model. We see also that this row has the lowest RMSE and highest Rsquared value, which means it has the lowest error and highest predictive power. We won’t dive into the specifics of the statistics behind these decisions right now, but next we will try adjusting a few different parts of the model to see whether our performance improves. For a detailed statistical explanation of random forest modeling, including more about mtry and tuning a model, please see Chapter 8 in the book “An Introduction to Statistical Learning with Applications in R” (James et al. 2013). What would happen if we do not fix min.node.size to five? We’re going to let min.node.size change and let mtry change as well. Let’s create our own grid of values to test for mtry and min.node.size. We’ll stick with the default bootstrap resampling method to choose the best model. We will randomly choose some values to use for mtry, including the three that were used previously (2, 10, and 19). Let’s try 2, 3, 7, 10, and 19. #create a grid of different values of mtry, different splitrules, and different minimum node sizes to test tune_grid &lt;- expand.grid( mtry = c(2, 3, 7, 10, 19), splitrule = c(&quot;variance&quot;, &quot;extratrees&quot;), min.node.size = c(1, 5, 10, 15, 20) ) #set a seed set.seed(62020) #fit a new model, using the tuning grid we created above rf_fit2 &lt;- train(final_grade ~ ., data = data_train, method = &quot;ranger&quot;, tuneGrid = tune_grid) rf_fit2 ## Random Forest ## ## 398 samples ## 12 predictor ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 398, 398, 398, 398, 398, 398, ... ## Resampling results across tuning parameters: ## ## mtry splitrule min.node.size RMSE Rsquared MAE ## 2 variance 1 15.32302 0.6110390 11.256089 ## 2 variance 5 15.41644 0.6033990 11.358638 ## 2 variance 10 15.58342 0.5963452 11.524727 ## 2 variance 15 15.76218 0.5868123 11.680710 ## 2 variance 20 15.89639 0.5806568 11.794798 ## 2 extratrees 1 17.35985 0.5535581 12.406218 ## 2 extratrees 5 17.50843 0.5441410 12.558312 ## 2 extratrees 10 17.68886 0.5307434 12.730148 ## 2 extratrees 15 17.94445 0.5172112 12.937712 ## 2 extratrees 20 18.09510 0.5111653 13.051717 ## 3 variance 1 14.52728 0.6280811 10.653547 ## 3 variance 5 14.57908 0.6260363 10.738165 ## 3 variance 10 14.71290 0.6190406 10.886307 ## 3 variance 15 14.87838 0.6115016 11.045861 ## 3 variance 20 15.10341 0.5993341 11.247855 ## 3 extratrees 1 15.93170 0.6051503 11.382563 ## 3 extratrees 5 16.11388 0.5974488 11.558824 ## 3 extratrees 10 16.40912 0.5831345 11.826247 ## 3 extratrees 15 16.68300 0.5716373 12.062076 ## 3 extratrees 20 16.85874 0.5650996 12.213533 ## 7 variance 1 13.61613 0.6524226 9.985055 ## 7 variance 5 13.70025 0.6472675 10.044141 ## 7 variance 10 13.78366 0.6435253 10.142727 ## 7 variance 15 13.92684 0.6361402 10.290956 ## 7 variance 20 14.05235 0.6292943 10.401780 ## 7 extratrees 1 13.98467 0.6691778 10.169688 ## 7 extratrees 5 14.14466 0.6619404 10.311411 ## 7 extratrees 10 14.35877 0.6554805 10.526819 ## 7 extratrees 15 14.58194 0.6469771 10.718176 ## 7 extratrees 20 14.78158 0.6406403 10.884044 ## 10 variance 1 13.44769 0.6554058 9.767580 ## 10 variance 5 13.46283 0.6546588 9.811348 ## 10 variance 10 13.55409 0.6498397 9.900068 ## 10 variance 15 13.69744 0.6428284 10.012504 ## 10 variance 20 13.80312 0.6368826 10.132796 ## 10 extratrees 1 13.51164 0.6793488 9.891000 ## 10 extratrees 5 13.58482 0.6759328 9.948678 ## 10 extratrees 10 13.72827 0.6714187 10.097534 ## 10 extratrees 15 13.93028 0.6641939 10.283625 ## 10 extratrees 20 14.10505 0.6573098 10.438428 ## 19 variance 1 13.49065 0.6473468 9.563349 ## 19 variance 5 13.54851 0.6444930 9.608090 ## 19 variance 10 13.62576 0.6401074 9.679835 ## 19 variance 15 13.73846 0.6343041 9.773467 ## 19 variance 20 13.81754 0.6297773 9.847623 ## 19 extratrees 1 13.02666 0.6843333 9.489768 ## 19 extratrees 5 13.07677 0.6819550 9.531943 ## 19 extratrees 10 13.20664 0.6756461 9.658847 ## 19 extratrees 15 13.28154 0.6734305 9.751242 ## 19 extratrees 20 13.37376 0.6700883 9.857556 ## ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 19, splitrule = extratrees ## and min.node.size = 1. The model with the same values as identified before for mtry (19) and splitrule (extratrees), but with min.node.size equal to 1 (not 5, as before) seems to fit best. We know this model fits best because the RMSE is lowest (13.08) and the variance explained is highest (.58) for this model, though the improvement seems to be fairly small relative to the difference the other tuning parameters seem to make. While the output above gives us a good summary of the model, we might want to look more closely at what we found with our rf_fit2 model. The code below is a way for us to zoom in and look specifically at the final random forest model generated by our rf_fit2. In the code chunk below, you’ll notice we are selecting the “finalModel” output using a $ operator rather than the familiar select. We cannot use dplyr and the tidyverse here because of the structure of the rf_fit2 object - we have stored a random forest model as a model, so it’s not a normal dataframe. Thus, we extract with a $. We want to select only the final model used, and not worry about the prior iterations of the model. #Here, we select the &quot;finalModel&quot; output from the rf_fit2 model rf_fit2$finalModel ## Ranger result ## ## Call: ## ranger::ranger(dependent.variable.name = &quot;.outcome&quot;, data = x, mtry = min(param$mtry, ncol(x)), min.node.size = param$min.node.size, splitrule = as.character(param$splitrule), write.forest = TRUE, probability = classProbs, ...) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 398 ## Number of independent variables: 19 ## Mtry: 19 ## Target node size: 1 ## Variable importance mode: none ## Splitrule: extratrees ## OOB prediction error (MSE): 154.3324 ## R squared (OOB): 0.6921864 In looking at this output, we see the same parameters we noted above: mtry is 19, the node size is 1, and the split rule is extra trees. We can also note the OOB prediction error (MSE), of 168.92, and the proportion of the variance explained, or R squared, of 0.59. As before, we want the error to be low and the variance explained to be high. Now that we understand how to develop a basic machine learning model, and how to use different tuning parameters (such as node size and the splitting rule), we can explore some other related themes. We might wonder about how we could examine the predictive accuracy of the random forest model we just developed. 14.6 Examining predictive accuracy on the test data set What if we use the test data set - data not used to train the model? Below, we’ll create a new object that uses the rf_fit2 model we developed above. We will put our testing data through the model, and assign the predicted values to a row called “pred.” At the same, time, we’ll make a row called “obs” that includes the real final grades that students earned. Later, we’ll compare these predicted and observed values to see how well our model did. set.seed(62020) ##Create a new object for the testing data including predicted values data_test_augmented &lt;- data_test %&gt;% mutate(pred = predict(rf_fit2, data_test), obs = final_grade) ## mutate: new variable &#39;pred&#39; with 96 unique values and 0% NA ## new variable &#39;obs&#39; with 86 unique values and 0% NA ###Transform this new object into a dataframe defaultSummary(as.data.frame(data_test_augmented)) ## RMSE Rsquared MAE ## 11.2145423 0.7433124 8.1097816 We can compare this to the values above to see how our model performs when given data that was not used to train the model. Comparing the RMSE values, we see that the RMSE is about the same when we use the model on the test data as it was on the training data. We get a value of 12.36 on the test data here, and it was 13.08 on the training data. The Rsquared value is 0.71 here, as compared to the 0.58 we got when we passed the training data through rf_fit2 earlier. While we might have expected that the model performance would be worse for the testing data as compared to the training data, we actually are seeing marginal improvements here: the model does better with the test data than with the training data. These results suggest to us that the model is fairly robust, as we get comparable - in fact, improved - results when running the model on data it has never “seen” before (the testing data). This is good news! 14.6.1 Variable importance measures One helpful characteristic of random forest models is that we can learn about which variables contributed most strongly to the predictions in our model, across all the trees in our forest. We can examine two different variable importance measures using the ranger method in caret. Note that importance values are not calculated automatically, but that “impurity” or “permutation” can be passed to the importance argument in train(). See more here. We’ll re-run the rf_fit2 model with the same specifications as before, but this time we will add an argument to call the variable importance metric. #set a seed set.seed(62020) #specify the same model as earlier in the chapter (rf_fit2) with the addition of the variable importance metric rf_fit2_imp &lt;- train( final_grade ~ ., data = data_train, method = &quot;ranger&quot;, tuneGrid = tune_grid, importance = &quot;permutation&quot; ) #extract the variable importance from this new model varImp(rf_fit2_imp) ## ranger variable importance ## ## Overall ## n 100.0000 ## subjectFrScA 18.5374 ## time_spent 13.5623 ## cogproc 5.3760 ## social 2.4427 ## negemo 2.3617 ## pre_uv 2.3189 ## semesterS216 2.1824 ## pre_int 2.1678 ## posemo 2.0695 ## subjectPhysA 1.6834 ## subjectOcnA 1.1885 ## pre_percomp 0.7249 ## enrollment_reasonOther 0.6607 ## enrollment_reasonScheduling Conflict 0.4559 ## enrollment_reasonCredit Recovery 0.4224 ## semesterT116 0.4199 ## subjectBioA 0.3773 ## enrollment_reasonLearning Preference of the Student 0.0000 Our results here give us a ranked order list of the variables in the order of their importance. Variables that appear at the top of the list are more important, and variables that appear at the bottom of the list are less important in the specification of our final random forest model. Remember that we are predicting final grade in the course, so this list will tell us which factors were most important in predicting final grade in online science courses. It can be a bit hard to visually scan a variable importance list, so we might be interested in doing a data visualization. We can visualize this variable importance list with ggplot. varImp(rf_fit2_imp) %&gt;% pluck(1) %&gt;% rownames_to_column(&quot;var&quot;) %&gt;% ggplot(aes(x = reorder(var, Overall), y = Overall)) + geom_col() + coord_flip() + theme_dataedu() Cool! We can now visualize which variables are most important in predicting final grade. The first thing we notice is that the variable “n” is the most important. This variable indicates how much students write in their discussion posts. The second most important variable is the amount of time students spend in their course. The third most important variable is subjectFrScA. This is one of the course subjects: forensic science. Being enrolled in the forensic science course has a large impact on final grade. That would indicate to us that the forensic science course - more than the other science subjects in this dataset - is strongly correlated with students’ final course grades. We can keep scanning down the list to see the other variables that were indicated as less and less important for the model’s predictions. Variable importance can thus help us to better understand the inner workings of a random forest model. Overall, there are some subject level differences in terms of how predictive subject is. Biology (subjectBioA) shows up pretty far down the list, whereas Physiology is in the middle (subjPhysA) and forensic science is towards the top (subjectFrScA). What this tells us is that the course students are in seems to have a different effect on final grade, depending on the course. Perhaps grades should be normalized within subject: would this still be an important predictor if we did that? We won’t dive into that question here, but you can see how the line of research inquiry might progress as you start to explore your data with a machine learning model. As a quick statistical note: above, we selected our variable importance method to be “permutation” for our demonstrative example. There are other options available in the caret package if you would like to explore those in your analyses. 14.6.2 Comparing a random forest to a regression You may be curious about comparing the predictive accuracy of the model to a linear model (a regression). Below, we’ll specify a linear model and check out how the linear model performs in terms of predicting the real outcomes. We’ll compare this with the random forest model’s performance (rf_fit2). Note that we are not actually re-running our random forest model here, but instead we are just making a dataset that includes the values that the rf_fit2 model predicted as well as the actual rf_fit2 values. #Make sure all variables stored as characters are converted to factors data_train_lm &lt;- data_train %&gt;% mutate_if(is.character, as.factor) ## mutate_if: no changes #Create a linear regression model, using the same formula approach as in the random forest: ~ . lm_fit &lt;- train(final_grade ~ ., data = data_train_lm, method = &quot;lm&quot;) ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading #Append the predicted values to the training dataset for the linear model, so we can see both the predicted and the actual values data_train_lm &lt;- data_train %&gt;% mutate(obs = final_grade, pred = predict(lm_fit, data_train_lm)) ## mutate: new variable &#39;obs&#39; with 273 unique values and 0% NA ## new variable &#39;pred&#39; with 398 unique values and 0% NA #Append the predicted values to the training dataset for the random forest data_train_randomfor &lt;- data_train %&gt;% mutate(pred = predict(rf_fit2, data_train), obs = final_grade) ## mutate: new variable &#39;pred&#39; with 398 unique values and 0% NA ## new variable &#39;obs&#39; with 273 unique values and 0% NA #Summarize, as data frames, the training data with the predicted and the actual values #for both the linear model defaultSummary(as.data.frame(data_train_lm)) ## RMSE Rsquared MAE ## 14.3559662 0.5879137 10.8437098 #and the random forest defaultSummary(as.data.frame(data_train_randomfor)) ## RMSE Rsquared MAE ## 4.6050924 0.9699852 3.3104543 Our output will come in the order we wrote the code, so the linear model output shows up above the random forest output. We can see that the random forest technique seems to perform better than regression. Specifically, the RMSE is lower for the random forest (4.80 as compared to 13.50 for the linear model). Second, the variance explained (Rsquared) is much higher in the random forest (0.96 as compared to 0.59 for the linear model). It may be interesting to compare the results from the random forest not to a more straightforward model, such as a regression, but to a more sophisticated model, like one for deep learning. As you expand your skills, you might be curious to do something like that! References "],
["c15.html", "15 Introducing Data Science Tools To Your Education Job 15.1 Introduction 15.2 The Gift of Speed and Scale 15.3 Other Ways to Reimagine the Scale of Your Work 15.4 Solving Problems Together 15.5 For K-12 Teachers", " 15 Introducing Data Science Tools To Your Education Job 15.1 Introduction The purpose of this section is to explore the reality of what it is like to take new found data science skills into your work place with the challenge of finding practical ways to use your skills, encouraging your coworkers to be better users of data, and develop analytic routines that are individualized to the needs of your organization. Whether you are helping an education institution as a consultant, an administrator leading teachers at a school, or a university department chair, there are things you can do to transform what you’ve learned in the abstract into more concrete learning objectives in the context of your education work place. We’ll discuss this topic using two areas of focus: bringing your organization the gift of speed and scale, and the importance of connecting well with others. We’ll close this chapter by discussing some of the ways that K-12 teachers in particular might engage a work culture that is bringing on data science as a problem-solving tool. 15.2 The Gift of Speed and Scale The power of doing data a analysis with a programming language like R comes from two improvements over tools like Excel and Google Sheets. These improvements are 1. a massive boost in the speed of your work and 2. a massive boost in the size of the size of the datasets you analyze. Here are some approaches to introducing data science to your education workplace that focus on making the most of these increases in speed and scale. 15.2.1 Working With Data Faster Data analysts who have have an efficient analytic process understand their clients’ questions and participate by rapidly cycling through analysis and discussion. They quickly accumulate skill and experience because their routines facilitate many cycles of data analysis. Roger Peng and Elizabeth Matsui discuss epicycles of analysis in their book The Art of Data Science. In their book R for Data Science, Garrett Grolemund and Hadley Wickham demonstrate a routine for data exploration. When the problem space is not clearly defined, as is often the case with education data analysis questions, the path to get from the initial question to analysis itself is full of detours and distractions. Having a routine that points you to the next immediate analytic step gets the analyst started quickly, and many quick starts results in a lot of data analyzed. But speed gives us more than just an accelerated flow of experience or the thrill of rapidly getting to the bottom of a teacher’s data inquiry. It fuels the creativity required to understand problems in education and the imaginative solutions required to address them. Analyzing data quickly keeps the analytic momentum going at the speed needed to indulge organic exploration of the problem. Imagine an education consultant working with a school district to help them measure the effect of a new intervention on how well their students are learning math. During this process the superintendent presents the idea of comparing quiz scores at the schools in the district. The speed at which the consultant offers answers is important for the purposes of keeping the analytic conversation going. When a consultant quickly answers a teacher’s analytic question about their students’ latest batch of quiz scores, the collaborative analytic process feels more like a fast-paced inspiring conversation with a teammate instead of sluggish correspondence between two people on opposite ends of the country. We’ve all experienced situations where a question like “Is this batch of quiz scores meaningfully different from the ones my students had six months ago?” took so long to answer that the question itself is unimportant by the time the answer arrives! Users of data science techniques in education have wonderful opportunities to contribute in situations like this because speedy answers can be the very thing that sparks more important analytic questions. In our example of the education consultant presented with a superintendent’s curiosity about quiz score results, it is not too hard to imagine many other great questions resulting from the initial answers: How big was the effect of the new intervention, if any? Do we see similar effects across student subgroups, especially the subgroups we are trying to help the most? Do we see similar effects across grade levels? The trick here is to use statistics, programming, and knowledge about education to raise and answer the right questions quickly so the process feels like a conversation. When there’s too much time between analytic questions and their answers, educators lose the momentum required to follow the logical and exploratory path towards understanding the needs of their students. 15.2.1.1 Example: Preparing quiz data to compute average scores # TODO: Add an intervention column to make this example feel more connected to the anecdote Let’s take our example of the education consultant tasked with computing the average quiz scores. Imagine the school district uses an online quiz system and each teacher’s quiz export looks like this: library(tidyverse) ## ── Attaching packages ────────────── ## ✓ ggplot2 3.2.1 ✓ purrr 0.3.3 ## ✓ tibble 2.1.3 ✓ dplyr 0.8.3 ## ✓ tidyr 1.0.0 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.4.0 ## ── Conflicts ─────────────────────── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() set.seed(45) quizzes_1 &lt;- tibble( teacher_id = 1, student_id = c(1:3), quiz_1 = sample(c(0:100), 3, replace = TRUE), quiz_2 = sample(c(0:100), 3, replace = TRUE), quiz_3 = sample(c(0:100), 3, replace = TRUE) ) quizzes_1 ## # A tibble: 3 x 5 ## teacher_id student_id quiz_1 quiz_2 quiz_3 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 36 95 82 ## 2 1 2 74 38 10 ## 3 1 3 45 57 63 Tools like Excel and Google Sheets can help you compute statistics like mean scores for each quiz or mean scores for each student fairly quickly, but what if you’d like to do that for five teachers using the exact same method? First, let’s tidy the data. This will prepare our data nicely to compute any number of summary statistics or plot results. Using gather to separate the quiz number and its score for each student will get us a long way: quizzes_1 %&gt;% gather(quiz_number, score, -c(teacher_id, student_id)) ## # A tibble: 9 x 4 ## teacher_id student_id quiz_number score ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 1 quiz_1 36 ## 2 1 2 quiz_1 74 ## 3 1 3 quiz_1 45 ## 4 1 1 quiz_2 95 ## 5 1 2 quiz_2 38 ## 6 1 3 quiz_2 57 ## 7 1 1 quiz_3 82 ## 8 1 2 quiz_3 10 ## 9 1 3 quiz_3 63 Note now that in the first version of this dataset, each individual row represented a unique combination of teacher and student. After using gather, each row is now a unique combination of teacher, student, and quiz number. This is often talked about as changing a dataset from “wide” to “narrow” because of the change in the width of the dataset. The benefit to this change is that we can compute summary statistics by grouping values in any of the new columns. For example, here is how we would compute the mean quiz score for each student: quizzes_1 %&gt;% gather(quiz_number, score, -c(teacher_id, student_id)) %&gt;% group_by(student_id) %&gt;% summarise(quiz_mean = mean(score)) ## # A tibble: 3 x 2 ## student_id quiz_mean ## &lt;int&gt; &lt;dbl&gt; ## 1 1 71 ## 2 2 40.7 ## 3 3 55 Again, for one dataset this computation is fairly straight forward and can be done with a number of software tools. But what if the education consultant in our example wants to do this repeatedly for twenty five teacher quiz exports? Let’s look at one way we can do this fairly quickly using R. We’ll start by creating two additional datasets as an example. To make things feel authentic, we’ll also add a column to show if the students participated in a new intervention. # Add intervention column to first dataset quizzes_1 &lt;- quizzes_1 %&gt;% mutate(intervention = sample(c(0, 1), 3, replace = TRUE)) # Second imaginary dataset quizzes_2 &lt;- tibble( teacher_id = 2, student_id = c(4:6), quiz_1 = sample(c(0:100), 3, replace = TRUE), quiz_2 = sample(c(0:100), 3, replace = TRUE), quiz_3 = sample(c(0:100), 3, replace = TRUE), intervention = sample(c(0, 1), 3, replace = TRUE) ) # Third imaginary dataset quizzes_3 &lt;- tibble( teacher_id = 3, student_id = c(7:9), quiz_1 = sample(c(0:100), 3, replace = TRUE), quiz_2 = sample(c(0:100), 3, replace = TRUE), quiz_3 = sample(c(0:100), 3, replace = TRUE), intervention = sample(c(0, 1), 3, replace = TRUE) ) The method we’ll use to compute the mean quiz score for each student is to: Combine all the datasets into one big dataset: Use bind_rows to combine all three quiz exports into one dataset. Remember, this can be done because each teacher’s export uses the same imaginary online quiz system and export feature and thus use the same number of columns and variable names Reuse the code from the first dataset on the new bigger dataset: Paste the code we used in the first example into the script so it cleans and computes the mean on the combined dataset Compute the mean of each student: Now that the data is arranged so that each row is a unique combination of teacher, student, quiz number, and intervention status, we can compute the mean quiz score for each student. # Use `bind_rows` to combine the three quiz exports into one big dataset all_quizzes &lt;- bind_rows(quizzes_1, quizzes_2, quizzes_3) Note there are now nine rows, one for each student in our dataset of three teacher quiz exports: all_quizzes ## # A tibble: 9 x 6 ## teacher_id student_id quiz_1 quiz_2 quiz_3 intervention ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 36 95 82 0 ## 2 1 2 74 38 10 1 ## 3 1 3 45 57 63 0 ## 4 2 4 92 27 15 0 ## 5 2 5 37 80 99 1 ## 6 2 6 67 52 99 1 ## 7 3 7 60 78 13 0 ## 8 3 8 29 1 89 0 ## 9 3 9 93 52 25 1 We’ll combine the cleaning and computation of the mean steps neatly into one this chunk of code: # Reuse the code from the first dataset on the new bigger dataset all_quizzes %&gt;% gather(quiz_number, score, -c(teacher_id, student_id, intervention)) %&gt;% # Compute the mean of each student group_by(student_id, intervention ) %&gt;% summarise(quiz_mean = mean(score)) ## # A tibble: 9 x 3 ## # Groups: student_id [9] ## student_id intervention quiz_mean ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 71 ## 2 2 1 40.7 ## 3 3 0 55 ## 4 4 0 44.7 ## 5 5 1 72 ## 6 6 1 72.7 ## 7 7 0 50.3 ## 8 8 0 39.7 ## 9 9 1 56.7 Note here that our imaginary education consultant from the example is thinking ahead by including the intervention column. By doing so she’s opened the possibility of collaboratively exploring any possible differences in the scores between the students who had the intervention and the students who did not when she reviews and discusses these results with the school staff. Adding these types of details ahead of time is one way to build conversation starters into your collaborations. It is also a way to get faster at responding to curiosities by anticipating useful questions from your clients. The difference in time it takes to do this on three quiz exports using R versus non-programming tools is perhaps not significant. But the speed of computing means across larger volumes of data–say thirty quiz exports–is truly useful to an education consultant looking to help many educators. Summary While getting fast at answering analytic questions is not a silver bullet (but really, what is?), it does have a chain effect often leads to creative solutions. It works something like this: Answering analytic questions faster helps more people Helping more people creates opportunities for more data science practice Helping more people also helps educate those same people about the solutions data science tools can offer Lots of practice combined with a common understanding of the value of data science tools in the education workplace nurtures confidence Confidence leads to the courage required to experiment with interesting solutions for designing the best solutions for students Here are more ways to get faster at answering analytic questions: Recognize when you are using similar chunks of code to do repetitive operations. Store that code in an accessible place and reuse it Keep a notebook of the questions teachers and administrators ask to help you develop an instinct for common patterns of questions. Write your code to anticipate these questions Learn to use functions and packages like purrr to work on many datasets at once Install a prototyping habit by getting comfortable with quickly producing rough first drafts of your analysis. Your audience can give valuable feedback early and feel like you are quickly on the path to developing useful answers to their questions 15.2.2 Working With More Data Improving outcomes in education is about learning, obviously for the students, but just as importantly for the people teaching the students. The more data is available to examine, the more school staff learn about what is working for their students. Using R to prepare and analyze data so it is repeatable and easy to share increases the amount of data you can work with on an order of magnitude compared to tools like Google Sheets. When cleaning and analyzing data is laborious, people tend to generate less data. This can be a problem because less data means less context for the data you do have. Without context, it is difficult to conduct one of the primary cognitive tasks of data analysis: making comparisons. For example, imagine a teacher whose students have an average quiz score of 75 percent. This information is helpful to the teacher because it shows her how close she is to some pre-determined average quiz score goal, say 95 percent. But that data alone doesn’t tell the teacher how unusual that class average is. For that, you need context. Say that line of code used to compute this teacher’s class average quiz score was applied to every classroom and she learned that the school average for the same quiz was 77 percent. From this information the teacher learns that her class average is not very different from everyone else’s. This is more information than just the knowledge that her class’s average was less than her pre-determined goal of 95 percent. This is where using R for data analysis enters the conversation. Working with data past a certain size, say 10,000 rows, is difficult because you have to interact with each row through the graphical user interface. Instead, you can work with larger datasets like using programming languages like R to issue complex instructions for acting on the data rather than using a mouse and keyboard to act on what you can see on the screen. 15.2.2.1 Example: Replacing Many Student Names With Numerical IDs Say, for example, an elementary school administrator wants to replace each student name in a classroom dataset with a unique numerical ID. Doing this in a spreadsheet using good old fashioned data entry is fairly straightforward. Doing this for a whole school’s worth of classrooms though, demands a different approach. Rather than hand enter a unique id into a spreadsheet, the administrator can write an R script that executes the following steps: Use read_csv to store every classroom’s student list into the computer’s memory Use bind_rows to combine the separate lists into one long list Use mutate to replace student names with a randomized and unique numerical ID Use split to separate the data into classrooms again Use purrr and write_csv to create and rename individual spreadsheets to send back to teachers With some initial investment into thoughtful coding on the front end of this problem, the admininistrator now has a script she can use repeatedly in the future when she needs to do this task again. # TODO: More examples of differences in scale 15.3 Other Ways to Reimagine the Scale of Your Work 15.3.1 Reflect on your current scale, then push to the next level When you’ve been using the same data analysis tools and routines for a long time, it’s easy to forget to reflect on how you work. The analytic questions we ask, the datasets we use, and the scale of the analytic questions become automatic because for the most part they’ve delivered results. When you introduce data science techniques and R into your education analysis workflow, you also introduce an opportunity to ask yourself: How can I put this analytic question in context by analyzing on a larger scale? When an education client or coworker asks for help answering an analytic question, consider the following: At what level is this question about, student, classroom, school, district, regional, state, or federal? What can we learn by answering the analytic question at the current level, but also at the next level of scale up? If a teacher asks you to analyze the attendance pattern of one student, see what you learn by comparing to the the attendance pattern of the whole classroom or the whole school. If a superintendent of a school district asks you to analyze the behavior referrals of a school, analyze the behavior referrals of every school in the district. One of the many benefits of using programming languages like R to analyze data is that once you write code for one dataset, it can be used with many datasets with a relatively small amount of additional work. 15.3.2 Look for lots of similarly structured data Train your eyes to be alert to repositories that contain many datasets that have the exact same structure, then design ways to act on all those datasets at once. Data systems in education generate standardized data tables all the time. It’s one of the side effects of automation. Software developers design data systems to automatically generate many datasets for many people. The result is many datasets that contain different data, but all have the same number of columns and the same column names. This uniformity creates the perfect condition for R scripts to automatically act on these datasets in a way that is predictable and repeatable. Imagine a student information system that exports a list of students, their teacher, their grade level, and the number of school days attended to date. School administrator’s that have a weekly routine of exporting this data and storing it in a folder on their laptop will generate many uniformly structured datasets. When you train your eyes to see this as an opportunity to act on a lot of data at once, you will find an abundance of chances to transform data on a large scale so school staff can freely explore and ask questions aimed at improving the student experience. 15.3.3 Cleaning data Folks who work in education want to look at data about their students with tools like Excel, but the data is frequently not ready for analysis. You can empower these folks to explore data and ask more questions by being alert to opportunities to prepare lots of data for analysis. Offer to clean a dataset! Then do it again and do it fast. When you get into this habit, you not only train your data cleaning skills but you also train your education client’s expectations for how quickly you can prepare data for them. 15.4 Solving Problems Together Steven Spielberg said, “When I was a kid, there was no collaboration; it’s you with a camera bossing your friend around. But as an adult, filmmaking is all about appreciating the talents of the people you surround yourself with and knowing you could never have made any of these films by yourself” (Murphy 2011). Data science techniques are a powerful addition to an educational organization’s problem-solving capacity. But when you’re the only person who codes or fits statistical models, it’s easy to forget that the best solutions magically arrive when many perspectives come crashing together. Here are some things to think about as you challenge yourself to introduce data science to your education workplace in a lasting and meaningful way. 15.4.1 Data Science in Education and Empathy One definition of empathy is seeing things as others do, which points to a barrier to our mission of discovering ways to use our data science skills to improve the experience of learners–it is all too easy to assume that our coworkers will be inspired by possibilities of data science as you are. In 1990 Elizabeth Newton, then a Stanford University graduate, asked research subjects to “tap” out well-known songs with their fingers and estimate how many people would recognize the songs (Newton 1991, @hbr2006). She found that they overestimated every time! When we know a subject well, we tend to forget the experience of not knowing that subject. So how do we make use of this knowledge? First, listen carefully to your coworkers as they work with data. As you listen, aim to understand the thinking process they use when making sense of reports, tables, and graphs. This will help you understand the problems and solutions they gravitate towards. Second, ask them if you can “borrow the problem” for a bit. “Borrowing a problem” is not solving it for them, it’s using a little data science magic to get them unstuck so they can continue solving the problem the way they want to. If they’re struggling to make a scatter plot from their pivot table data, offer to help by cleaning and summarizing the dataset before they try again. Third, if your first attempt at borrowing the problem didn’t help, make an effort to learn more. Doing data science together is a conversation, so ask them how it went after you cleaned the dataset. Then listen, understand, and try again. After many rounds of this process, you may find your coworkers willing to try new methods for advancing their goals. A workplace going from not using data science to using data science regularly is a process that takes longer than you think. Responses to new ideas might include excitement and inspiration, but they might just as likely include resistance and fear. Changing the way an organization works requires new skills which often take years to learn. But here we are talking about one part of this change that is easily missed: listening to people and the system and using empathy to determine the unique place in your education organization that your data science skills will help students the most. Introducing data science techniques to your system is as much about having good people skills and empathy as it is about learning how to code and fit models. Data scientists and non-data scientists in education are similar in this regard–they both get excited and inspired by solving meaningful problems for their students. Once we recognize that that is the unifying goal, the exploration of how we do that with a diversity of expertise and tools begins. When we use empathy to connect with our coworkers about the common problems we are solving, we open the door to all kinds of solutions. Data science in education becomes a tool for a student-centered common cause, not an end in and of itself. Here are some reflection questions and exercise to use to inspire connection in your education workplace. Practice these questions both as personal reflections and also as questions you ask your coworkers: What does data analysis in our organiztion look like today? How do I wish data analysis will look like in the future? What is the hardest challenge I face in building my vision of student learning? What is one story about a rewarding experience I had with a student? 15.4.2 Create a Daily Practice Commitment That Answers Someone Else’s Question In his book Feck Perfuction, designer Victore (2019) writes “Success goes to those who keep moving, to those who can practice, make mistakes, fail, and still progress. It all adds up. Like exercise for muscles, the more you learn, the more you develop, and the stronger your skills become” (p. 31). Doing data science is a skill and like all skills, repetition and mistakes are their fuel for learning. But what happens if you are the first person to do data science in your education workplace? When you have no data science mentors, analytics routines, or examples of past practice, it can feel aimless to say the least. The antidote to that aimlessness is daily practice. Commit to writing code everyday. Even the the simplest three line scripts have a way of adding to your growing programming instincts. Train your ears to be radars for data projects that are usually done in a spreadsheet, then take them on and do them i R. Need the average amount of time a student with disabilities spends in speech and language sessions? Try it in R. Need to rename the columns in a student quiz dataset? Try it in R. The principal is hand assembling twelve classroom attendance sheets into one dataset? You get the picture. Now along the path of data science daily practice you may discover that your non-data science coworkers start kindly declining your offers for help. In my experience there is nothing mean happening here, but rather this is a response to imagining what it’s like to do what you are offering to do using the more commonly found spreadsheet applications. As your programming and statistics skills progress, some of the tasks you offer to help with will be the kind that, if done in a spreadsheet app, are overwhelmingly difficult and time intensive. So in environments where programming is not used for data analysis, declining your offers of help are more perceived acts of kindness to you and probably not statements about the usefulness of your work. As frustrating as these situations might be, they are necessary experiences as an organization learns just how available speed and scale of data analysis are when you use programming as a tool. In fact, these are opportunities you should seize because they serve both as daily practice and as demonstrations of the speed and scale programming for data analysis provides. 15.4.3 Build Your Network It is widely accepted that participating in personal and professional networks is important for survival, thriving, and innovation. The path to connecting to a data science in education network is apparent if your education workplace has an analytics department, but it will take a little more thought if you are the lone data scientist. When looking for allies that will inspire and teach you, the mind immediately searches for other programmers and statisticians and to be sure, these are relationships that will help you and the organization grow in its analytic approach. What the authors argue here is that data science in eduation is not just about bringing programming and statistics, but in the broader view is about evolving the whole approach to analytics. When viewed that way, members of a network broaden beyond just programmers and statisticians. It grows to include administrators and staff who are endlessly curious about the lives of students, graduate students fascinated with unique research methodologies, and designers who create interesting approaches to measurement. Networks for growing data science in education are not limited to the workplace. There are plenty of online and in real life chances to participate in a network that are just as rewarding as the networks you participate in during regular work hours. Here are a few to check out: Communities on Twitter like #RLadies and #rstats Local coding communities Conferences like rstudio::conf and useR! Online forums like RStudio Community 15.5 For K-12 Teachers We’ve used almost all of this chapter to explore what to think about and what to do to help you bring your data science skills to your education workplace. So far the discussion has been from the data scientist’s point of view, but what if you are one of the many who have an interest in analytics but very little interest in programming and statistics? Teachers in elementary and high schools are faced with a mind boggling amount of student data. A study by the Campaign (2018) estimated that “95 percent of teachers use a combination of academic data (test scores, graduation rates, etc.) and nonacademic data (attendance, classroom, behavior, etc.) to understand their students’ performance”. 57 percent of the teachers in the study said a lack of time was a barrier to using the data they have. Data literacy is also increasingly important within teacher preparation programs (Mandinach and Gummer 2013). Yet the majority of teachers aren’t interested in learning a programming language and statistical methods as a way to get better at analytics, and both time and professional development with respect to working with data are necessary (Datnow and Hubbard 2015). After all, most teachers chose their profession because they love teaching, not because they enjoy cleaning datasets and evaluating statistical model output. But to leave them out feels like a glaring ommission in a field where perhaps the most important shared value is the effective teaching of students. If you do happen to be an elementary or high school teacher who wants use programming and statistics to improve how you use data, you will find the approaches in this book useful. But if you are not that person, there is still much to explore that will lead to a rewarding experience as you grow your analytic skill. This book lacks the scope to explore this topic thoroughly, but there are many ways to improve how you use data without requiring a programming language or deep knowledge of statistics. For example, you can explore what is perhaps the most important element of starting a data analysis: asking the correct question. Chapter three of Peng and Matsui (2015)’s book, The Art of Data Science provides a useful process for getting better at asking data questions. Given how often data is served to us through data visualizations, it is important to learn the best ways to create and consume these visualizations. Chapter one of Healy (2019)’s book Data Visualization: A Practical Introduction, explores this topic using excellent examples and writing. For practical applications of a data-informed approach, Learning to Improve: How America’s Schools Can Get Better at Getting Better by Bryk et al. (2015) offers a thorough explanation of the improvement science process. The book is filled with examples of how data is used to understand problems and trial solutions. The final recommendation for elementary and secondary teachers wanting to get better at analysis is this: find, and partner with, someone who can help you answer the questions you have about how to serve your students better. You have the professional experience to come up with the right ideas and the curiosity to see what these ideas look like in the classroom. Inviting someone who can collaborate with you and help you measure the success of your ideas can be a rewarding partnership for you and your students. References "],
["c16.html", "16 Teaching Data Science 16.1 The pedagogical principles this book is based upon 16.2 Strategies for teaching data science 16.3 General strategies related to teaching and learning 16.4 Summary", " 16 Teaching Data Science This book is focused on the application of data science to education. In other words, this book focuses on how to apply data science to questions of teaching, learning, and educational systems. The previous chapters have addresses these topics narratively and in the form of walk-throughs for common questions (or problems) and the types of data encountered in education. While this book is focused on applying data science to education, an important consideration for data science is how to teach others about it. This is particularly the case for a book that is by a team of authors who are involved in education. Also, we expect readers of this book - many who will also be involved in education - will be interested in teaching others about data science. This chapter, then, focuses on teaching data science to others. This chapter is organized around three topics, which progress from concrete and specific (with reference to pedagogical approach we used to guide how we wrote this book) to more general ideas and findings from educational research about teaching and learning. The pedagogical principles this book is based upon Strategies for teaching data science General strategies related to teaching and learning 16.1 The pedagogical principles this book is based upon As the authors of a book about data science in education - and readers of books that taught us about data science - we considered what would make it effective for our readers when we set out to write it. The result of this process was a pedagogical framework that consists of three principles: problem-based learning, differentiation, and universal design. We consider each of these in turn. 16.1.1 Problem-based learning Problem-based learning (PBL) is a method of instruction that presents learners with a real-world challenge in which they must apply their skills and knowledge to solve. We applied this principle to the design of this book through including walk-throughs for common questions with respect to data science and education. Whereas some topics may benefit less from such an approach (and the inclusion of walk-throughs), for data science, we believe this is important because we do not have all of the right answers in this text. Moreover, there is not one right statistical model or algorithm, a technique to write code or even software and tools to utilize. Thus, the text features walkthroughs that reflect the types of challenges that educational data scientists may encounter in the course of their work: readers may choose to go about approaching the analysis of the data used in each walkthrough differently. At the end of walkthrough, there exist exercises that provide the opportunity for readers to extend and apply the ideas presented in the chapter. Moreover, the challenges are structured in such a way that readers return to some of them, but with different aims, over the course of the book. 16.1.2 Differentiation Differentiation is a method for providing multiple pathways for learners to engage with, understand, and ultimately apply new content and new skills. To differentiate this text, we first created personas for who we expected to be common groups of readers of the book (see Wilson (2009 https://teachtogether.tech/) for an example). We then differentiate the book by recognizing and providing background content/skills (either explicitly or through reference to other resources), embedded checks for understanding, and recommendations for where to begin based upon prior expertise. We also provide screenshots that are annotated and reflective of the content in the text to help to show readers how they are able to use what they are reading about. Part of differentiating this book concerns for whom we are differentiating it. We consider inclusivity (in terms of who belongs as a part of the audience for this text and how this broader view of who participates in data science implies the types of challenges, topics, and data that we include in the book) and accessibility (technically, in terms of how a wide audience of readers is able to access and use the book, as well as in terms of the ways in which the content is written based on the unique assets that those in education bring) along with how we differentiate the book. 16.1.3 Universal Design Universal design is a series of principles which guide the creation of spaces that are inclusive and accessible to individuals from all walks of life regardless of age, size, ability, or disability (Steinfeld and Maisel 2012). While traditionally applied to physical spaces, we have extended these principles to the creation of this book in such a way that the text and accompanying materials will be designed for individuals from all walks of life, regardless of educational level, background, ability, or disability. Many of the seven guiding principles of Universal Design are readily transferable to the creation of a text, such as equitable use, flexibility in use (aided in large part through differentiation), simple and intuitive use, perceptible information, and tolerance for error. 16.1.4 Working in the Open We started writing this book in the open, on GitHub. This allowed us to share the book as it developed. Writing the book in the open also allowed others from the wider educational data science and data science community to contribute. These contributions included writing sections of the book in which contributors had specific expertise, asking clarifying questions, and, even, creating a logo for the book which informed our choice of a color palette. We decided to write this book in the open after witnessing the success of other books on data science (such as Wickham (2019) Advanced R (https://adv-r.hadley.nz/) book. 16.2 Strategies for teaching data science You may be interested in teaching others’ data science. You may be doing this informally (such as by teaching a colleague with whom you work in your school district or organization), in a formal environment (such as a class on data science for educational data scientists or analysts), or in some setting in-between (such as a workshop you are asked to provide). There is some research on teaching data science, as well as practical advice from experienced instructors that can inform these efforts, which we detail in this section. 16.2.1 Provide a home base for learners to access resources (and to learn more) As we discuss in the next section, along with other important factors (such as learners’ motivation and having a supportive atmospher), learning strategies can make a difference for learners. Especially when it comes to learning to do data science, there are many tools and resources to keep track of, such as: How to download and install R How to download and install R Studio How to install packages How to access resources related to the workshop or course (or simply other resources you wish to share) How to contact the instructor How to get help and learn more Having a “home base,” where you can remind learners to first look to for resources, can help to lower some of learners’ demands in terms of remembering how these tools and resources can be accessed. One way to do this is through a personal website. Another is through GitHub pages. For some organizations, a proprietary learning management system - such as Desire2Learn, Blackboard, Moodle, or Canvas - can be helpful (especially if your learners are accustomed to using them). 16.2.2 When it comes to writing code, think early and often It is important to get learners to start writing code early and often. It can be tempting to teach classes or workshops that front-load content about data science and using R. While this information is doubtlessly important, it can mean that those you are teaching do not have the chance to do the things that they want to do, including installing R (and R Studio) and beginning to run analyses. Because of this, we recommend starting with strategies that lower the barrier to writing code for learners. Ways to do this in teaching settings include: Using R Studio Cloud Providing an R Markdown document for learners to work through Providing a data set and ideas for how to begin exploring it While these strategies are especially helpful for courses or workshops, they can be translated to teaching and learning R in tutoring (or “one-on-one”) opportunities for learners. In these cases, being able to work through and to modify an existing analysis (perhaps in R Studio Cloud) can be a way to quickly begin to run analyses - and to use the analysis as a template for analyses associated with other projects. Also, having a data set associated with a project or analysis - and a real need to analyze it using R - can be an outstanding way for an individual to learn to use R. 16.2.3 Don’t touch that keyboard! Resist helping learners to the point of hindering their learning. Wilson (2009) writes about the way in which those teaching others about R - or to program, in general - can find it easier to correct errors in learners’ work. But, by fixing errors, learners’ may perceive themselves to not be capable of carrying out all of the steps needed in an analysis on their own. This strategy relates to a broader issue, as well: issues that have to do with writing code in a way that runs correctly (e.g., with the correct capitalization and syntax) can be minor to those with experience programming, but can be major barriers to using R in an independent way for those new to it. For example, becoming comfortable with where arguments to functions belong and how to separate them, how to use brackets belong in functions or loops, and when it is necessary to use an assignment operator can be completely new to beginners: doing these steps for learners may push their learning later when they do not have as many resources available to help them than when you are teaching them. So, consider taking the additional time needed to help learners to navigate minor issues and errors in their code: it can pay off in increased motivation on their part in the longer-term. 16.2.4 Anticipate issues (and sacrifice accuracy for clarity) Don’t worry about being perfectly accurate early on, especially if doing so would lead to learners who are less interested in the topic you are teaching. For example, there are complicated issues at the heart of why data that is built-in to packages or to R (such as the iris dataset) appear in the environment after they are first used in an R session (see the section on “promises” in Wickham (2019)). Similarly, there are complicated issues that pertain to how functions are evaluated that can explain why it is important to provide the name of packages installed via install.packages() (whereas the names of arguments to othe functions, such as dplyr::select() do not need to be quoted). In these cases, wherein additional details may not be helpful to beginners, it can be valuable to important these questions (and the issues that are assocaited with them), but to have responses or answers that provide more clarity, rather than confusion. For example, 16.2.5 Start lessons or activities with visualizing data There are examples from data science books Grolemund and Wickham (2018) and past research (e.g. Lehrer and Schauble (2015)) that suggests that starting with visualizing data can be benefician in terms of learners’ ability to work with data. (???) write that they begin their book, Data Science Using R, with a chapter on visualization, because doing so allows learners to create something that they can share immediately, whereas tasks such as loading data can be rife with issues - and does not immediately lead learners to have a product they can share. Lehrer, Kim, and Schauble (2007) show how providing students with an opportunity to invent statistics by displaying the data in new ways. This led to (productive) critique among fifth- and sixth-grade students and their teacher. 16.2.6 Consider representation in the data and examples you use One way to think about data is that it is objective, free of decisions about what to value or prioritize. Another is to consider data as a process that is value-laden, from deciding what question to ask (and what data to collect) to interpreting findings with attention to how others will make sense of them (e.g., O’Neil (2016)’s Weapons of Math Destruction, and Lehrer, Kim, and Schauble (2007)’s description of data modeling). From this broader view, choosing representative data is a choice, like others, that teachers can make. For example, instructors can choose data that directs attention to issues– equity-related issues in education, for example–that she or he believes would be valuable for students to analyze. We think this consideration is important - particularly when it comes to data-related issues that we consider to be objective, such as how variables are assumed and constructed to be dichotomous (such as variables for individual’s gender) or categorical (such as variables for individual’s race), when the truth may be that such variables are based on decisions that analysts or those collecting data made - decisions that may benefit from questioning. This consideration is also important when it comes to what data is used for teaching and learning. If names of individual’s in data exclusively from individuals from majority racial or ethnic group, for example, some learners may implicitly perceive the content being taught to be designed for others. While we may think that such issues are better left to those we are teaching to decide later on, setting the stage in classes, courses, and other contexts in which data science is taught and learned can set an important precedent for the data our learners use and how they use it. 16.2.7 Draw on other resources In this section of this chapter, we presented some strategies for teaching data science. There are others that go more into depth on this topic from different perspectives, such as the following: GAISE Guidelines: guidelines for teaching statistics Data Science for Undergraduates: a report on undergraduate data science education R Studio Education: https://education.rstudio.com/ There are also a number of data science-related curricula (for the K-12 level) which may be helpful: Bootstrap Data Science Exploring CS, unit 5 Chromebook Data Science: http://jhudatascience.org/chromebookdatascience/ Oceans of Data Institute Curricula 16.3 General strategies related to teaching and learning The National Academy of Science commissioned a report, How People Learn (Bransford et al. 2000), that aimed to summarize research from the educational psychology and the learning sciences on teaching and learning. In 2018, the book was updated in How People Learn II (National Academies of Sciences, Medicine, and others 2018), which aimed to emphasize the social and cultural aspects of teaching and learning, which were not as much the focus of the earlier report. In this section, we highlight general strategies related to teaching and learning from the latest report (abbreviated as HPL2), with an emphasis on strategies applicable to teaching and learning data science. 16.3.1 Teaching and learning are complex One principle that HPL2 begins with is that learning is complex. in short, learning is not just about what learners know, or think, but is also about developmental, cultural, contextual, and historical factors - and distinctions between individuals with respect to each of these factors. This is an asset to teachers, as the authors of the report state: learners bring resources that can serve as a starting point for their learning trajectory when it comes to data science. These distinctions also mean that educators need to be concerned with - and to consider within their purview - factors well beyond what learners know, but also what their prior educational experiences have been and what resources and other individuals they have access to at work and at home, for example. 16.3.2 Learners learn many different things (consciously and unconsciously) We often think of learning in terms of objectives for specific lessons, but learners learn many different things at different times. The authors of HPL2 point out that individuals learn in response to different challenges and circumstances, including those in formal learning environements, such as workshops or classes. This principle implies a strategy that involves supporting learners to do data science, however and whenever they learn it. This means that it is both okay - and even to be expected - that learners may take away more from a problem they try to solve on their own, than what they do from a workshop or class (or even a degree!). This also suggests that learners may learn things that we do not anticipate, such as how intructors try to solve problems that arise in class. 16.3.3 Metacognition is important (even though it sounds more sophisticated than it is!) Educators and educational researchers often talk about metacognition, or thinking (and ideas) about thinking, as if it is something only very sophisticated learners do, but it is truly something much more commonplace, as people (and learners) are thinking about what they are learning and doing regularly. One strategy for instructors to support metacognition is to include moments wherein learners are asked to consider what they learned and what they would like to learn more about (exit tickets can be a great way to do this, but brief period in-class can also be used). Another strategy is to help learners to recognize when it is important to ask for help: Often, when doing data science, the right question to the right person (or community) can save hours of work. 16.3.4 Learning strategies matter While teachers are responsible for designing learning opportunities, learners also play an important role - in their own learning! Learning strategies, according to the authors of HPL2, matter, including those that help students to retrieve information and summarize and explain what they have learned (for themseleves and othres). There are many specific strategies documented in chapter four of HPL2. What is important for teachers of data science to know is less the specific strategies, and more the commitment to teaching learners how to learn. In addition to strategies for learners, teaching strategies, such as how content is spaced and sequenced, can also help learners. (???)’s Design for How People Learn presents these strategies, based largely about instructional design research, that may be helpful to those teaching data science. 16.3.5 Educators can help students to learn Educators know that how motivated learners are matters. According to the authors of HPL2, teachers can make an impact on how motivated learners are. Some specific things that educators can do to support learners include helping learners to set (and to work toward) goals, selecting content that is valuable and interesting to learners, helping learners to have choices and showing them how they are in-control of their learning value, and supporting learners to feel good at and supported in what they are doing. In short, motivation matters, and may be especially important when teaching learners who do not see the value of data science (initially!). 16.4 Summary In this section, we described the pedagogical principles for this book and strategies for teaching data science. We also directed attention to more general strategies for teaching and learning. Teaching data science is a relatively new area, but data science educators are not alone, given resources that are beign developed and those that can be adapted from other disciplines and the wider body of educational research. References "],
["c17.html", "17 Learning More 17.1 Introduction 17.2 Adopt a growth mindset 17.3 Discover new information 17.4 Ask for help 17.5 Share what you’ve learned 17.6 Welcome others", " 17 Learning More 17.1 Introduction If you’re reading this book cover to cover, you’ve been through quite a journey! So far, you’ve: Learned about the challenges of doing data science in education Practiced some basic coding and statistics techniques Worked through examples of analytic routines using education datasets Reflected on introducing data science to your education organization over time Learned about teaching data science to others We hope this book sparked an interest in data science that you want to nurture. We’ve talked to many people in your shoes–folks who care about educating students and want to help by using their data skills. Indeed we’ve found the common thread in our audience is wanting to use data to improve the experience of learners. It’s important to nurture this passion by keeping the learning going. Surrounding yourself with continuous learning experiences can turn this spark into a specialization that makes a real contribution to the lives of students. There are three reasons we feel these ongoing learning experiences are essential to realizing your vision for data in education. First, developing technical skills is an continuous process. The learning mindset is the same whether you’re taking your first steps toward using data science techniques or you’re a seasoned data scientist trying to make a bigger impact in education: there is always something new to learn about programming and statistics. Setting regular time aside to evolve your craft is a commitment to this mindset. Second, education and data science are like most industries–they are constantly evolving. That means today’s tools and best practices might be tomorrow’s outdated techniques. To keep up with changes, it is important to develop a learning routine that exposes you to the pulse of these two fields. Sometimes this means learning a new technique, sometimes it means deepening expertise in a technique you haven’t mastered, and other times it means revisiting a skill you’ve mastered long ago. And last, when you surround yourself with learning experiences, you inevitably surround yourself with others who are learning. Along your journey, you’ll interact with folks who are struggling through the same concepts as you, folks who are struggling through more complex concepts, and folks who are struggling with concepts you’ve already mastered. Participating in a community of learners has magical properties–it’s a place to learn, teach, inspire, and get inspired all at once. In his book Creative Calling Jarvis (2019) touches on this very point: Whether online or in person, connecting with a community will support your learning efforts. It will also expose you to a diverse set of ideas that will dramatically enrich your perspective on what you’re learning. If you weren’t in love with your new skill before, this step can tip the balance. Passion is infectious. You’ll need to use your intuition to find the areas where you want to deepend your knowledge. When you feel it, go there and dive in. Remember that the learning experience includes all kinds of activities. It’s a combination of reading, doing, discussing, walking away, and coming back. Here are some activities to include in your practice. We hope you take these and construct your own system of rewarding learning experiences. s 17.2 Adopt a growth mindset It’s normal to feel overwhelmed while learning skills like R and data science. This is particularly true when these fields themsleves are learning and growing. The R, data science, and education communities are constantly developing new techniques to move the field forward. It’s part of the beauty of this work! When you’re feeling overwhemed by everything you’re trying to learn, consider adopting a growth mindset. Carol Dweck’s argues that we think of ourselves as being or not being a type of person. For example, we might think of ourselves as “math people” or “reading people”. What matters is whether or not this state is changeable. When we believe we can change, we adopt a desire to learn, choose to be around people who help us learn, and make the effort to learn. When we move from a fixed mindset to a growth mindset, we create the possibility of mastering new techniques and realizing our vision for using data in education. The (nuances of the growth mindset)(https://www.edweek.org/ew/articles/2015/09/23/carol-dweck-revisits-the-growth-mindset.html)) are beyond the scope of this book, but we do encourage the general belief that we can learn how to apply these techniques. We encourage you to adopt a growth mindset as a way to inspire learning and belief that you can introduce data science in your education job. In doing so, you’ll be joining other data scientists who created a way to contribute to their fields. 17.3 Discover new information The content you surround yourself with matters. You can learn a lot and stay inspired by high quality books, blog posts, journals, journalism, and talks. In his book Steal Like An Artist, author Austin Kleon encourages people to surround themselves with great content: “There’s an economic theory out there that if you take the incomes of your five closest friends and average them, the resulting number will be pretty close to your own income. I think the same thing is true of our idea incomes. You’re only going to be as good as the stuff you surround yourself with.” In our Resources chapter, we share books and online resources that inspire us and help us learn. Use these as a starting point and build on them by seeking out authors, data scientists, and educators that inspire you to learn and master your craft. There are lots of ways to do this. Some folks follow data scientists on social media and take note of articles or talks that are getting attention. Others read data informed publications like Fivethirtyeight, The Economist, or The Upshot in the New York Times. Whichever you choose, make sure to stick with something that you’re drawn to and you just might find yourself with a new learning habit that is rewarding and fun. 17.4 Ask for help So far, we’ve discussed learning activities you can do on your own. Data science is a team sport, so at eventually your learning will lead you to others in the data science community. You can do this in many ways, both virtual and in real life. Here are a few examples you can try online. Try these and learn about what you’re comfortable with. Then build on that to surround yourself with many ways to ask and answer questions. 17.4.1 Discussion forums Visiting discussion forums is a common way to learn and participate in the R community. Websites like R Studio Community and Stack Overflow are very popular ways to do this. On these forums you’ll find many years worth of discussion about R and statistics. It’s quite unusual to search these and not find a way to get unstuck. Many discussions include a reproducible example of code that you can copy and paste into your own R console. This is a fantastic way to learn! Consider learning best practices for asking forum questions. Including a reproducible example, or “reprex”, to communiate problems is a widely-accepted norm. Bryan (2019)’s video about making reproducible examples is a great place to learn more. 17.4.2 GitHub repositories When you want to learn more about how a package works or engage a package’s online community, consider visiting the its GitHub repository. dplyr’s repository (https://github.com/tidyverse/dplyr) is a great example. You can start with the README then dive deeper in the vignettes, which contain demonstrations of the package’s functions. You can even browse the code on GitHub to learn more about how the packages work. Don’t worry, you won’t break anything! When you’re ready to see how the community engages a package’s authors, you can read through the Issues page. Each respository’s Issues page contains questions, feature requests, and bugs submitted by the programming community. Visit this page when you want to see if someone’s already submitted the coding challenge you’re working through. If you find you’re working on something that’s not a known problem, you can contribute by adding an issue. And finally, you can contribute to the development of packages by submitting code to the respository–this process is called a pull request. To learn more about contributing to packages, check out (???) talk. 17.5 Share what you’ve learned If you keep asking questions and finding solutions, you will soon find yourself ready to help others who are just getting started. The adage of learning by teaching applies here–answering someone else’s question also helps you deepen your learning and build empathy for new learners. Adopting a regular sharing routine is a great way to start helping others. A sharing routine encourages participation in the community, invites feedback for improvement, and calls on you to build your craft in a way that others can understand it. So what can you share? Really, what can’t you share? If you’ve built a cool function or visualization that took your project to the next level, you just might help or inspire someone else by sharing it. Maybe you’ve found an R package that really helped you–chances are it will help others. Sharing isn’t always about the output of your work, it can also be about how you work. Consider sharing a workflow you’ve developed or your experience at a recent data science conference. Anything that you learned or found interesting will be relevant to others too! What you share doesn’t have to be perfect. You can decide when you’re ready to share. Some data scientist’s blogs are polished and others are ideas-in-progress or shorter posts. You never know when someone will find value in your work, regardless of whether your work is in a refined state or not. Laslty, you can select your best work from all your sharing and use it as an online work portfolio. 17.5.1 Where to share There are many ways to share your work online. For rapid fire conversational sharing, Twitter. Be sure to use the hashtag #rstats to reach more data scientists. For long form sharing, consider posting to a data science blog. Robinson (2018)’s blog post Advice to aspiring data scientists: start a blog is wonderful inspiration for getting started. If you decide to post to a blog, there are tools to help you post data science content regularly. As noted earlier, Xie, Thomas, and Hill (2019)’s blogdown is designed to help you create websites using R Markdown and a static website creator called Hugo. Blogdown makes it easy to create, run, and publish code directly from R Studio. (???) has a great introduction on getting started with with Blogdown. When you do share a blog post or a tweet, broadcast what you have to say! On Twitter, use hashtags or “at” other community members to include them in the Tweet. On your blog, use blog aggregators that help share your posts to a wider audience. Here are two aggregators to get you started: R Weekly newsletter (https://rweekly.org/) R Bloggers (https://www.r-bloggers.com) Finally, share the love by engaging your fellow data scientists! Retweet others, leave comments, and interact with the vibrant data science and R communities online. 17.6 Welcome others If you find yourself becoming an envangelist for R and data science in education–that’s what happened to us!–welcome folks who are curious and ready to learn. The strength of any community comes from its inclusiveness, safe learning environment, and capacity to welcome new members. The data science community is no exception–many members work hard to create an environment with active participants, engaging conversations, and celebrations for little and big data science wins. Our call to action is this: continue growing this inclusive and positive environment by being the community member you’d want in your own network. Data science in education is a wonderful Venn diagram of communities, with new members joining every day. Welcoming, helping, and teaching new members is a great way to contribute to a positive community and to continue your own learning. What better way to inspire new members than to share your work and how it has impacted the lives of students! References "],
["c18.html", "18 Additional Resources 18.1 Data science courses 18.2 Workshop materials 18.3 Education resources 18.4 Data visualization 18.5 Books related to data science in education 18.6 Articles related to data science in education 18.7 Programming with R 18.8 Helpful package vignettes and descriptions of packages 18.9 Statistics 18.10 Software and R Packages 18.11 A career in data science 18.12 Places to share your work 18.13 Cheat Sheets 18.14 Learning communities", " 18 Additional Resources 18.1 Data science courses Anderson, D. J. (2019). University of Oregon Data Science Specialization for the College of Education. https://github.com/uo-datasci-specialization A series of courses that emphasize the use of R on data science in education (graduate-level). Landers, R. N. (2019). Data science for social scientists. http://datascience.tntlab.org/ A data science course for social scientists. R Studio. (2019). Data Science in a Box. https://datasciencebox.org/hello/ A complete course, including a curriculum and teaching materials, for data science. 18.2 Workshop materials Staudt Willet, B., Greenhalgh, S., &amp; Rosenberg, J. M. (2019, October). Workshop on using R at the Association for Educational Communications and Technology. https://github.com/bretsw/aect19-workshop Contains slides and code for a workshop carried out at an educational research conference, focused on how R can be used to analyze Internet (and social media) data. Anderson, D. J., and Rosenberg, J. M. (2019, April). Transparent and reproducible research with R. Workshop carried out at the Annual Meeting of the American Educational Research Association, Toronto, Canada. https://github.com/ResearchTransparency/rr_aera19 Slides and code for another workshop carried out at an educational research conference, focused on reproducible research and R Markdown. 18.3 Education resources Bryk et al (2015). Learning to improve: How America’s schools can get better at getting better. Cambridge, MA: Harvard Education Press. A general educational text related to systemic improvement. Penuel et al (2016, April). Findings from a national study on research use among school and district leaders. http://ncrpp.org/assets/documents/NCRPP_Technical-Report-1_National-Survey-of-Research-Use.pdf Findings on how stakeholders use educational research. Geller et al (2019, October). Education data done right: lessons from the trenches of applied data science. Independently published. A text on applying data science in education. 18.4 Data visualization Tufte, E. (2006). Beautiful evidence. Cheshire, CT: Graphics Press LLC. A classic text on data visualization. Healy, K. (2018). Data visualization: A practical introduction. Princeton, NJ: Princeton University Press. A programming- (and R-) based introduction to data visualization. Chang, W. (2013). R graphics cookbook. Sebastopol, CA: O’Reilly. Wilke, C. (2019). Fundamentals of data visualization. O’Reilly. https://serialmentor.com/dataviz/ A fantastic (though more conceptual than practical, i.e., there is no R code or other software implementation ror creating the plots) introduction to data visualization. 18.5 Books related to data science in education Krumm, A., Means, B., &amp; Bienkowski, M. (2018). Learning analytics goes to school: A collaborative approach to improving education. Routledge. Powers, K., &amp; Henderson, A. E. (Eds.). (2018). Cultivating a data culture in higher education. Routledge. Williamson, B. (2017). Big data in education: The digital future of learning, policy and practice. Sage. Lawson, J. (2015). Data Science in Higher Education: A Step-by-Step Introduction to Machine Learning for Institutional Researchers. CreateSpace. Swing, R. L. (2018). The Analytics Revolution in Higher Education: Big Data, Organizational Learning, and Student Success. Stylus Publishing, LLC. 18.6 Articles related to data science in education Williamson, B. (2017). Who owns educational theory? Big data, algorithms and the expert power of education data science. E-Learning and Digital Media, 14(3), 105-122. Liu, M. C., &amp; Huang, Y. M. (2017). The use of data science for education: The case of social-emotional learning. Smart Learning Environments, 4(1), 1. Rosenberg, J. M., Lawson, M. A., Anderson, D. J., Rutherford, T., &amp; Jones, R. S. (accepted pending minor revisions). Making Data Science “Count”: Data Science and Learning, Design, and Technology Research. In E. Romero-Hall (Ed.), Research Methods in Learning Design &amp; Technology. Routledge: New York, NY. 18.7 Programming with R Wickham, H. &amp; Grolemund, G. (2017). R for data science. Sebastopol, CA: O’Reilly. A new classic, with a focus on accessible tools. Teetor, P. (2011). R cookbook. Sebastopol, CA: O’Reilly. Bryan, J. &amp; Hestor, J. Happy git and github for the useR. Retrieved from https://happygitwithr.com A fantastic and accessible introduction to using git and GitHub. 18.8 Helpful package vignettes and descriptions of packages Introduction to dplyr. Retreived from https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html A short introduction to the caret package. Retrieved from https://cran.r-project.org/web/packages/caret/vignettes/caret.html tidy data. Retrieved from https://tidyr.tidyverse.org/articles/tidy-data.html Wickham et al. (2019). Welcome to the Tidyverse. Journal of Open Source Software, 4(43). 1686-1691. https://joss.theoj.org/papers/10.21105/joss.01686 18.9 Statistics 18.9.1 Introductory Open Intro. (2019). Textbooks. https://www.openintro.org/ Three open-source textbooks for statistics, one for high school students. Bruce, P. &amp; Bruce, A. (2017). Practical statistics for data scientists. Sebastopol, CA: O’Reilly. Navarro, D. (2019). Learning Statistics With R. https://learningstatisticswithr.com/ Field, A., Miles, J., &amp; Field, Z. (2012). Discovering statistics using R. Sage publications. Covers the foundations thoroughly and in an entertaining way. Ismay, C., &amp; Kim, A. Y. (2019). ModernDice: Statistical inference via data science. CRC Press. https://moderndive.com/ James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2015). An introduction to statistical learning with applications in R. New York, NY: Springer. This is an introductory (and R-based) version of a classic book on machine learning by Hastie, Tibshirani, and Friedman (2009). Peng, R. D. (2019). R programming for data science. Leanpub. https://leanpub.com/rprogramming Peng, R. D., &amp; Matsui, E. (2018). The art of data science. Leanpub. https://leanpub.com/artofdatascience 18.9.2 Advanced Gelman, A., &amp; Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press. A fantastic introduction not only to regression (and multi-level/hierarchical linear models, as well as Bayesian methods), but also to statistical analysis in general. Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction. Springer Science &amp; Business Media. A classic text on machine learning. West, B. T., Welch, K. B., &amp; Galecki, A. T. (2014). Linear mixed models: a practical guide using statistical software. Chapman and Hall/CRC. A solid introduction to multi-level/hierarchical linear models, including code in R (with an emphasis on the lme4 R package). McElreath, R. (2018). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC. [see also https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/] A new classic, accessible introduction to Bayesian methods. We note that this book has been “translated” into tidyverse code by Kurz (2019). 18.10 Software and R Packages Peng, R. D. (2019). Mastering software development in R. Leanpub. https://leanpub.com/msdr Wickham, H. (2015). R packages: Organize, test, document, and share your code. O’Reilly. http://r-pkgs.had.co.nz/ A comprehensive introduction to (and walkthrough for) creating your own R packages. 18.11 A career in data science Robinson, E., &amp; Nolis, J. (2020). Building a career in data science. Manning. https://www.manning.com/books/build-a-career-in-data-science?a_aid=buildcareer&amp;a_bid=76784b6a 18.12 Places to share your work Twitter: twitter.com Especially through the hashtags we mentioned below. LinkedIn: linkedin.com Can be a place not only to share career updates, but also data science-related works-in-progress. Medium : medium.com &lt;– !Say more here about how to share here? –&gt; 18.13 Cheat Sheets R Studio Cheat Sheets. https://rstudio.com/resources/cheatsheets/ See especially the dplyr, tidyr, purrr, ggplot2, and other cheat sheets 18.14 Learning communities Here some online communities and community resources we recommend: #rstats #tidyverse #RLadies #tidytuesday Here are two resources by co-author Mostipak related to the #r4ds community (from which #tidytuesday came): Mostipak, J. (2017). R4DS: the next iteration. Retrieved fromhttps://medium.com/(???) Mostipak, J. (2019). R4DS online learning community Improvements to self-taught data science &amp; the critical need for diversity, equity, and inclusion. Retrieved from https://resources.rstudio.com/rstudio-conf-2019/r4ds-online-learning-community-improvements-to-self-taught-data-science-and-the-critical-need-for-diversity-equity-and-inclusion-in-data-science-education References "],
["c19.html", "19 Conclusion: To where next? 19.1 Take on challenges 19.2 Know when to turn to something else", " 19 Conclusion: To where next? At the conclusion of this book, we hope that you feel ready to take on the data- related question or problem you have wanted to address. We think that the work that you will do is important. In education, particularly, there is a lot of promise for how data can be used - by teachers, administrators, parents, and even by students. There is also some peril, though, especially if data is used that does not align with the goals and values of educators - and individually and collectively learning and changing. As you work toward that goal, we wish to offer some closing thoughts. 19.1 Take on challenges Many of the things that you surely want to do are likely beyond the realm of what is possible now. These challenges, from the social (the problem you are addressing is incredibly hard) and organizational (if your organization is not ready for the types of work you are trying to do) to the technical (perhaps what you want to do has literally never been done before), can be really difficult to tackle. One small suggestion we have is to tell a friend. When you’re frustrated, having a group you can turn to can be valuable. Relatedly, when R tells you that NULL is not TRUE (or any one of innumerable pain points), try to laugh. It is so common when doing data science to run into these frustrations that social media accounts have emerged to chronicle them (see (???)(https:/twitter.com/accidentalart))! Everyone, from the most experienced R coder (Hadley Wickham has mentioned this many times) to the person using R for the first time constantly runs into difficulties when trying something new - you are not alone! So, take on challenges: a joy of doing data science is that it is a field at this time, especially in education, is that the promise and peril of using data are both very real, and those taking on challenges have the chance to shape the direction of how data is used 19.2 Know when to turn to something else In addition to taking on challenges (and turning to friends and laughing), we want to acknowledge everyone needs a break. Sometimes, instead of writing one last line of R code, it can be better to do something else! An implication of this is that leaving off so that you can pick back up where you stopped is important. But, by doing so, you can live to code another day. We need as many data scientists working in different corners of education as possible, and we don’t want to lose you to burn-out! Similarly, when working on any project - even (mand aybe particularly those you are passionate about - it can be important to switch to something else (data science-related or otherwise) to retain the curiosity, passion, and interest that brought you to the work in the first place. Consider working on a different project or even taking a break from a project to pursue another hobby or interest. That curiosity, passion, and interest for educational and social (and data science in education-related) topics is important to cultivate - perhaps as important as the capabilities that this book focused on. if (knitr::is_html_output()) ‘# References {c20}’ "]
]
